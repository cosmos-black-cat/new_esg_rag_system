# =============================================================================
# ä¿®å¾©çš„LLMå¢å¼·å’ŒAPIè¼ªæ›é‚è¼¯
# =============================================================================

import json
import re
import time
import random
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

class GeminiAPIManager:
    """æ”¹é€²çš„APIç®¡ç†å™¨ï¼Œç¢ºä¿è¼ªæ›ç”Ÿæ•ˆ"""
    
    def __init__(self, api_keys: List[str], model_name: str = "gemini-1.5-flash"):
        self.api_keys = api_keys
        self.model_name = model_name
        self.current_key_index = 0
        self.key_cooldowns = {}
        self.key_usage_count = {key: 0 for key in api_keys}
        self.last_request_time = 0
        self.min_request_interval = 0.8  # æ¸›å°‘é–“éš”ä»¥æ›´å¿«è§¸ç™¼è¼ªæ›
        self.request_count = 0
        self.rotation_threshold = 20  # æ¯20æ¬¡è«‹æ±‚å¼·åˆ¶è¼ªæ›
        
        print(f"ğŸ”‘ åˆå§‹åŒ–æ”¹é€²APIç®¡ç†å™¨ï¼Œå…±æœ‰ {len(api_keys)} å€‹API key")
        self._initialize_current_llm()
    
    def _initialize_current_llm(self):
        """åˆå§‹åŒ–ç•¶å‰çš„LLMå¯¦ä¾‹"""
        from langchain_google_genai import ChatGoogleGenerativeAI
        
        current_key = self.api_keys[self.current_key_index]
        self.current_llm = ChatGoogleGenerativeAI(
            model=self.model_name,
            google_api_key=current_key,
            temperature=0.1,
            max_tokens=1024,
            convert_system_message_to_human=True
        )
        print(f"ğŸ¯ ç•¶å‰ä½¿ç”¨API key: {current_key[:10]}... (ç¬¬{self.current_key_index + 1}å€‹)")
    
    def _should_rotate_api(self) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è¼ªæ›API"""
        # æ¢ä»¶1: é”åˆ°è¼ªæ›é–¾å€¼
        if self.request_count % self.rotation_threshold == 0 and self.request_count > 0:
            return True
        
        # æ¢ä»¶2: ç•¶å‰APIä½¿ç”¨æ¬¡æ•¸éå¤š
        current_key = self.api_keys[self.current_key_index]
        if self.key_usage_count[current_key] > 50:  # é™ä½é–¾å€¼
            return True
        
        # æ¢ä»¶3: éš¨æ©Ÿè¼ªæ›ï¼ˆå¢åŠ è¼ªæ›æ©Ÿæœƒï¼‰
        if len(self.api_keys) > 1 and random.random() < 0.1:  # 10%æ©Ÿç‡
            return True
        
        return False
    
    def _force_rotate_to_next_key(self):
        """å¼·åˆ¶è¼ªæ›åˆ°ä¸‹ä¸€å€‹å¯ç”¨çš„API key"""
        if len(self.api_keys) <= 1:
            return False
        
        original_index = self.current_key_index
        
        # å˜—è©¦æ‰¾åˆ°ä¸‹ä¸€å€‹å¯ç”¨çš„key
        for i in range(1, len(self.api_keys)):
            next_index = (self.current_key_index + i) % len(self.api_keys)
            next_key = self.api_keys[next_index]
            
            # æª¢æŸ¥æ˜¯å¦åœ¨å†·å»æœŸ
            if next_key not in self.key_cooldowns or datetime.now() >= self.key_cooldowns[next_key]:
                self.current_key_index = next_index
                self._initialize_current_llm()
                print(f"ğŸ”„ å¼·åˆ¶è¼ªæ›API key: ç¬¬{original_index + 1}å€‹ â†’ ç¬¬{next_index + 1}å€‹")
                return True
        
        print("âš ï¸ æ‰€æœ‰å…¶ä»–API keyéƒ½åœ¨å†·å»æœŸï¼Œç¹¼çºŒä½¿ç”¨ç•¶å‰key")
        return False
    
    def invoke(self, prompt: str, max_retries: int = 3) -> str:
        """æ”¹é€²çš„APIèª¿ç”¨æ–¹æ³•"""
        self.request_count += 1
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦è¼ªæ›
        if self._should_rotate_api():
            self._force_rotate_to_next_key()
        
        for attempt in range(max_retries):
            try:
                # æ·»åŠ è«‹æ±‚å»¶é²
                self._add_request_delay()
                
                # è¨˜éŒ„ä½¿ç”¨æ¬¡æ•¸
                current_key = self.api_keys[self.current_key_index]
                self.key_usage_count[current_key] += 1
                
                # èª¿ç”¨API
                response = self.current_llm.invoke(prompt)
                
                print(f"âœ… APIèª¿ç”¨æˆåŠŸ (key {self.current_key_index + 1}, ç¬¬{self.key_usage_count[current_key]}æ¬¡ä½¿ç”¨)")
                return response.content if hasattr(response, 'content') else str(response)
                
            except Exception as e:
                print(f"âš ï¸ APIèª¿ç”¨å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                
                if "rate limit" in str(e).lower() or "quota" in str(e).lower():
                    # é€Ÿç‡é™åˆ¶ï¼Œå˜—è©¦åˆ‡æ›API
                    if self._force_rotate_to_next_key():
                        continue
                
                if attempt == max_retries - 1:
                    raise e
                
                # ç­‰å¾…é‡è©¦
                wait_time = (attempt + 1) * 2
                print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
        
        raise Exception("æ‰€æœ‰é‡è©¦å˜—è©¦éƒ½å¤±æ•—äº†")
    
    def _add_request_delay(self):
        """æ·»åŠ è«‹æ±‚é–“éš”"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last
            sleep_time += random.uniform(0, 0.3)  # æ·»åŠ éš¨æ©Ÿå»¶é²
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()

class ImprovedLLMEnhancer:
    """æ”¹é€²çš„LLMå¢å¼·å™¨ï¼Œæé«˜æˆåŠŸç‡"""
    
    def __init__(self, api_manager):
        self.api_manager = api_manager
        self.success_count = 0
        self.total_count = 0
        self.failed_responses = []  # è¨˜éŒ„å¤±æ•—çš„éŸ¿æ‡‰ç”¨æ–¼èª¿è©¦
    
    def build_improved_prompt(self, extraction) -> str:
        """æ§‹å»ºæ”¹é€²çš„é©—è­‰æç¤º"""
        return f"""è«‹åˆ†æä»¥ä¸‹æ•¸æ“šæå–çµæœæ˜¯å¦èˆ‡å†ç”Ÿå¡‘è† /å¡‘æ–™ç›¸é—œï¼š

é—œéµå­—: {extraction.keyword}
æå–å€¼: {extraction.value}
æ•¸æ“šé¡å‹: {extraction.value_type}

æ–‡æœ¬å…§å®¹:
{extraction.paragraph[:300]}...

åˆ¤æ–·æ¨™æº–:
1. æ˜¯å¦èˆ‡å†ç”Ÿå¡‘è† ã€å›æ”¶å¡‘æ–™ã€PCRææ–™ç­‰ç›´æ¥ç›¸é—œï¼Ÿ
2. æå–çš„æ•¸å€¼æ˜¯å¦ç¢ºå¯¦æè¿°å†ç”Ÿææ–™çš„ä½¿ç”¨é‡ã€æ¯”ä¾‹æˆ–ç”¢èƒ½ï¼Ÿ
3. æ˜¯å¦æ’é™¤äº†ç„¡é—œä¸»é¡Œï¼ˆå¦‚å“¡å·¥ã€é™é›¨ã€è³½äº‹ç­‰ï¼‰ï¼Ÿ

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "is_relevant": true,
    "confidence": 0.85,
    "explanation": "æè¿°ç›¸é—œæ€§çš„ç°¡çŸ­èªªæ˜"
}}"""
    
    def parse_llm_response(self, response_text: str) -> Optional[Dict]:
        """æ”¹é€²çš„LLMéŸ¿æ‡‰è§£æ"""
        if not response_text:
            return None
        
        # æ–¹æ³•1: å˜—è©¦ç›´æ¥è§£æJSON
        try:
            # æ¸…ç†éŸ¿æ‡‰æ–‡æœ¬
            cleaned_response = response_text.strip()
            
            # å°‹æ‰¾JSONå…§å®¹
            json_match = re.search(r'\{[^{}]*\}', cleaned_response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # é©—è­‰å¿…è¦å­—æ®µ
                if 'is_relevant' in result and 'confidence' in result:
                    return {
                        'is_relevant': bool(result.get('is_relevant', False)),
                        'confidence': float(result.get('confidence', 0.5)),
                        'explanation': str(result.get('explanation', 'ç„¡èªªæ˜'))
                    }
        except Exception as e:
            pass
        
        # æ–¹æ³•2: é—œéµå­—è§£æ
        try:
            response_lower = response_text.lower()
            
            # åˆ¤æ–·ç›¸é—œæ€§
            is_relevant = False
            if any(word in response_lower for word in ['true', 'ç›¸é—œ', 'æ˜¯', 'relevant', 'yes']):
                is_relevant = True
            elif any(word in response_lower for word in ['false', 'ä¸ç›¸é—œ', 'å¦', 'irrelevant', 'no']):
                is_relevant = False
            else:
                # å¦‚æœç„¡æ³•åˆ¤æ–·ï¼Œé»˜èªç‚ºä¸ç›¸é—œï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                is_relevant = False
            
            # æå–ä¿¡å¿ƒåˆ†æ•¸
            confidence = 0.5
            confidence_match = re.search(r'(?:confidence|ä¿¡å¿ƒ).*?(\d+\.?\d*)', response_lower)
            if confidence_match:
                confidence = min(float(confidence_match.group(1)), 1.0)
                if confidence > 5:  # å¦‚æœæ˜¯ç™¾åˆ†æ¯”æ ¼å¼
                    confidence = confidence / 100
            
            return {
                'is_relevant': is_relevant,
                'confidence': confidence,
                'explanation': response_text[:100] + "..." if len(response_text) > 100 else response_text
            }
            
        except Exception as e:
            pass
        
        # æ–¹æ³•3: é»˜èªè™•ç†
        return {
            'is_relevant': False,  # ä¿å®ˆç­–ç•¥ï¼šç„¡æ³•è§£ææ™‚èªç‚ºä¸ç›¸é—œ
            'confidence': 0.3,
            'explanation': f"éŸ¿æ‡‰è§£æå¤±æ•—: {response_text[:50]}..."
        }
    
    def enhance_extraction(self, extraction) -> Tuple[bool, Dict]:
        """å¢å¼·å–®å€‹æå–çµæœ"""
        self.total_count += 1
        
        try:
            # æ§‹å»ºæç¤º
            prompt = self.build_improved_prompt(extraction)
            
            # èª¿ç”¨LLM
            response_text = self.api_manager.invoke(prompt)
            
            # è§£æéŸ¿æ‡‰
            llm_result = self.parse_llm_response(response_text)
            
            if llm_result:
                self.success_count += 1
                return True, llm_result
            else:
                self.failed_responses.append({
                    'prompt': prompt[:100],
                    'response': response_text[:200],
                    'keyword': extraction.keyword
                })
                return False, {'is_relevant': False, 'confidence': 0.3, 'explanation': 'è§£æå¤±æ•—'}
        
        except Exception as e:
            self.failed_responses.append({
                'error': str(e),
                'keyword': extraction.keyword
            })
            return False, {'is_relevant': False, 'confidence': 0.2, 'explanation': f'APIèª¿ç”¨å¤±æ•—: {e}'}
    
    def get_enhancement_stats(self) -> Dict:
        """ç²å–å¢å¼·çµ±è¨ˆä¿¡æ¯"""
        success_rate = (self.success_count / self.total_count * 100) if self.total_count > 0 else 0
        
        return {
            'success_count': self.success_count,
            'total_count': self.total_count,
            'success_rate': success_rate,
            'failed_responses_count': len(self.failed_responses)
        }
    
    def print_debug_info(self):
        """æ‰“å°èª¿è©¦ä¿¡æ¯"""
        print(f"\nğŸ”§ LLMå¢å¼·èª¿è©¦ä¿¡æ¯:")
        print(f"æˆåŠŸæ¬¡æ•¸: {self.success_count}")
        print(f"ç¸½æ¬¡æ•¸: {self.total_count}")
        print(f"æˆåŠŸç‡: {(self.success_count/self.total_count*100):.1f}%")
        
        if self.failed_responses:
            print(f"\nâŒ å¤±æ•—æ¡ˆä¾‹ (å‰3å€‹):")
            for i, failure in enumerate(self.failed_responses[:3], 1):
                print(f"{i}. é—œéµå­—: {failure.get('keyword', 'N/A')}")
                if 'error' in failure:
                    print(f"   éŒ¯èª¤: {failure['error']}")
                else:
                    print(f"   éŸ¿æ‡‰: {failure.get('response', 'N/A')[:100]}...")

class ImprovedESGExtractor:
    """æ”¹é€²çš„ESGæå–å™¨"""
    
    def __init__(self, vector_db_path: str = None, enable_llm: bool = True, auto_dedupe: bool = True):
        self.vector_db_path = vector_db_path or VECTOR_DB_PATH
        self.enable_llm = enable_llm
        self.auto_dedupe = auto_dedupe
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.matcher = EnhancedMatcher()  # ä½¿ç”¨å¢å¼·çš„åŒ¹é…å™¨
        self.keyword_config = EnhancedKeywordConfig()  # ä½¿ç”¨å¢å¼·çš„é—œéµå­—é…ç½®
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        self._load_vector_database()
        
        # åˆå§‹åŒ–æ”¹é€²çš„LLM
        if self.enable_llm:
            self._init_improved_llm()
    
    def _init_improved_llm(self):
        """åˆå§‹åŒ–æ”¹é€²çš„LLM"""
        try:
            print(f"ğŸ¤– åˆå§‹åŒ–æ”¹é€²çš„LLMå¢å¼·å™¨...")
            
            if len(GEMINI_API_KEYS) > 1:
                print(f"ğŸ”„ å•Ÿç”¨æ”¹é€²çš„å¤šAPIè¼ªæ›æ¨¡å¼ï¼Œå…± {len(GEMINI_API_KEYS)} å€‹Keys")
                self.api_manager = ImprovedAPIManager(
                    api_keys=GEMINI_API_KEYS,
                    model_name=GEMINI_MODEL
                )
                self.llm_enhancer = ImprovedLLMEnhancer(self.api_manager)
                self.llm_mode = "improved_multi_api"
            else:
                print("ğŸ”‘ ä½¿ç”¨æ”¹é€²çš„å–®APIæ¨¡å¼")
                single_api_manager = ImprovedAPIManager(
                    api_keys=GEMINI_API_KEYS,
                    model_name=GEMINI_MODEL
                )
                self.llm_enhancer = ImprovedLLMEnhancer(single_api_manager)
                self.llm_mode = "improved_single_api"
            
            print("âœ… æ”¹é€²çš„LLMåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def improved_llm_enhancement(self, extractions: List) -> List:
        """æ”¹é€²çš„LLMå¢å¼·éç¨‹"""
        if not self.enable_llm or not extractions:
            return extractions
        
        print("ğŸ¤– åŸ·è¡Œæ”¹é€²çš„LLMå¢å¼·...")
        print(f"ğŸ”„ è™•ç† {len(extractions)} å€‹æå–çµæœ")
        
        enhanced_extractions = []
        
        for i, extraction in enumerate(extractions):
            if i % 10 == 0:  # æ¯10å€‹é¡¯ç¤ºé€²åº¦
                progress = (i / len(extractions)) * 100
                print(f"ğŸ“Š é€²åº¦: {progress:.1f}% ({i}/{len(extractions)})")
            
            # LLMå¢å¼·
            success, llm_result = self.llm_enhancer.enhance_extraction(extraction)
            
            if success and llm_result.get("is_relevant", False):
                # æ›´æ–°ä¿¡å¿ƒåˆ†æ•¸
                llm_confidence = llm_result.get("confidence", extraction.confidence)
                extraction.confidence = min((extraction.confidence + llm_confidence) / 2, 1.0)
                
                # æ·»åŠ LLMçš„è§£é‡‹
                extraction.context_window += f"\n[LLMé©—è­‰]: {llm_result.get('explanation', '')}"
                
                enhanced_extractions.append(extraction)
            elif llm_result.get("confidence", 0) > 0.7:
                # å³ä½¿LLMèªç‚ºä¸ç›¸é—œï¼Œä½†ä¿¡å¿ƒåˆ†æ•¸å¾ˆé«˜çš„æƒ…æ³ä¸‹ä¿ç•™
                extraction.confidence *= 0.8  # é™ä½ä¿¡å¿ƒåˆ†æ•¸
                extraction.context_window += f"\n[LLMæ³¨æ„]: {llm_result.get('explanation', '')}"
                enhanced_extractions.append(extraction)
            # å…¶ä»–æƒ…æ³ä¸‹ä¸Ÿæ£„è©²æå–çµæœ
        
        # æ‰“å°çµ±è¨ˆä¿¡æ¯
        stats = self.llm_enhancer.get_enhancement_stats()
        print(f"\nâœ… æ”¹é€²çš„LLMå¢å¼·å®Œæˆ:")
        print(f"   LLMæˆåŠŸç‡: {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_count']})")
        print(f"   ä¿ç•™çµæœ: {len(enhanced_extractions)}/{len(extractions)} ({len(enhanced_extractions)/len(extractions)*100:.1f}%)")
        
        # é¡¯ç¤ºAPIä½¿ç”¨çµ±è¨ˆ
        if hasattr(self, 'api_manager'):
            self.api_manager.print_usage_statistics()
        
        # é¡¯ç¤ºèª¿è©¦ä¿¡æ¯ï¼ˆå¦‚æœæˆåŠŸç‡å¤ªä½ï¼‰
        if stats['success_rate'] < 70:
            print("âš ï¸ LLMæˆåŠŸç‡åä½ï¼Œé¡¯ç¤ºèª¿è©¦ä¿¡æ¯:")
            self.llm_enhancer.print_debug_info()
        
        return enhanced_extractions

# =============================================================================
# æ¸¬è©¦å‡½æ•¸
# =============================================================================

def test_improved_llm_enhancement():
    """æ¸¬è©¦æ”¹é€²çš„LLMå¢å¼·åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦æ”¹é€²çš„LLMå¢å¼·åŠŸèƒ½")
    print("=" * 50)
    
    # æ¨¡æ“¬æå–çµæœ
    from dataclasses import dataclass
    
    @dataclass
    class MockExtraction:
        keyword: str
        value: str
        value_type: str
        paragraph: str
        confidence: float
        context_window: str = ""
    
    test_extractions = [
        MockExtraction(
            keyword="å†ç”Ÿå¡‘è† ",
            value="12,000å™¸",
            value_type="number",
            paragraph="å…¬å¸å†ç”Ÿå¡‘è† ææ–™ç”¢èƒ½é”åˆ°12,000å™¸ï¼Œä¸»è¦ç”¨æ–¼ç’°ä¿åŒ…è£ç”¢å“è£½é€ ã€‚",
            confidence=0.9
        ),
        MockExtraction(
            keyword="å›æ”¶",
            value="3,500ä»¶",
            value_type="number",
            paragraph="ç‚ºç½é•ä¸‰å¹´é‡å•Ÿçš„ç››å¤§è³½äº‹ã€Œå‚ç›´é¦¬æ‹‰æ¾ã€æ‰“é€ å²ä¸Šæœ€ç’°ä¿è³½è¡£ï¼Œé è¨ˆæä¾›3,500ä»¶ã€‚",
            confidence=0.7
        ),
        MockExtraction(
            keyword="å›æ”¶é‡",
            value="249å™¸",
            value_type="number",
            paragraph="2023å¹´å› æœˆå¹³å‡é™é›¨é‡æ¸›å°‘18%ï¼Œè‡´é›¨æ°´å›æ”¶é‡æ¸›å°‘249å™¸ã€‚",
            confidence=0.6
        )
    ]
    
    try:
        # åˆå§‹åŒ–APIç®¡ç†å™¨
        api_manager = ImprovedAPIManager(GEMINI_API_KEYS, GEMINI_MODEL)
        enhancer = ImprovedLLMEnhancer(api_manager)
        
        print(f"æ¸¬è©¦ {len(test_extractions)} å€‹æå–çµæœ...")
        
        enhanced_results = []
        for extraction in test_extractions:
            print(f"\næ¸¬è©¦: {extraction.keyword} - {extraction.value}")
            success, result = enhancer.enhance_extraction(extraction)
            
            print(f"LLMåˆ¤æ–·: {'ç›¸é—œ' if result.get('is_relevant') else 'ä¸ç›¸é—œ'}")
            print(f"ä¿¡å¿ƒåˆ†æ•¸: {result.get('confidence', 0):.2f}")
            print(f"èªªæ˜: {result.get('explanation', 'N/A')[:100]}...")
            
            if success and result.get('is_relevant'):
                enhanced_results.append(extraction)
        
        print(f"\nğŸ“Š æ¸¬è©¦çµæœ:")
        print(f"åŸå§‹çµæœ: {len(test_extractions)}")
        print(f"é€šéLLMé©—è­‰: {len(enhanced_results)}")
        
        # é¡¯ç¤ºçµ±è¨ˆ
        enhancer.print_debug_info()
        api_manager.print_usage_statistics()
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    test_improved_llm_enhancement()