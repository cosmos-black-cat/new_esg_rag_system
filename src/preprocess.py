#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGå ±å‘Šæ›¸é è™•ç†æ¨¡çµ„ v1.0
è™•ç†PDFæ–‡ä»¶ã€å»ºç«‹å‘é‡è³‡æ–™åº«ã€æ¨™æº–åŒ–æª”å
"""

import os
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from tqdm import tqdm

sys.path.append(str(Path(__file__).parent))
from config import *

# =============================================================================
# å°ç£ä¸Šå¸‚æ«ƒå…¬å¸ä»£è™Ÿæ˜ å°„è¡¨
# =============================================================================

COMPLETE_COMPANY_MAPPING = {
    # 13é–‹é ­å¡‘è† å·¥æ¥­
    "å°å¡‘": ("1301", "å°å¡‘"),
    "å°ç£å¡‘è† ": ("1301", "å°å¡‘"),
    "å°ç£å¡‘è† å·¥æ¥­": ("1301", "å°å¡‘"),
    "å°å¡‘å·¥æ¥­": ("1301", "å°å¡‘"),
    "å°å¡‘é›†åœ˜": ("1301", "å°å¡‘"),
    
    "å—äº": ("1303", "å—äº"),
    "å—äºå¡‘è† ": ("1303", "å—äº"),
    "å—äºå¡‘è† å·¥æ¥­": ("1303", "å—äº"),
    "å—äºå…¬å¸": ("1303", "å—äº"),
    
    "å°èš": ("1304", "å°èš"),
    "å°ç£èšåˆ": ("1304", "å°èš"),
    "å°ç£èšåˆåŒ–å­¸": ("1304", "å°èš"),
    
    "è¯å¤": ("1305", "è¯å¤"),
    "è¯å¤æµ·ç£": ("1305", "è¯å¤"),
    "è¯å¤æµ·ç£å¡‘è† ": ("1305", "è¯å¤"),
    
    "ä¸‰èŠ³": ("1307", "ä¸‰èŠ³"),
    "ä¸‰èŠ³åŒ–å­¸": ("1307", "ä¸‰èŠ³"),
    "ä¸‰èŠ³åŒ–å­¸å·¥æ¥­": ("1307", "ä¸‰èŠ³"),
    "ä¸‰èŠ³åŒ–å·¥": ("1307", "ä¸‰èŠ³"),
    
    "äºèš": ("1308", "äºèš"),
    "äºæ´²èšåˆ": ("1308", "äºèš"),
    
    "å°é”åŒ–": ("1309", "å°é”åŒ–"),
    "å°ç£é”åŒ–": ("1309", "å°é”åŒ–"),
    
    "å°è‹¯": ("1310", "å°è‹¯"),
    "å°ç£è‹¯ä¹™çƒ¯": ("1310", "å°è‹¯"),
    
    "åœ‹å–¬": ("1312", "åœ‹å–¬"),
    "åœ‹å–¬çŸ³åŒ–": ("1312", "åœ‹å–¬"),
    "åœ‹æ©‹": ("1312", "åœ‹å–¬"),
    
    "åœ‹å–¬ç‰¹": ("1312A", "åœ‹å–¬ç‰¹"),
    
    "è¯æˆ": ("1313", "è¯æˆ"),
    "è¯æˆåŒ–å­¸": ("1313", "è¯æˆ"),
    "è¯æˆåŒ–ç§‘": ("1313", "è¯æˆ"),
    
    "ä¸­çŸ³åŒ–": ("1314", "ä¸­çŸ³åŒ–"),
    "ä¸­åœ‹çŸ³æ²¹åŒ–å­¸": ("1314", "ä¸­çŸ³åŒ–"),
    "é¼è¶Š": ("1314", "ä¸­çŸ³åŒ–"),
    "é¼è¶Šé–‹ç™¼": ("1314", "ä¸­çŸ³åŒ–"),
    
    "é”æ–°": ("1315", "é”æ–°"),
    "é”æ–°å·¥æ¥­": ("1315", "é”æ–°"),
    
    "ä¸Šæ›œ": ("1316", "ä¸Šæ›œ"),
    
    "æ±é™½": ("1319", "æ±é™½"),
    "æ±é™½å¯¦æ¥­": ("1319", "æ±é™½"),
    
    "å¤§æ´‹": ("1321", "å¤§æ´‹"),
    "å¤§æ´‹å¡‘è† ": ("1321", "å¤§æ´‹"),
    
    "æ°¸è£•": ("1323", "æ°¸è£•"),
    "æ°¸è£•å¡‘è† ": ("1323", "æ°¸è£•"),
    
    "åœ°çƒ": ("1324", "åœ°çƒ"),
    "åœ°çƒåŒ–å­¸": ("1324", "åœ°çƒ"),
    
    "æ†å¤§": ("1325", "æ†å¤§"),
    
    "å°åŒ–": ("1326", "å°åŒ–"),
    "å°ç£åŒ–å­¸": ("1326", "å°åŒ–"),
    "å°ç£åŒ–å­¸çº–ç¶­": ("1326", "å°åŒ–"),
    
    "å°ç¿°": ("1336", "å°ç¿°"),
    
    "å†ç”Ÿ": ("1337", "å†ç”Ÿ-KY"),
    "å†ç”Ÿ-KY": ("1337", "å†ç”Ÿ-KY"),
    "å†ç”Ÿè³‡æº": ("1337", "å†ç”Ÿ-KY"),
    
    "å»£è¯": ("1338", "å»£è¯-KY"),
    "å»£è¯-KY": ("1338", "å»£è¯-KY"),
    
    "æ˜­è¼": ("1339", "æ˜­è¼"),
    
    "å‹æ‚…": ("1340", "å‹æ‚…-KY"),
    "å‹æ‚…-KY": ("1340", "å‹æ‚…-KY"),
    
    "å¯Œæ—": ("1341", "å¯Œæ—-KY"),
    "å¯Œæ—-KY": ("1341", "å¯Œæ—-KY"),
    
    "å…«è²«": ("1342", "å…«è²«"),
    
    # 43é–‹é ­åŒ–å­¸ç”ŸæŠ€é†«ç™‚
    "ä¿¡ç«‹": ("4303", "ä¿¡ç«‹"),
    "ä¿¡ç«‹åŒ–å­¸": ("4303", "ä¿¡ç«‹"),
    "ä¿¡ç«‹åŒ–": ("4303", "ä¿¡ç«‹"),
    
    "å‹æ˜±": ("4304", "å‹æ˜±"),
    "å‹æ˜±ç²¾å¯†": ("4304", "å‹æ˜±"),
    "å‹æ˜±ç²¾": ("4304", "å‹æ˜±"),
    
    "ä¸–å¤": ("4305", "ä¸–å¤"),
    "ä¸–å¤ç§‘æŠ€": ("4305", "ä¸–å¤"),
    "ä¸–å¤ç§‘": ("4305", "ä¸–å¤"),
    
    "ç‚æ´²": ("4306", "ç‚æ´²"),
    "ç‚æ´²ç§‘æŠ€": ("4306", "ç‚æ´²"),
    "ç‚æ´²ç§‘": ("4306", "ç‚æ´²"),
    
    # 99é–‹é ­å…¶ä»–
    "è¬åœ‹é€š": ("9950", "è¬åœ‹é€š"),
    "è¬åœ‹é€šè·¯": ("9950", "è¬åœ‹é€š"),
}

# =============================================================================
# æ–‡æª”å…ƒæ•¸æ“šæå–å™¨
# =============================================================================

class DocumentMetadataExtractor:
    """æ–‡æª”å…ƒæ•¸æ“šæå–å™¨"""
    
    def __init__(self):
        # å…¬å¸åç¨±åŒ¹é…æ¨¡å¼
        self.company_patterns = [
            # é«˜å„ªå…ˆç´šï¼šåŒ…å«å®Œæ•´å ±å‘Šæ¨™é¡Œçš„æ¨¡å¼
            r'([^,\n\d]{2,25}?)(?:è‚¡ä»½)?æœ‰é™å…¬å¸\s*(202[0-9])\s*å¹´(?:åº¦)?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            r'([^,\n\d]{2,25}?)(?:è‚¡ä»½)?æœ‰é™å…¬å¸.*?(202[0-9]).*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            
            # ä¸­å„ªå…ˆç´šï¼šå…¬å¸åç¨±+å ±å‘Šé¡å‹
            r'([^,\n\d]{2,25}?)(?:è‚¡ä»½)?æœ‰é™å…¬å¸.*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            r'([^,\n\d]{2,25}?)å…¬å¸.*?(?:202[0-9]).*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            
            # ä½å„ªå…ˆç´šï¼šåƒ…å…¬å¸åç¨±
            r'([\u4e00-\u9fff]{2,15})(?:è‚¡ä»½)?æœ‰é™å…¬å¸',
            r'([\u4e00-\u9fff]{2,15})å…¬å¸(?:[^\u4e00-\u9fff]|$)',
        ]
        
        # å¹´åº¦åŒ¹é…æ¨¡å¼
        self.year_patterns = [
            # é«˜ç²¾ç¢ºåº¦ï¼šæ˜ç¢ºçš„å ±å‘Šå¹´åº¦è¡¨é”
            r'(202[0-9])\s*å¹´(?:åº¦)?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š(?:æ›¸)?',
            r'(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š(?:æ›¸)?.*?(202[0-9])\s*å¹´(?:åº¦)?',
            
            # ä¸­ç²¾ç¢ºåº¦ï¼šå ±å‘Šæ¨™é¡Œä¸­çš„å¹´åº¦
            r'(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š(?:æ›¸)?.*?(202[0-9])',
            r'(202[0-9]).*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š(?:æ›¸)?',
            
            # åŒ…å«"å¹´å ±"çš„æ¨¡å¼
            r'(202[0-9])\s*å¹´(?:åº¦)?å¹´å ±',
            r'å¹´å ±.*?(202[0-9])',
            
            # å…¶ä»–å¯èƒ½çš„å¹´åº¦è¡¨é”
            r'å ±å‘Š(?:æ›¸)?æœŸé–“.*?(202[0-9])',
            r'è²¡æ”¿å¹´åº¦.*?(202[0-9])',
            r'æœƒè¨ˆå¹´åº¦.*?(202[0-9])',
            
            # æœ€ä½å„ªå…ˆç´šï¼šä»»ä½•å››ä½æ•¸å¹´ä»½
            r'(202[0-9])',
        ]
    
    def extract_metadata(self, pdf_path: str) -> Dict[str, str]:
        """
        æå–æ–‡æª”å…ƒæ•¸æ“š
        
        Returns:
            DictåŒ…å« 'company_name' å’Œ 'report_year'
        """
        print(f"ğŸ“‹ æå–æ–‡æª”å…ƒæ•¸æ“š: {Path(pdf_path).name}")
        
        try:
            loader = PyPDFLoader(pdf_path)
            pages = loader.load()
            
            # æª¢æŸ¥å‰8é 
            text_for_extraction = ""
            pages_to_check = min(8, len(pages))
            
            for page in pages[:pages_to_check]:
                text_for_extraction += page.page_content + "\n"
            
            # å…ˆå˜—è©¦å¾æ–‡ä»¶åæå–ä½œç‚ºåƒè€ƒ
            filename_metadata = self._extract_from_filename(Path(pdf_path).name)
            
            # æå–å…¬å¸åç¨±å’Œå ±å‘Šå¹´åº¦
            company_name = self._extract_company_name(text_for_extraction, filename_metadata.get('company_name', ''))
            report_year = self._extract_report_year(text_for_extraction, filename_metadata.get('report_year', ''))
            
            # å¦‚æœä»ç„¡æ³•æå–åˆ°æœ‰æ•ˆä¿¡æ¯ï¼Œä½¿ç”¨æ–‡ä»¶åä½œç‚ºå‚™ç”¨
            if not company_name or company_name == "æœªçŸ¥å…¬å¸":
                company_name = filename_metadata.get('company_name', Path(pdf_path).stem)
            
            if not report_year or report_year == "æœªçŸ¥å¹´åº¦":
                report_year = filename_metadata.get('report_year', 'æœªçŸ¥å¹´åº¦')
            
            result = {
                'company_name': company_name,
                'report_year': report_year
            }
            
            print(f"âœ… æå–åˆ°ï¼š{company_name} - {report_year}")
            return result
            
        except Exception as e:
            print(f"âš ï¸ å…ƒæ•¸æ“šæå–å¤±æ•—: {e}")
            return {
                'company_name': Path(pdf_path).stem,
                'report_year': 'æœªçŸ¥å¹´åº¦'
            }
    
    def _extract_company_name(self, text: str, filename_hint: str = "") -> str:
        """æå–å…¬å¸åç¨±"""
        text_clean = re.sub(r'\s+', ' ', text[:3000])
        
        best_match = ""
        best_confidence = 0
        
        for i, pattern in enumerate(self.company_patterns):
            matches = re.findall(pattern, text_clean, re.IGNORECASE | re.MULTILINE)
            
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        company_candidate = match[0].strip()
                    else:
                        company_candidate = match.strip()
                    
                    # æ¸…ç†å…¬å¸åç¨±
                    company_candidate = self._clean_company_name(company_candidate)
                    
                    if self._is_valid_company_name(company_candidate):
                        # è¨ˆç®—ä¿¡å¿ƒåº¦
                        confidence = (len(self.company_patterns) - i) / len(self.company_patterns)
                        
                        # å¦‚æœèˆ‡æª”åæç¤ºåŒ¹é…ï¼ŒåŠ åˆ†
                        if filename_hint and filename_hint in company_candidate:
                            confidence += 0.2
                        
                        if confidence > best_confidence:
                            best_match = company_candidate
                            best_confidence = confidence
        
        return best_match if best_match else ""
    
    def _extract_report_year(self, text: str, filename_hint: str = "") -> str:
        """æå–å ±å‘Šå¹´åº¦"""
        text_clean = re.sub(r'\s+', ' ', text[:3000])
        
        best_year = ""
        best_confidence = 0
        
        for i, pattern in enumerate(self.year_patterns):
            matches = re.findall(pattern, text_clean, re.IGNORECASE | re.MULTILINE)
            
            if matches:
                for match in matches:
                    year_candidate = match if isinstance(match, str) else match[0]
                    
                    # é©—è­‰å¹´ä»½åˆç†æ€§
                    if self._is_valid_year(year_candidate):
                        # è¨ˆç®—ä¿¡å¿ƒåº¦
                        confidence = (len(self.year_patterns) - i) / len(self.year_patterns)
                        
                        # ç‰¹åˆ¥åŠ åˆ†ï¼šå¦‚æœæ˜¯æ˜ç¢ºçš„å ±å‘Šæ›¸å¹´åº¦è¡¨é”
                        if i < 4:
                            confidence += 0.3
                        
                        # å¦‚æœèˆ‡æª”åæç¤ºåŒ¹é…ï¼ŒåŠ åˆ†
                        if filename_hint and year_candidate == filename_hint:
                            confidence += 0.2
                        
                        # å„ªå…ˆé¸æ“‡è¼ƒæ–°çš„å¹´åº¦
                        if confidence > best_confidence or (confidence == best_confidence and int(year_candidate) > int(best_year or "0")):
                            best_year = year_candidate
                            best_confidence = confidence
        
        return best_year if best_year else ""
    
    def _clean_company_name(self, raw_name: str) -> str:
        """æ¸…ç†å…¬å¸åç¨±"""
        if not raw_name:
            return ""
        
        # å»é™¤å‰å¾Œçš„ç©ºç™½ã€æ•¸å­—ã€ç‰¹æ®Šç¬¦è™Ÿ
        cleaned = re.sub(r'^[\s\d\-\.ã€‚ï¼Œ,\(\)ï¼ˆï¼‰ã€ã€‘]+', '', raw_name)
        cleaned = re.sub(r'[\s\-\.ã€‚ï¼Œ,\(\)ï¼ˆï¼‰ã€ã€‘]+$', '', cleaned)
        
        # å»é™¤å¸¸è¦‹çš„ç„¡é—œè©å½™
        noise_words = [
            'å ±å‘Š', 'æ›¸', 'æ°¸çºŒ', 'ESG', 'ä¼æ¥­ç¤¾æœƒè²¬ä»»', 
            'ç¬¬', 'ç« ', 'ç¯€', 'é ', 'é™„éŒ„', 'ç›®éŒ„'
        ]
        for word in noise_words:
            cleaned = re.sub(f'^{word}', '', cleaned)
            cleaned = re.sub(f'{word}$', '', cleaned)
        
        return cleaned.strip()
    
    def _is_valid_company_name(self, name: str) -> bool:
        """é©—è­‰å…¬å¸åç¨±çš„æœ‰æ•ˆæ€§"""
        if not name or len(name) < 2 or len(name) > 25:
            return False
        
        # æ’é™¤æ˜é¡¯ä¸æ˜¯å…¬å¸åç¨±çš„è©å½™
        invalid_patterns = [
            r'^[0-9\.\-\s]+$',  # ç´”æ•¸å­—æˆ–ç¬¦è™Ÿ
            r'^[a-zA-Z\s]+$',   # ç´”è‹±æ–‡
            r'ç¬¬.*?ç« |ç¬¬.*?ç¯€|é .*?ç¢¼',  # ç« ç¯€é ç¢¼
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, name):
                return False
        
        return True
    
    def _is_valid_year(self, year: str) -> bool:
        """é©—è­‰å¹´ä»½çš„æœ‰æ•ˆæ€§"""
        try:
            year_int = int(year)
            return 2015 <= year_int <= 2030
        except ValueError:
            return False
    
    def _extract_from_filename(self, filename: str) -> Dict[str, str]:
        """å¾æ–‡ä»¶åæå–å…ƒæ•¸æ“šä½œç‚ºè¼”åŠ©ä¿¡æ¯"""
        result = {'company_name': '', 'report_year': ''}
        
        # æå–å¹´ä»½
        year_patterns = [
            r'(202[0-9])',
            r'(20[12][0-9])',
        ]
        
        for pattern in year_patterns:
            year_match = re.search(pattern, filename)
            if year_match:
                result['report_year'] = year_match.group(1)
                break
        
        # æå–å…¬å¸åç¨±
        company_part = filename
        
        # å»é™¤å‰¯æª”å
        company_part = re.sub(r'\.pdf$', '', company_part, flags=re.IGNORECASE)
        
        # å»é™¤å¹´ä»½
        company_part = re.sub(r'202[0-9]', '', company_part)
        company_part = re.sub(r'20[12][0-9]', '', company_part)
        
        # å»é™¤å¸¸è¦‹é—œéµè©
        keywords_to_remove = [
            'ESG', 'esg', 'æ°¸çºŒ', 'å ±å‘Š', 'æ›¸', 'ä¼æ¥­ç¤¾æœƒè²¬ä»»', 
            '_', '-', 'æå–', 'çµæœ'
        ]
        for keyword in keywords_to_remove:
            company_part = re.sub(keyword, '', company_part, flags=re.IGNORECASE)
        
        # æ¸…ç†å‰©é¤˜çš„ç¬¦è™Ÿå’Œç©ºç™½
        company_part = re.sub(r'[_\-\s]+', ' ', company_part).strip()
        
        if company_part and len(company_part) >= 2:
            result['company_name'] = company_part
        
        return result

# =============================================================================
# æª”åæ¨™æº–åŒ–åŠŸèƒ½
# =============================================================================

def smart_extract_company_from_filename(filename: str) -> List[str]:
    """æ™ºèƒ½å¾æª”åæå–å…¬å¸åç¨±å€™é¸"""
    candidates = []
    
    # ç§»é™¤å‰¯æª”åå’Œå¸¸è¦‹å¾Œç¶´
    name = filename.replace('.pdf', '').replace('.PDF', '')
    name = re.sub(r'_esgå ±å‘Šæ›¸.*$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'_2024.*$', '', name)
    name = re.sub(r'_2023.*$', '', name)
    name = re.sub(r'_2022.*$', '', name)
    
    # ç­–ç•¥1ï¼šæª¢æŸ¥æ˜¯å¦åŒ…å«å·²çŸ¥å…¬å¸åç¨±
    for company_name in COMPLETE_COMPANY_MAPPING.keys():
        if company_name in filename:
            candidates.append(company_name)
    
    # ç­–ç•¥2ï¼šåˆ†å‰²æª”åä¸¦æª¢æŸ¥æ¯å€‹éƒ¨åˆ†
    separators = ['_', '-', ' ', 'å¹´', 'esg', 'ESG', 'å ±å‘Š', 'æ›¸']
    parts = [name]
    
    for sep in separators:
        new_parts = []
        for part in parts:
            new_parts.extend(part.split(sep))
        parts = new_parts
    
    # æ¸…ç†å’Œéæ¿¾éƒ¨åˆ†
    clean_parts = []
    for part in parts:
        part = part.strip()
        if len(part) >= 2 and not part.isdigit() and not re.match(r'^202[0-9]$', part):
            clean_parts.append(part)
    
    # æª¢æŸ¥æ¯å€‹éƒ¨åˆ†æ˜¯å¦ç‚ºå·²çŸ¥å…¬å¸
    for part in clean_parts:
        for company_name in COMPLETE_COMPANY_MAPPING.keys():
            if company_name in part or part in company_name:
                if len(company_name) >= 2:
                    candidates.append(company_name)
    
    # ç­–ç•¥3ï¼šæª¢æŸ¥æª”åä¸­æ˜¯å¦åŒ…å«è‚¡ç¥¨ä»£è™Ÿ
    for company_name, (stock_code, standard_name) in COMPLETE_COMPANY_MAPPING.items():
        if stock_code and stock_code in filename:
            candidates.append(company_name)
    
    return list(set(candidates))  # å»é‡

def extract_year_from_filename(filename: str) -> str:
    """å¾æª”åæå–å¹´åº¦"""
    year_match = re.search(r'(202[0-9])', filename)
    return year_match.group(1) if year_match else ""

def standardize_pdf_filenames(data_path: str = None) -> Dict[str, str]:
    """
    æ¨™æº–åŒ–PDFæª”åç‚ºï¼šä»£è™Ÿ_å…¬å¸å_å¹´åº¦_esgå ±å‘Šæ›¸.pdf
    
    Returns:
        Dict: {åŸæª”å: æ–°æª”å} çš„æ˜ å°„
    """
    if data_path is None:
        data_path = DATA_PATH
    
    data_dir = Path(data_path)
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"âŒ åœ¨ {data_path} ç›®éŒ„ä¸­æ‰¾ä¸åˆ°PDFæ–‡ä»¶")
        return {}
    
    print(f"ğŸ“ é–‹å§‹æ¨™æº–åŒ– {len(pdf_files)} å€‹PDFæª”å...")
    print("ğŸ¯ ä½¿ç”¨æ™ºèƒ½æª”ååˆ†æ + PDFå…§å®¹æå–é›™é‡ç­–ç•¥")
    print("ğŸ“‹ æ”¯æ´å°ç£ä¸Šå¸‚æ«ƒå…¬å¸ä»£è™Ÿè­˜åˆ¥")
    print("=" * 60)
    
    metadata_extractor = DocumentMetadataExtractor()
    rename_mapping = {}
    
    for pdf_file in pdf_files:
        try:
            print(f"\nğŸ“„ è™•ç†: {pdf_file.name}")
            
            # ç­–ç•¥1ï¼šå¾æª”åæ™ºèƒ½æå–å…¬å¸
            filename_candidates = smart_extract_company_from_filename(pdf_file.name)
            print(f"   ğŸ“ æª”ååˆ†æå€™é¸: {filename_candidates}")
            
            # ç­–ç•¥2ï¼šå¾PDFå…§å®¹æå–
            try:
                metadata = metadata_extractor.extract_metadata(str(pdf_file))
                pdf_company = metadata['company_name']
                pdf_year = metadata['report_year']
                print(f"   ğŸ“„ PDFæå–: å…¬å¸={pdf_company}, å¹´åº¦={pdf_year}")
            except Exception as e:
                print(f"   âš ï¸ PDFæå–å¤±æ•—: {e}")
                pdf_company = "æœªçŸ¥å…¬å¸"
                pdf_year = ""
            
            # ç­–ç•¥3ï¼šå¾æª”åæå–å¹´åº¦
            filename_year = extract_year_from_filename(pdf_file.name)
            if filename_year:
                print(f"   ğŸ“… æª”åæå–å¹´åº¦: {filename_year}")
            
            # æ±ºå®šæœ€ä½³å…¬å¸åç¨±å’Œä»£è™Ÿ
            stock_code = ""
            standard_name = ""
            
            # å„ªå…ˆä½¿ç”¨æª”åå€™é¸ï¼ˆé€šå¸¸æ›´æº–ç¢ºï¼‰
            if filename_candidates:
                for candidate in filename_candidates:
                    if candidate in COMPLETE_COMPANY_MAPPING:
                        stock_code, standard_name = COMPLETE_COMPANY_MAPPING[candidate]
                        print(f"   âœ… æª”ååŒ¹é…: {stock_code} {standard_name}")
                        break
            
            # å¦‚æœæª”ååŒ¹é…å¤±æ•—ï¼Œå˜—è©¦PDFæå–çš„åç¨±
            if not stock_code and pdf_company != "æœªçŸ¥å…¬å¸":
                pdf_clean = pdf_company.strip()
                # ç§»é™¤å¸¸è¦‹å¾Œç¶´
                for suffix in ["è‚¡ä»½æœ‰é™å…¬å¸", "æœ‰é™å…¬å¸", "å…¬å¸", "å·¥æ¥­", "åŒ–å­¸", "å¡‘è† "]:
                    pdf_clean = pdf_clean.replace(suffix, "").strip()
                
                for company_name, (code, std_name) in COMPLETE_COMPANY_MAPPING.items():
                    if company_name in pdf_clean or pdf_clean in company_name:
                        stock_code = code
                        standard_name = std_name
                        print(f"   âœ… PDFåŒ¹é…: {stock_code} {standard_name}")
                        break
            
            # æ±ºå®šæœ€çµ‚ä½¿ç”¨çš„å¹´åº¦
            final_year = filename_year or pdf_year
            if final_year == "æœªçŸ¥å¹´åº¦":
                final_year = ""
            
            # ç”Ÿæˆæ–°æª”å
            if stock_code and standard_name:
                if final_year:
                    new_filename = f"{stock_code}_{standard_name}_{final_year}_esgå ±å‘Šæ›¸.pdf"
                else:
                    new_filename = f"{stock_code}_{standard_name}_esgå ±å‘Šæ›¸.pdf"
                print(f"   ğŸ¯ ç”Ÿæˆæ¨™æº–æª”å: {new_filename}")
            else:
                # ç„¡æ³•è­˜åˆ¥çš„æƒ…æ³ï¼Œä½¿ç”¨æ¸…ç†å¾Œçš„åŸæª”å
                if filename_candidates:
                    clean_name = filename_candidates[0]
                elif pdf_company != "æœªçŸ¥å…¬å¸":
                    clean_name = re.sub(r'[^\w\s-]', '', pdf_company)
                    clean_name = re.sub(r'\s+', '', clean_name)
                else:
                    # å¾åŸæª”åæå–åˆç†åç¨±
                    base_name = pdf_file.stem
                    base_name = re.sub(r'_esg.*$', '', base_name, flags=re.IGNORECASE)
                    base_name = re.sub(r'_202[0-9].*$', '', base_name)
                    clean_name = base_name[:20]  # é™åˆ¶é•·åº¦
                
                if final_year:
                    new_filename = f"{clean_name}_{final_year}_esgå ±å‘Šæ›¸.pdf"
                else:
                    new_filename = f"{clean_name}_esgå ±å‘Šæ›¸.pdf"
                
                print(f"   âš ï¸ æœªè­˜åˆ¥å…¬å¸ï¼Œä½¿ç”¨: {new_filename}")
            
            # æª¢æŸ¥æª”åæ˜¯å¦éœ€è¦æ›´æ”¹
            if pdf_file.name == new_filename:
                print(f"   âœ“ æª”åå·²æ¨™æº–åŒ–")
                continue
            
            # è™•ç†æª”åè¡çª
            new_path = data_dir / new_filename
            counter = 1
            original_new_filename = new_filename
            
            while new_path.exists():
                name_without_ext = original_new_filename.replace('.pdf', '')
                new_filename = f"{name_without_ext}_{counter}.pdf"
                new_path = data_dir / new_filename
                counter += 1
            
            if counter > 1:
                print(f"   ğŸ“ é¿å…æª”åè¡çªï¼Œä½¿ç”¨: {new_filename}")
            
            # åŸ·è¡Œé‡å‘½å
            pdf_file.rename(new_path)
            rename_mapping[pdf_file.name] = new_filename
            
            print(f"   âœ… é‡å‘½åæˆåŠŸ: {new_filename}")
            
        except Exception as e:
            print(f"   âŒ è™•ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nğŸ‰ æª”åæ¨™æº–åŒ–å®Œæˆï¼")
    print(f"âœ… æˆåŠŸé‡å‘½å: {len(rename_mapping)} å€‹æª”æ¡ˆ")
    
    if rename_mapping:
        print(f"\nğŸ“‹ é‡å‘½åæ¸…å–®:")
        for old_name, new_name in rename_mapping.items():
            print(f"   â€¢ {old_name}")
            print(f"     â†’ {new_name}")
    
    return rename_mapping

# =============================================================================
# åŸæœ‰çš„é è™•ç†åŠŸèƒ½
# =============================================================================

def preprocess_documents(pdf_path: str, output_db_path: str = None, metadata: Dict[str, str] = None):
    """é è™•ç†PDFæ–‡æª”ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«"""
    
    if output_db_path is None:
        output_db_path = VECTOR_DB_PATH
    
    print(f"é–‹å§‹è™•ç†PDF: {pdf_path}")
    
    # 1. è¼‰å…¥PDF
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°PDFæ–‡ä»¶: {pdf_path}")
    
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    print(f"æˆåŠŸè¼‰å…¥ {len(pages)} é ")
    
    # 2. ç‚ºæ¯å€‹æ–‡æª”æ·»åŠ å…ƒæ•¸æ“š
    if metadata:
        for page in pages:
            page.metadata.update(metadata)
            page.metadata['source_file'] = Path(pdf_path).name
            
            # æ·»åŠ é ç¢¼ä¿¡æ¯ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
            if 'page' not in page.metadata:
                page.metadata['page'] = pages.index(page) + 1
    
    # 3. æ–‡æœ¬åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=180,
        separators=["\n\n", "\n", ".", "ã€‚", "ï¼Œ", " ", ""]
    )
    
    print("æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
    chunks = text_splitter.split_documents(pages)
    print(f"åˆ†å‰²æˆ {len(chunks)} å€‹æ–‡æœ¬å¡Š")
    
    # 4. åˆå§‹åŒ–embeddingæ¨¡å‹
    print(f"è¼‰å…¥embeddingæ¨¡å‹: {EMBEDDING_MODEL}")
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}
    )
    
    # 5. å»ºç«‹å‘é‡è³‡æ–™åº«
    print("å»ºç«‹å‘é‡è³‡æ–™åº«...")
    db = FAISS.from_documents(chunks, embedding_model)
    
    # 6. ä¿å­˜è³‡æ–™åº«
    os.makedirs(os.path.dirname(output_db_path), exist_ok=True)
    db.save_local(output_db_path)
    print(f"å‘é‡è³‡æ–™åº«å·²ä¿å­˜åˆ°: {output_db_path}")
    
    return db

def preprocess_multiple_documents(pdf_paths: List[str]) -> Dict[str, Dict]:
    """
    æ‰¹é‡é è™•ç†å¤šå€‹PDFæ–‡æª”
    
    Returns:
        Dict: {pdf_path: {'db_path': str, 'metadata': dict}}
    """
    print(f"ğŸš€ é–‹å§‹æ‰¹é‡é è™•ç† {len(pdf_paths)} å€‹PDFæ–‡ä»¶")
    print("=" * 60)
    
    metadata_extractor = DocumentMetadataExtractor()
    results = {}
    
    for pdf_path in pdf_paths:
        try:
            print(f"\nğŸ“„ è™•ç†æ–‡ä»¶: {Path(pdf_path).name}")
            
            # 1. å…ƒæ•¸æ“šæå–
            metadata = metadata_extractor.extract_metadata(pdf_path)
            
            # 2. ç‚ºæ¯å€‹æ–‡ä»¶å‰µå»ºç¨ç«‹çš„å‘é‡è³‡æ–™åº«
            pdf_name = Path(pdf_path).stem
            db_path = os.path.join(
                os.path.dirname(VECTOR_DB_PATH),
                f"esg_db_{pdf_name}"
            )
            
            # 3. é è™•ç†æ–‡æª”
            preprocess_documents(pdf_path, db_path, metadata)
            
            results[pdf_path] = {
                'db_path': db_path,
                'metadata': metadata,
                'pdf_name': pdf_name
            }
            
            print(f"âœ… å®Œæˆ: {metadata['company_name']} - {metadata['report_year']}")
            
        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•— {Path(pdf_path).name}: {e}")
            continue
    
    print(f"\nğŸ‰ æ‰¹é‡é è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(pdf_paths)} å€‹æ–‡ä»¶")
    
    # é¡¯ç¤ºæå–çµæœæ‘˜è¦
    print(f"\nğŸ“‹ æå–æ‘˜è¦:")
    companies_years = {}
    for pdf_path, result in results.items():
        metadata = result['metadata']
        company = metadata['company_name']
        year = metadata['report_year']
        
        if company not in companies_years:
            companies_years[company] = []
        companies_years[company].append(year)
    
    for company, years in companies_years.items():
        years_str = ', '.join(sorted(set(years), reverse=True))
        print(f"   ğŸ¢ {company}: {years_str}")
    
    return results

# =============================================================================
# æ¸¬è©¦å’Œè¨ºæ–·åŠŸèƒ½
# =============================================================================

def test_company_mapping():
    """æ¸¬è©¦å…¬å¸æ˜ å°„è¡¨"""
    target_companies = ["ä¿¡ç«‹", "å‹æ˜±", "ä¸–å¤", "ç‚æ´²", "è¬åœ‹é€š", "å—äº", "å°å¡‘", "ä¸‰èŠ³"]
    
    print("ğŸ§ª æ¸¬è©¦å…¬å¸æ˜ å°„è¡¨")
    print("=" * 40)
    
    for company in target_companies:
        if company in COMPLETE_COMPANY_MAPPING:
            code, name = COMPLETE_COMPANY_MAPPING[company]
            print(f"âœ… {company} â†’ {code}_{name}_2024_esgå ±å‘Šæ›¸.pdf")
        else:
            print(f"âŒ {company} â†’ æœªæ‰¾åˆ°æ˜ å°„")
    
    print(f"\nğŸ“Š æ˜ å°„è¡¨çµ±è¨ˆ:")
    print(f"   ç¸½å…¬å¸æ•¸: {len(COMPLETE_COMPANY_MAPPING)}")
    print(f"   13é–‹é ­å¡‘è† å·¥æ¥­: {len([k for k, v in COMPLETE_COMPANY_MAPPING.items() if v[0].startswith('13')])}")
    print(f"   43é–‹é ­åŒ–å­¸ç”ŸæŠ€: {len([k for k, v in COMPLETE_COMPANY_MAPPING.items() if v[0].startswith('43')])}")
    print(f"   å…¶ä»–å…¬å¸: {len([k for k, v in COMPLETE_COMPANY_MAPPING.items() if not v[0].startswith(('13', '43'))])}")

def diagnose_filename_issues(data_path: str = None):
    """è¨ºæ–·æª”åå•é¡Œ"""
    if data_path is None:
        data_path = DATA_PATH
    
    data_dir = Path(data_path)
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"âŒ åœ¨ {data_path} ç›®éŒ„ä¸­æ‰¾ä¸åˆ°PDFæ–‡ä»¶")
        return
    
    print("ğŸ” è¨ºæ–·PDFæª”åå•é¡Œ")
    print("=" * 50)
    
    for pdf_file in pdf_files:
        print(f"\nğŸ“„ è¨ºæ–·: {pdf_file.name}")
        
        # æª”ååˆ†æ
        candidates = smart_extract_company_from_filename(pdf_file.name)
        year = extract_year_from_filename(pdf_file.name)
        
        print(f"   ğŸ“ æª”åå€™é¸å…¬å¸: {candidates}")
        print(f"   ğŸ“… æª”åæå–å¹´åº¦: {year}")
        
        # æª¢æŸ¥æ˜ å°„
        matched = False
        for candidate in candidates:
            if candidate in COMPLETE_COMPANY_MAPPING:
                code, name = COMPLETE_COMPANY_MAPPING[candidate]
                print(f"   âœ… åŒ¹é…: {candidate} â†’ {code} {name}")
                matched = True
                break
        
        if not matched:
            print(f"   âŒ ç„¡æ³•è‡ªå‹•åŒ¹é…")
            print(f"   ğŸ’¡ å»ºè­°æ‰‹å‹•æª¢æŸ¥å…¬å¸åç¨±æ˜¯å¦åœ¨æ˜ å°„è¡¨ä¸­")

def main():
    """ä¸»å‡½æ•¸"""
    # æª¢æŸ¥dataç›®éŒ„ä¸­çš„PDFæ–‡ä»¶
    data_dir = Path(DATA_PATH)
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"éŒ¯èª¤: åœ¨ {DATA_PATH} ç›®éŒ„ä¸­æ‰¾ä¸åˆ°PDFæ–‡ä»¶")
        print("è«‹å°‡ESGå ±å‘ŠPDFæ–‡ä»¶æ”¾å…¥dataç›®éŒ„ä¸­")
        return
    
    print(f"æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶:")
    for pdf_file in pdf_files:
        print(f"  - {pdf_file.name}")
    
    # åŸ·è¡Œé è™•ç†
    if len(pdf_files) == 1:
        # å–®æ–‡ä»¶æ¨¡å¼
        pdf_path = pdf_files[0]
        print(f"\nå–®æ–‡ä»¶æ¨¡å¼ï¼šè™•ç† {pdf_path.name}")
        
        try:
            metadata_extractor = DocumentMetadataExtractor()
            metadata = metadata_extractor.extract_metadata(str(pdf_path))
            preprocess_documents(str(pdf_path), metadata=metadata)
            print("âœ… é è™•ç†å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ é è™•ç†å¤±æ•—: {e}")
    else:
        # å¤šæ–‡ä»¶æ¨¡å¼
        print(f"\nå¤šæ–‡ä»¶æ¨¡å¼ï¼šè™•ç† {len(pdf_files)} å€‹æ–‡ä»¶")
        confirm = input("ç¢ºå®šè¦æ‰¹é‡è™•ç†æ‰€æœ‰æ–‡ä»¶å—ï¼Ÿ(y/n): ").strip().lower()
        
        if confirm == 'y':
            try:
                results = preprocess_multiple_documents([str(f) for f in pdf_files])
                print(f"âœ… æ‰¹é‡é è™•ç†å®Œæˆï¼")
                
            except Exception as e:
                print(f"âŒ æ‰¹é‡é è™•ç†å¤±æ•—: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--test":
            test_company_mapping()
        elif sys.argv[1] == "--diagnose":
            diagnose_filename_issues()
        elif sys.argv[1] == "--standardize":
            result = standardize_pdf_filenames()
            if result:
                print(f"âœ… æ¨™æº–åŒ–å®Œæˆï¼Œè™•ç†äº† {len(result)} å€‹æª”æ¡ˆ")
        else:
            print("ç”¨æ³•:")
            print("  python preprocess.py --test        # æ¸¬è©¦å…¬å¸æ˜ å°„è¡¨")
            print("  python preprocess.py --diagnose    # è¨ºæ–·æª”åå•é¡Œ")
            print("  python preprocess.py --standardize # åŸ·è¡Œæª”åæ¨™æº–åŒ–")
            print("  python preprocess.py              # åŸ·è¡Œé è™•ç†")
    else:
        # é è¨­åŸ·è¡Œæ¸¬è©¦
        test_company_mapping()