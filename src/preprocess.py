import os
import sys
import re
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from tqdm import tqdm
from typing import List, Tuple, Dict
sys.path.append(str(Path(__file__).parent))
from config import *

def extract_company_info_from_filename(pdf_path: str) -> Tuple[str, str]:
    """å¾PDFæ–‡ä»¶åæå–å…¬å¸åç¨±å’Œå¹´åº¦"""
    filename = Path(pdf_path).stem
    
    # å˜—è©¦åŒ¹é…å¹´åº¦ (2020-2030)
    year_match = re.search(r'(202[0-9])', filename)
    year = year_match.group(1) if year_match else "æœªçŸ¥å¹´åº¦"
    
    # æå–å…¬å¸åç¨± (ç§»é™¤å¹´åº¦å¾Œçš„å‰©é¤˜éƒ¨åˆ†)
    company_name = filename
    if year_match:
        company_name = re.sub(r'[_\-\s]*202[0-9][_\-\s]*', '', company_name)
    
    # æ¸…ç†å…¬å¸åç¨±
    company_name = re.sub(r'[_\-\s]+', ' ', company_name).strip()
    if not company_name:
        company_name = "æœªçŸ¥å…¬å¸"
    
    return company_name, year

def extract_company_info_from_content(pdf_path: str) -> Tuple[str, str]:
    """å¾PDFå…§å®¹æå–å…¬å¸åç¨±å’Œå¹´åº¦ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        
        if not pages:
            return "æœªçŸ¥å…¬å¸", "æœªçŸ¥å¹´åº¦"
        
        # æª¢æŸ¥å‰å¹¾é å…§å®¹
        first_pages_content = " ".join([page.page_content for page in pages[:3]])
        
        # å°‹æ‰¾å¹´åº¦
        year_patterns = [
            r'(202[0-9])\s*å¹´.*å ±å‘Š',
            r'(202[0-9])\s*å¹´.*æ°¸çºŒ',
            r'(202[0-9])\s*Annual\s*Report',
            r'(202[0-9])'
        ]
        
        year = "æœªçŸ¥å¹´åº¦"
        for pattern in year_patterns:
            match = re.search(pattern, first_pages_content)
            if match:
                year = match.group(1)
                break
        
        # å°‹æ‰¾å…¬å¸åç¨±
        company_patterns = [
            r'([^ã€‚\n]{2,20}(?:è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™å…¬å¸|å…¬å¸))',
            r'([^ã€‚\n]{2,20}(?:Corporation|Corp|Company|Ltd))',
        ]
        
        company = "æœªçŸ¥å…¬å¸"
        for pattern in company_patterns:
            matches = re.findall(pattern, first_pages_content)
            if matches:
                # é¸æ“‡æœ€çŸ­çš„åŒ¹é…ï¼ˆé€šå¸¸æ˜¯å…¬å¸ç°¡ç¨±ï¼‰
                company = min(matches, key=len).strip()
                break
        
        return company, year
        
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å¾å…§å®¹æå–å…¬å¸ä¿¡æ¯: {e}")
        return "æœªçŸ¥å…¬å¸", "æœªçŸ¥å¹´åº¦"

def get_pdf_company_info(pdf_path: str) -> Dict[str, str]:
    """ç²å–PDFçš„å…¬å¸ä¿¡æ¯"""
    # é¦–å…ˆå˜—è©¦å¾æ–‡ä»¶åæå–
    company_from_filename, year_from_filename = extract_company_info_from_filename(pdf_path)
    
    # å¦‚æœæ–‡ä»¶åæå–å¤±æ•—ï¼Œå˜—è©¦å¾å…§å®¹æå–
    if company_from_filename == "æœªçŸ¥å…¬å¸" or year_from_filename == "æœªçŸ¥å¹´åº¦":
        company_from_content, year_from_content = extract_company_info_from_content(pdf_path)
        
        # ä½¿ç”¨æ›´å¥½çš„çµæœ
        company = company_from_content if company_from_filename == "æœªçŸ¥å…¬å¸" else company_from_filename
        year = year_from_content if year_from_filename == "æœªçŸ¥å¹´åº¦" else year_from_filename
    else:
        company = company_from_filename
        year = year_from_filename
    
    return {
        'company_name': company,
        'report_year': year,
        'pdf_filename': Path(pdf_path).name
    }

def preprocess_single_document(pdf_path: str, output_db_path: str = None) -> Tuple[FAISS, Dict[str, str]]:
    """é è™•ç†å–®å€‹PDFæ–‡æª”ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«"""
    
    if output_db_path is None:
        # ç‚ºæ¯å€‹PDFå‰µå»ºç¨ç«‹çš„è³‡æ–™åº«è·¯å¾‘
        pdf_stem = Path(pdf_path).stem
        output_db_path = f"./vector_db/{pdf_stem}_db"
    
    print(f"ğŸ“„ é–‹å§‹è™•ç†PDF: {Path(pdf_path).name}")
    
    # æå–å…¬å¸ä¿¡æ¯
    company_info = get_pdf_company_info(pdf_path)
    print(f"ğŸ“Š è­˜åˆ¥å…¬å¸: {company_info['company_name']} ({company_info['report_year']})")
    
    # 1. è¼‰å…¥PDF
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°PDFæ–‡ä»¶: {pdf_path}")
    
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(pages)} é ")
    
    # åœ¨æ–‡æª”metadataä¸­åŠ å…¥å…¬å¸ä¿¡æ¯
    for page in pages:
        page.metadata.update(company_info)
    
    # 2. æ–‡æœ¬åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # å¢åŠ chunk sizeä»¥æ¸›å°‘ä¿¡æ¯ä¸Ÿå¤±
        chunk_overlap=200,  # å¢åŠ overlap
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
    )
    
    print("ğŸ”„ æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
    chunks = text_splitter.split_documents(pages)
    print(f"âœ… åˆ†å‰²æˆ {len(chunks)} å€‹æ–‡æœ¬å¡Š")
    
    # 3. åˆå§‹åŒ–embeddingæ¨¡å‹
    print(f"ğŸ§  è¼‰å…¥embeddingæ¨¡å‹: {EMBEDDING_MODEL}")
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}
    )
    
    # 4. å»ºç«‹å‘é‡è³‡æ–™åº«
    print("ğŸ” å»ºç«‹å‘é‡è³‡æ–™åº«...")
    db = FAISS.from_documents(chunks, embedding_model)
    
    # 5. ä¿å­˜è³‡æ–™åº«
    os.makedirs(os.path.dirname(output_db_path), exist_ok=True)
    db.save_local(output_db_path)
    print(f"ğŸ’¾ å‘é‡è³‡æ–™åº«å·²ä¿å­˜åˆ°: {output_db_path}")
    
    return db, company_info

def preprocess_multiple_documents(data_dir: str = None) -> List[Tuple[str, Dict[str, str]]]:
    """é è™•ç†å¤šå€‹PDFæ–‡æª”"""
    if data_dir is None:
        data_dir = DATA_PATH
    
    data_path = Path(data_dir)
    pdf_files = list(data_path.glob("*.pdf"))
    
    if not pdf_files:
        print(f"âŒ åœ¨ {data_dir} ç›®éŒ„ä¸­æ‰¾ä¸åˆ°PDFæ–‡ä»¶")
        return []
    
    print(f"ğŸ“š æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶")
    
    processed_files = []
    
    for pdf_path in pdf_files:
        try:
            print(f"\n{'='*60}")
            db, company_info = preprocess_single_document(str(pdf_path))
            
            # è¨˜éŒ„è™•ç†çµæœ
            db_path = f"./vector_db/{pdf_path.stem}_db"
            processed_files.append((db_path, company_info))
            
            print(f"âœ… {pdf_path.name} è™•ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ è™•ç† {pdf_path.name} å¤±æ•—: {e}")
            continue
    
    print(f"\nğŸ‰ æ‰¹é‡é è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(processed_files)}/{len(pdf_files)} å€‹æ–‡ä»¶")
    return processed_files

def preprocess_documents(pdf_path: str, output_db_path: str = None):
    """é è™•ç†PDFæ–‡æª”ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
    db, company_info = preprocess_single_document(pdf_path, output_db_path)
    return db

def main():
    """ä¸»å‡½æ•¸"""
    # æª¢æŸ¥dataç›®éŒ„ä¸­çš„PDFæ–‡ä»¶
    data_dir = Path(DATA_PATH)
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"âŒ åœ¨ {DATA_PATH} ç›®éŒ„ä¸­æ‰¾ä¸åˆ°PDFæ–‡ä»¶")
        print("è«‹å°‡ESGå ±å‘ŠPDFæ–‡ä»¶æ”¾å…¥dataç›®éŒ„ä¸­")
        return
    
    if len(pdf_files) == 1:
        # å–®å€‹æ–‡ä»¶è™•ç†
        pdf_path = pdf_files[0]
        print(f"ğŸ¯ è™•ç†å–®å€‹æ–‡ä»¶: {pdf_path}")
        
        try:
            preprocess_documents(str(pdf_path))
            print("âœ… é è™•ç†å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ é è™•ç†å¤±æ•—: {e}")
    else:
        # å¤šå€‹æ–‡ä»¶æ‰¹é‡è™•ç†
        print(f"ğŸ¯ æ‰¹é‡è™•ç† {len(pdf_files)} å€‹æ–‡ä»¶")
        
        try:
            processed_files = preprocess_multiple_documents()
            if processed_files:
                print("âœ… æ‰¹é‡é è™•ç†å®Œæˆï¼")
                print("\nğŸ“‹ è™•ç†çµæœ:")
                for db_path, company_info in processed_files:
                    print(f"   ğŸ“ {company_info['company_name']} ({company_info['report_year']})")
            else:
                print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æ–‡ä»¶")
        except Exception as e:
            print(f"âŒ æ‰¹é‡é è™•ç†å¤±æ•—: {e}")

if __name__ == "__main__":
    main()