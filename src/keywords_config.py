# =============================================================================
# å¢å¼·ç‰ˆé—œéµå­—é…ç½® v2.0 - ç²¾ç¢ºæ’é™¤ä¸ç›¸é—œå…§å®¹
# =============================================================================

import re
from typing import List, Dict, Tuple, Union

class EnhancedKeywordConfig:
    """å¢å¼·çš„é—œéµå­—é…ç½®ï¼Œç²¾ç¢ºæ’é™¤ä¸ç›¸é—œä¸»é¡Œ"""
    
    # æ ¸å¿ƒå†ç”Ÿå¡‘è† é—œéµå­—ï¼ˆå¿…é ˆèˆ‡å¯¦éš›ç”Ÿç”¢/ä½¿ç”¨ç›¸é—œï¼‰
    CORE_RECYCLED_PLASTIC_KEYWORDS = {
        "é«˜ç›¸é—œé€£çºŒé—œéµå­—": [
            "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”Ÿpp",
            "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™", 
            "rPET", "å›æ”¶å¡‘è† ç²’", "å›æ”¶å¡‘æ–™ç²’", 
            "å†ç”Ÿèšé…¯", "å†ç”Ÿå°¼é¾", "å›æ”¶èšé…¯"
        ],
        
        "é«˜ç›¸é—œä¸é€£çºŒé—œéµå­—": [
            ("å†ç”Ÿ", "å¡‘è† ", "ææ–™"), ("å†ç”Ÿ", "å¡‘æ–™", "ç”¢å“"),
            ("PP", "å†ç”Ÿ", "ç”¢èƒ½"), ("å¡‘è† ", "å›æ”¶", "é€ ç²’"),
            ("å›æ”¶", "å¡‘è† ", "è£½å“"), ("PCR", "å«é‡"),
            ("å†ç”Ÿ", "å¡‘è† ", "æ¯”ä¾‹"), ("MLCC", "å›æ”¶", "ç”¢èƒ½"),
            ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("PET", "å›æ”¶"), ("å›æ”¶", "èšé…¯")
        ]
    }
    
    # å¼·åŒ–æ’é™¤æ¨¡å¼ - åŸºæ–¼å¯¦éš›èª¤æå–æ¡ˆä¾‹
    EXCLUSION_PATTERNS = {
        # å®Œå…¨æ’é™¤çš„ä¸»é¡Œé ˜åŸŸ
        "æ’é™¤ä¸»é¡Œ": [
            # æ´»å‹•å’Œè³½äº‹
            "è³½äº‹", "é¦¬æ‹‰æ¾", "æ¯”è³½", "æ´»å‹•", "ç››æœƒ", "é¸æ‰‹", "åƒè³½",
            "è³½è¡£", "é‹å‹•", "é«”è‚²", "ç«¶è³½", "å„ç•Œå¥½æ‰‹",
            
            # è·æ¥­å®‰å…¨å’ŒäººåŠ›
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "å“¡å·¥å‚·äº¡", "è·ç½", 
            "å·¥å‚·", "äº‹æ•…ç‡", "ç½å®³æ¯”ç‡", "å®‰å…¨çµ±è¨ˆ",
            
            # æ°´è³‡æºå’Œæ°£è±¡
            "é™é›¨é‡", "é›¨æ°´", "è‡ªä¾†æ°´", "ç”¨æ°´é‡", "æ°´è³‡æº", 
            "åœ°ä¸‹æ°´", "åœŸå£¤", "æ°´è³ª", "æ°£è±¡", "å¤©æ°£",
            
            # å…¬å¸æ²»ç†å’Œè²¡å‹™
            "å…¬å¸æ²»ç†", "è‘£äº‹æœƒ", "è‚¡æ±", "è‚¡åƒ¹", "è²¡å‹™", 
            "ç‡Ÿæ”¶", "ç²åˆ©", "è–ªè³‡", "è–ªé…¬", "ç¦åˆ©", "äººäº‹",
            
            # ç¯€èƒ½ç¯€æ°´æ”¹å–„æ¡ˆ
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ",
            "ç¯€èƒ½æ¡ˆä¾‹", "ç¯€æ°´æ¡ˆä¾‹", "å„ªè‰¯æ¡ˆä¾‹",
            
            # ç’°å¢ƒç›£æ¸¬
            "ç›£æ¸¬", "æª¢æ¸¬", "æ¸¬è©¦", "åˆ†æ", "åŒ–é©—", "æª¢é©—é »ç‡",
            "ç›£æ¸¬çµæœ", "æª¢æ¸¬ç®¡ç†", "å“è³ªç›£æ§"
        ],
        
        # æ’é™¤çš„å…·é«”ä¸Šä¸‹æ–‡ç‰‡æ®µ
        "æ’é™¤ä¸Šä¸‹æ–‡": [
            # è³½äº‹ç›¸é—œ
            "å‚ç›´é¦¬æ‹‰æ¾", "å°åŒ—101", "ç››å¤§è³½äº‹", "å²ä¸Šæœ€ç’°ä¿è³½è¡£",
            "åƒèˆ‡ç››æœƒ", "å„ç•Œå¥½æ‰‹", "é‹å‹•ç‹€æ…‹", "é¸æ‰‹å€‘",
            
            # è·æ¥­ç½å®³ç›¸é—œ
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å‚·çµ±è¨ˆ", "å®‰å…¨äº‹æ•…", "ç½å®³äººæ•¸",
            "è·ç½æ¯”ç‡", "äº‹æ•…çµ±è¨ˆ", "å®‰å…¨æŒ‡æ¨™",
            
            # æ°´è³‡æºç›¸é—œ
            "æœˆå¹³å‡é™é›¨é‡", "é›¨æ°´å›æ”¶é‡", "ç”¨æ°´é‡æ¸›å°‘", "ç¯€ç´„ç”¨æ°´",
            "åœ°ä¸‹æ°´ç›£æ¸¬", "åœŸå£¤å“è³ª", "æ°´è³ªç®¡ç†", "å»¢æ°´è™•ç†",
            
            # æ”¹å–„æ¡ˆç›¸é—œ
            "å®Œæˆ440ä»¶ç¯€èƒ½æ”¹å–„æ¡ˆ", "å®Œæˆ42ä»¶ç¯€æ°´æ”¹å–„æ¡ˆ", 
            "ç¯€èƒ½æ¸›æ’", "CO2æ¸›é‡ç´„", "æŠ•è³‡æ•ˆç›Š", "æ”¹å–„å°ˆæ¡ˆ",
            
            # æ²»ç†å’Œå ±å‘Šç›¸é—œ
            "å…¬å¸æ²»ç†è©•é‘‘", "ESGæŒ‡æ¨™", "æ°¸çºŒå ±å‘Š", "å¹´å ±",
            "è³‡è¨Šæ­éœ²", "å°ç…§è¡¨", "æŒ‡æ¨™é …ç›®",
            
            # æŠ€è¡“æ‡‰ç”¨ç‡ï¼ˆéç”¢èƒ½ï¼‰
            "æŠ€è¡“æ‡‰ç”¨åœ¨æ—¢æœ‰", "æ‡‰ç”¨ç‡å·²é”", "æŠ€è¡“æ§åˆ¶",
            "BPA-ClearæŠ€è¡“", "é›™é…šAé™¤æ§"
        ],
        
        # æ’é™¤çš„æ•¸å€¼æ¨¡å¼ï¼ˆé€™äº›æ•¸å€¼é€šå¸¸ä¸ç›¸é—œï¼‰
        "æ’é™¤æ•¸å€¼æ¨¡å¼": [
            # ç½å®³å’Œäº‹æ•…ç›¸é—œæ•¸å€¼
            r'è·æ¥­ç½å®³.*?\d+(?:\.\d+)?%',
            r'å·¥å®‰.*?\d+(?:\.\d+)?',
            r'äº‹æ•….*?\d+(?:\.\d+)?',
            
            # é™é›¨å’Œæ°´è³‡æºç›¸é—œæ•¸å€¼
            r'é™é›¨é‡.*?\d+(?:\.\d+)?%',
            r'é›¨æ°´.*?\d+(?:\.\d+)?å™¸',
            r'ç”¨æ°´é‡.*?\d+(?:\.\d+)?',
            
            # æ”¹å–„æ¡ˆæ•¸é‡ï¼ˆä»¶æ•¸ï¼‰
            r'\d+\s*ä»¶.*?æ”¹å–„æ¡ˆ',
            r'æ”¹å–„æ¡ˆ.*?\d+\s*ä»¶',
            r'æ¡ˆä¾‹.*?\d+\s*ä»¶',
            
            # æŠ€è¡“æ‡‰ç”¨ç‡
            r'æ‡‰ç”¨ç‡.*?\d+(?:\.\d+)?%',
            r'æŠ€è¡“.*?\d+(?:\.\d+)?%.*?æ‡‰ç”¨',
            
            # ç›£æ¸¬é »ç‡
            r'\d+\s*æ¬¡.*?ç›£æ¸¬',
            r'ç›£æ¸¬.*?\d+\s*æ¬¡',
            r'æª¢æ¸¬.*?\d+\s*æ¬¡'
        ]
    }
    
    # å¿…é ˆåŒ…å«çš„ç›¸é—œè©å½™ï¼ˆæ›´åš´æ ¼ï¼‰
    REQUIRED_CONTEXT_WORDS = {
        "ç”Ÿç”¢è£½é€ ç›¸é—œ": [
            "ç”Ÿç”¢", "è£½é€ ", "ç”¢èƒ½", "ç”¢é‡", "è£½ç¨‹", "å·¥å» ",
            "ç”¢ç·š", "é‡ç”¢", "ç”Ÿç”¢ç·š", "è£½é€ å•†"
        ],
        
        "ææ–™å’Œç”¢å“ç›¸é—œ": [
            "ææ–™", "ç”¢å“", "è£½å“", "åŸæ–™", "ç²’å­", "é¡†ç²’",
            "çº–ç¶­", "è†œ", "ç‰‡æ", "æ¿æ", "å®¹å™¨"
        ],
        
        "æ•¸é‡å’Œæ¯”ä¾‹ç›¸é—œ": [
            "å«é‡", "æ¯”ä¾‹", "ä½¿ç”¨", "æ‡‰ç”¨", "æ·»åŠ ", "æ‘»é…",
            "å™¸", "å…¬æ–¤", "è¬å™¸", "åƒå™¸", "kg", "KG"
        ],
        
        "ç’°ä¿æ•ˆç›Šç›¸é—œ": [
            "æ¸›ç¢³", "ç¢³è¶³è·¡", "ç¯€èƒ½", "ç’°ä¿", "æ°¸çºŒ", "å¾ªç’°",
            "ç¶ è‰²", "ä½ç¢³", "ç¢³æ’æ”¾", "æ¸›æ’"
        ]
    }

class AdvancedMatcher:
    """é«˜ç´šåŒ¹é…å™¨ï¼Œå¯¦ç¾ç²¾ç¢ºçš„ç›¸é—œæ€§åˆ¤æ–·"""
    
    def __init__(self):
        self.config = EnhancedKeywordConfig()
        self.relevance_threshold = 0.75  # æé«˜ç›¸é—œæ€§é–¾å€¼
    
    def comprehensive_relevance_check(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """
        å…¨é¢çš„ç›¸é—œæ€§æª¢æŸ¥
        
        Returns:
            Tuple[æ˜¯å¦ç›¸é—œ, ç›¸é—œæ€§åˆ†æ•¸, è©³ç´°èªªæ˜]
        """
        text_lower = text.lower()
        
        # ç¬¬1æ­¥ï¼šå¼·æ’é™¤æª¢æŸ¥
        exclusion_score = self._check_strong_exclusions(text_lower)
        if exclusion_score > 0.5:
            return False, 0.0, f"å¼·æ’é™¤åŒ¹é…: {exclusion_score:.2f}"
        
        # ç¬¬2æ­¥ï¼šé—œéµå­—åŒ¹é…æª¢æŸ¥
        keyword_match, keyword_confidence, keyword_details = self._match_keyword_basic(text, keyword)
        if not keyword_match:
            return False, 0.0, "é—œéµå­—ä¸åŒ¹é…"
        
        # ç¬¬3æ­¥ï¼šä¸Šä¸‹æ–‡ç›¸é—œæ€§æª¢æŸ¥
        context_score = self._check_context_relevance(text_lower)
        if context_score < 0.3:
            return False, 0.0, f"ä¸Šä¸‹æ–‡ç›¸é—œæ€§ä¸è¶³: {context_score:.2f}"
        
        # ç¬¬4æ­¥ï¼šæ•¸å€¼ç›¸é—œæ€§æª¢æŸ¥
        value_relevance = self._check_value_context_relevance(text_lower)
        
        # ç¬¬5æ­¥ï¼šç”Ÿç”¢è£½é€ ç›¸é—œæ€§æª¢æŸ¥
        production_relevance = self._check_production_context(text_lower)
        
        # è¨ˆç®—ç¶œåˆç›¸é—œæ€§åˆ†æ•¸
        final_score = (
            keyword_confidence * 0.25 +      # é—œéµå­—åŒ¹é… 25%
            context_score * 0.30 +           # ä¸Šä¸‹æ–‡ç›¸é—œæ€§ 30%
            value_relevance * 0.25 +         # æ•¸å€¼ç›¸é—œæ€§ 25%
            production_relevance * 0.20      # ç”Ÿç”¢ç›¸é—œæ€§ 20%
        )
        
        # æ‡‰ç”¨æ’é™¤æ‡²ç½°
        final_score = final_score * (1 - exclusion_score * 0.8)
        
        is_relevant = final_score > self.relevance_threshold
        
        details = (f"é—œéµå­—: {keyword_confidence:.2f}, "
                  f"ä¸Šä¸‹æ–‡: {context_score:.2f}, "
                  f"æ•¸å€¼: {value_relevance:.2f}, "
                  f"ç”Ÿç”¢: {production_relevance:.2f}, "
                  f"æ’é™¤: {exclusion_score:.2f}")
        
        return is_relevant, final_score, details
    
    def _check_strong_exclusions(self, text: str) -> float:
        """æª¢æŸ¥å¼·æ’é™¤æ¨¡å¼"""
        exclusion_score = 0.0
        
        # æª¢æŸ¥æ’é™¤ä¸»é¡Œ
        for exclusion_topic in self.config.EXCLUSION_PATTERNS["æ’é™¤ä¸»é¡Œ"]:
            if exclusion_topic in text:
                exclusion_score += 0.15  # æ¯å€‹ä¸»é¡Œå¢åŠ 0.15åˆ†
        
        # æª¢æŸ¥æ’é™¤ä¸Šä¸‹æ–‡ï¼ˆæ¬Šé‡æ›´é«˜ï¼‰
        for exclusion_context in self.config.EXCLUSION_PATTERNS["æ’é™¤ä¸Šä¸‹æ–‡"]:
            if exclusion_context in text:
                exclusion_score += 0.25  # æ¯å€‹ä¸Šä¸‹æ–‡å¢åŠ 0.25åˆ†
        
        # æª¢æŸ¥æ’é™¤æ•¸å€¼æ¨¡å¼
        for pattern in self.config.EXCLUSION_PATTERNS["æ’é™¤æ•¸å€¼æ¨¡å¼"]:
            if re.search(pattern, text, re.IGNORECASE):
                exclusion_score += 0.20  # æ¯å€‹æ¨¡å¼å¢åŠ 0.20åˆ†
        
        return min(exclusion_score, 1.0)
    
    def _check_context_relevance(self, text: str) -> float:
        """æª¢æŸ¥ä¸Šä¸‹æ–‡ç›¸é—œæ€§"""
        total_score = 0.0
        category_count = 0
        
        for category, words in self.config.REQUIRED_CONTEXT_WORDS.items():
            category_score = 0.0
            found_words = 0
            
            for word in words:
                if word in text:
                    found_words += 1
            
            if found_words > 0:
                category_score = min(found_words / len(words), 1.0)
                total_score += category_score
                category_count += 1
        
        if category_count == 0:
            return 0.0
        
        # éœ€è¦è‡³å°‘2å€‹é¡åˆ¥æœ‰ç›¸é—œè©å½™
        if category_count < 2:
            return total_score * 0.5
        
        return total_score / 4.0  # å¹³å‡åˆ†æ•¸
    
    def _check_value_context_relevance(self, text: str) -> float:
        """æª¢æŸ¥æ•¸å€¼èˆ‡å†ç”Ÿå¡‘è† çš„ä¸Šä¸‹æ–‡ç›¸é—œæ€§"""
        # å°‹æ‰¾æ•¸å€¼
        numbers = re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:å™¸|kg|%|ï¼…|è¬å™¸|åƒå™¸))', text)
        
        if not numbers:
            return 0.0
        
        relevance_score = 0.0
        
        for number in numbers:
            # æª¢æŸ¥æ•¸å€¼å‘¨åœçš„ä¸Šä¸‹æ–‡
            number_pos = text.find(number)
            if number_pos == -1:
                continue
            
            # æª¢æŸ¥å‰å¾Œ100å­—ç¬¦çš„ä¸Šä¸‹æ–‡
            start_pos = max(0, number_pos - 100)
            end_pos = min(len(text), number_pos + len(number) + 100)
            context_window = text[start_pos:end_pos]
            
            # æª¢æŸ¥å†ç”Ÿå¡‘è† ç›¸é—œè©å½™
            recycling_words = [
                "å†ç”Ÿ", "å›æ”¶", "PCR", "å¾ªç’°", "ç’°ä¿", "æ°¸çºŒ",
                "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "ææ–™"
            ]
            
            found_relevant = sum(1 for word in recycling_words if word in context_window)
            
            # æª¢æŸ¥ç”Ÿç”¢ç›¸é—œè©å½™
            production_words = [
                "ç”Ÿç”¢", "è£½é€ ", "ç”¢èƒ½", "ç”¢é‡", "ä½¿ç”¨", "æ‡‰ç”¨", "å«é‡"
            ]
            
            found_production = sum(1 for word in production_words if word in context_window)
            
            if found_relevant >= 2 and found_production >= 1:
                relevance_score += 0.4
            elif found_relevant >= 1:
                relevance_score += 0.2
        
        return min(relevance_score, 1.0)
    
    def _check_production_context(self, text: str) -> float:
        """æª¢æŸ¥æ˜¯å¦èˆ‡å¯¦éš›ç”Ÿç”¢è£½é€ ç›¸é—œ"""
        production_indicators = [
            "ç”¢èƒ½", "ç”¢é‡", "ç”Ÿç”¢", "è£½é€ ", "å·¥å» ", "ç”¢ç·š",
            "é‡ç”¢", "è£½ç¨‹", "åŠ å·¥", "ç”Ÿç”¢ç·š"
        ]
        
        application_indicators = [
            "ä½¿ç”¨", "æ‡‰ç”¨", "æ·»åŠ ", "å«é‡", "æ¯”ä¾‹", "æ‘»é…",
            "è£½æˆ", "ç”¨æ–¼", "æ‡‰ç”¨æ–¼"
        ]
        
        found_production = sum(1 for word in production_indicators if word in text)
        found_application = sum(1 for word in application_indicators if word in text)
        
        # å¿…é ˆæœ‰ç”Ÿç”¢æˆ–æ‡‰ç”¨çš„æ˜ç¢ºæŒ‡æ¨™
        if found_production > 0:
            return 0.8
        elif found_application > 0:
            return 0.6
        else:
            return 0.0
    
    def _match_keyword_basic(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """åŸºç¤é—œéµå­—åŒ¹é…"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            if keyword.lower() in text_lower:
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {keyword}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            positions = []
            
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            distance = max(positions) - min(positions)
            
            if distance <= 50:
                return True, 0.9, f"è¿‘è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 100:
                return True, 0.8, f"ä¸­è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 200:
                return True, 0.7, f"é è·é›¢åŒ¹é…({distance}å­—)"
            else:
                return True, 0.5, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—)"
        
        return False, 0.0, ""

def advanced_filtering_pipeline(text_content: str, keywords: list) -> Tuple[bool, List[Dict]]:
    """
    é«˜ç´šéæ¿¾ç®¡é“ - ç²¾ç¢ºæ’é™¤ä¸ç›¸é—œå…§å®¹
    """
    matcher = AdvancedMatcher()
    passed_matches = []
    
    for keyword in keywords:
        is_relevant, relevance_score, details = matcher.comprehensive_relevance_check(text_content, keyword)
        
        if is_relevant and relevance_score > 0.75:  # é«˜é–¾å€¼
            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
            passed_matches.append({
                'keyword': keyword_str,
                'original_keyword': keyword,
                'relevance_score': relevance_score,
                'match_details': details,
                'match_type': 'continuous' if isinstance(keyword, str) else 'discontinuous'
            })
    
    return len(passed_matches) > 0, passed_matches

# ç‚ºäº†èˆ‡ç¾æœ‰ç³»çµ±å…¼å®¹
def enhanced_filtering_pipeline(text_content: str, keywords: list) -> Tuple[bool, List[Dict]]:
    """èˆ‡ç¾æœ‰ç³»çµ±å…¼å®¹çš„ä»‹é¢"""
    return advanced_filtering_pipeline(text_content, keywords)

# ä¿æŒèˆ‡ç¾æœ‰ç³»çµ±çš„å…¼å®¹æ€§
class KeywordConfig:
    @classmethod
    def get_all_keywords(cls):
        config = EnhancedKeywordConfig()
        all_keywords = []
        all_keywords.extend(config.CORE_RECYCLED_PLASTIC_KEYWORDS["é«˜ç›¸é—œé€£çºŒé—œéµå­—"])
        all_keywords.extend(config.CORE_RECYCLED_PLASTIC_KEYWORDS["é«˜ç›¸é—œä¸é€£çºŒé—œéµå­—"])
        return all_keywords

class EnhancedMatcher:
    """ä¿æŒå‘å¾Œå…¼å®¹çš„åŒ¹é…å™¨"""
    def __init__(self):
        self.advanced_matcher = AdvancedMatcher()
    
    def extract_numbers_and_percentages(self, text: str):
        """æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”"""
        numbers = re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:è¬|åƒ)?(?:å™¸|kg|KG|å…¬æ–¤|ä»¶))', text)
        percentages = re.findall(r'\d+(?:\.\d+)?(?:\s*%|ï¼…)', text)
        return numbers, percentages

# =============================================================================
# æ¸¬è©¦å‡½æ•¸
# =============================================================================

def test_enhanced_filtering_v2():
    """æ¸¬è©¦å¢å¼·éæ¿¾åŠŸèƒ½v2.0"""
    
    # åŸºæ–¼å¯¦éš›èª¤æå–æ¡ˆä¾‹çš„æ¸¬è©¦
    test_cases = [
        {
            "name": "âŒ è³½äº‹æ¡ˆä¾‹ - æ‡‰è©²è¢«æ’é™¤",
            "text": """å—äºå…¬å¸æ——ä¸‹ç’°ä¿å†ç”Ÿå“ç‰Œã€ŒSAYA é¤˜å‰µã€ï¼Œä»¥SAYAã€ŒRscuw ç¹”ç‰©å›æ”¶çµ²ã€ç‚ºç½é•ä¸‰å¹´é‡å•Ÿçš„ç››å¤§è³½äº‹ã€Œå‚ç›´é¦¬æ‹‰æ¾ã€æ‰“é€ å²ä¸Šæœ€ç’°ä¿è³½è¡£ï¼Œé è¨ˆæä¾› 3,500 ä»¶çµ¦åƒèˆ‡ç››æœƒçš„å„ç•Œå¥½æ‰‹ã€‚""",
            "expected": False
        },
        {
            "name": "âŒ è·æ¥­ç½å®³æ¡ˆä¾‹ - æ‡‰è©²è¢«æ’é™¤", 
            "text": """è·æ¥­ç½å®³æ¯”ç‡ é‡åŒ– 0.035% æ¯”ç‡ (%) äº” å¡‘è† è£½å“ç”¢å€¼ é‡åŒ– 31,116,351 æ–°å°å¹£åƒå…ƒ""",
            "expected": False
        },
        {
            "name": "âŒ é›¨æ°´å›æ”¶æ¡ˆä¾‹ - æ‡‰è©²è¢«æ’é™¤",
            "text": """2023 å¹´å› æœˆå¹³å‡é™é›¨é‡æ¸›å°‘ 18%ï¼Œè‡´é›¨æ°´å›æ”¶é‡æ¸›å°‘ 249 å™¸ / æ—¥ï¼Œä½†é›¨æ°´å›æ”¶ç‡æé«˜ 2.5%ã€‚""",
            "expected": False
        },
        {
            "name": "âŒ ç¯€èƒ½æ”¹å–„æ¡ˆæ¡ˆä¾‹ - æ‡‰è©²è¢«æ’é™¤",
            "text": """æ¨å‹•ç¯€èƒ½æ”¹å–„æ¡ˆï¼š 2023 å¹´å®Œæˆ 440 ä»¶ç¯€èƒ½æ”¹å–„æ¡ˆï¼Œ CO2 æ¸›é‡ç´„ 126,153 å™¸ / å¹´ã€‚æé«˜å†ç”Ÿæ–™ä½¿ç”¨æ¯”ä¾‹ï¼Œé™ä½åŸæ–™ç«¯ç¢³æ’æ”¾é‡ã€‚""",
            "expected": False
        },
        {
            "name": "âŒ æŠ€è¡“æ‡‰ç”¨ç‡æ¡ˆä¾‹ - æ‡‰è©²è¢«æ’é™¤",
            "text": """ç›®å‰ BPA-Clear æŠ€è¡“æ‡‰ç”¨åœ¨æ—¢æœ‰èšé…¯å›æ”¶ç”¢èƒ½æ‡‰ç”¨ç‡å·²é” 50ï¼…ï¼Œç›®æ¨™æ˜å¹´é€²ä¸€æ­¥æ‹‰åˆ° 80ï¼…ï¼Œå¼·åŒ–å›æ”¶ç”¢èƒ½å“è³ªå‡ç´šã€‚""",
            "expected": False
        },
        {
            "name": "âœ… çœŸæ­£ç›¸é—œæ¡ˆä¾‹1 - æ‡‰è©²é€šé",
            "text": """PET å·¥æ¥­ææ–™å›æ”¶ï¼Œå†è£½çš„ PET é…¯ç²’ï¼Œè¼ƒä¸€èˆ¬å‚³çµ±çŸ³åŒ–è£½ç¨‹çš„ PET é…¯ç²’ï¼ŒCO2 æ’æ”¾é‡æ¸›é‡æ•ˆæœæ˜é¡¯ï¼Œå¯æ¸›é‡ç´„86.2~95.67%ï¼Œ2023 å¹´æ¸›ç¢³ 814.2 å™¸""",
            "expected": True
        },
        {
            "name": "âœ… çœŸæ­£ç›¸é—œæ¡ˆä¾‹2 - æ‡‰è©²é€šé", 
            "text": """å¯¶ç‰¹ç“¶å›æ”¶é€ ç²’å¾Œå–ä»£åŸç”Ÿèšé…¯ç²’è¼ƒåŸè£½ç¨‹ç”Ÿç”¢ä¹‹ç¢³æ’æ”¾é‡å¯æ¸›å°‘72%ï¼Œ2023å¹´å›æ”¶å¯¶ç‰¹ç“¶64å„„æ”¯ï¼Œæ¸›ç¢³æ’æ”¾13.9è¬å™¸/å¹´""",
            "expected": True
        },
        {
            "name": "âœ… çœŸæ­£ç›¸é—œæ¡ˆä¾‹3 - æ‡‰è©²é€šé",
            "text": """MLCCç”¨é›¢å‹è†œå›æ”¶ç”¢èƒ½ 600 å™¸/æœˆï¼Œå¯å›æ”¶åœ‹å…§ MLCC åŠå…‰å­¸å®¢æˆ¶ä½¿ç”¨å¾Œä¹‹é›¢å‹è†œï¼Œå›æ”¶å†ç”Ÿå¾Œè£½æˆæ”¹è³ªç²’ã€‚""",
            "expected": True
        }
    ]
    
    # æ¸¬è©¦é—œéµå­—
    config = EnhancedKeywordConfig()
    keywords = (
        config.CORE_RECYCLED_PLASTIC_KEYWORDS["é«˜ç›¸é—œé€£çºŒé—œéµå­—"] +
        config.CORE_RECYCLED_PLASTIC_KEYWORDS["é«˜ç›¸é—œä¸é€£çºŒé—œéµå­—"]
    )
    
    print("ğŸ§ª å¢å¼·éæ¿¾åŠŸèƒ½ v2.0 æ¸¬è©¦")
    print("=" * 70)
    
    correct_predictions = 0
    total_tests = len(test_cases)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\næ¸¬è©¦æ¡ˆä¾‹ {i}: {test_case['name']}")
        print(f"æ–‡æœ¬: {test_case['text'][:120]}...")
        
        passed, matches = advanced_filtering_pipeline(test_case['text'], keywords)
        
        expected_result = "âœ… é€šé" if test_case['expected'] else "âŒ æ‹’çµ•"
        actual_result = "âœ… é€šé" if passed else "âŒ æ‹’çµ•"
        
        print(f"é æœŸçµæœ: {expected_result}")
        print(f"å¯¦éš›çµæœ: {actual_result}")
        
        if passed == test_case['expected']:
            print("ğŸ¯ æ¸¬è©¦ âœ… æ­£ç¢º")
            correct_predictions += 1
        else:
            print("ğŸš« æ¸¬è©¦ âŒ éŒ¯èª¤")
        
        if matches:
            print(f"åŒ¹é…è©³æƒ…:")
            for match in matches[:1]:  # åªé¡¯ç¤ºç¬¬ä¸€å€‹åŒ¹é…
                print(f"  - {match['keyword']}: {match['relevance_score']:.3f}")
                print(f"    {match['match_details']}")
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š æ¸¬è©¦ç¸½çµ:")
    print(f"æ­£ç¢ºé æ¸¬: {correct_predictions}/{total_tests}")
    print(f"æº–ç¢ºç‡: {correct_predictions/total_tests*100:.1f}%")
    
    if correct_predictions >= total_tests * 0.875:  # 87.5%ä»¥ä¸Š
        print("ğŸ‰ æ¸¬è©¦é€šéï¼éæ¿¾ç²¾ç¢ºåº¦å·²é”åˆ°è¦æ±‚")
    else:
        print("âš ï¸ éœ€è¦é€²ä¸€æ­¥èª¿æ•´éæ¿¾è¦å‰‡")

if __name__ == "__main__":
    test_enhanced_filtering_v2()