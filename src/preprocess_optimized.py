#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å„ªåŒ–çš„é è™•ç†å™¨ v2.3
å°ˆç‚ºé«˜ç²¾åº¦æå–è¨­è¨ˆï¼Œç‰¹åˆ¥å„ªåŒ–è¡¨æ ¼æ•¸æ“šè™•ç†
"""

import os
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from tqdm import tqdm
sys.path.append(str(Path(__file__).parent))
from config import *

class DocumentMetadataExtractor:
    """æ–‡æª”å…ƒæ•¸æ“šæå–å™¨ï¼Œç”¨æ–¼æå–å…¬å¸åç¨±å’Œå ±å‘Šå¹´åº¦"""
    
    def __init__(self):
        # å…¬å¸åç¨±åŒ¹é…æ¨¡å¼
        self.company_patterns = [
            r'([^,\n]+?)(?:è‚¡ä»½)?æœ‰é™å…¬å¸.*?(?:å¹´|å¹´åº¦).*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            r'([^,\n]+?)(?:è‚¡ä»½)?æœ‰é™å…¬å¸.*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            r'([^,\n\d]+?)å…¬å¸.*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            r'([\u4e00-\u9fff]{2,10})(?:è‚¡ä»½)?æœ‰é™å…¬å¸'
        ]
        
        # å¹´åº¦åŒ¹é…æ¨¡å¼
        self.year_patterns = [
            r'(20\d{2})\s*å¹´(?:åº¦)?.*?(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š',
            r'(?:æ°¸çºŒ|ESG|ä¼æ¥­ç¤¾æœƒè²¬ä»»)å ±å‘Š.*?(20\d{2})',
            r'(20\d{2})\s*å¹´(?:åº¦)?å ±å‘Š',
            r'å ±å‘Š.*?æœŸé–“.*?(20\d{2})',
            r'(20\d{2})',  # æœ€å¾Œå‚™ç”¨ï¼šä»»ä½•å››ä½æ•¸å¹´ä»½
        ]
    
    def extract_metadata(self, pdf_path: str) -> Dict[str, str]:
        """
        å¾PDFæ–‡ä»¶ä¸­æå–å…¬å¸åç¨±å’Œå ±å‘Šå¹´åº¦
        
        Returns:
            DictåŒ…å« 'company_name' å’Œ 'report_year'
        """
        print(f"ğŸ“‹ æå–æ–‡æª”å…ƒæ•¸æ“š: {Path(pdf_path).name}")
        
        try:
            loader = PyPDFLoader(pdf_path)
            pages = loader.load()
            
            # ä¸»è¦å¾å‰å¹¾é æå–ä¿¡æ¯
            text_for_extraction = ""
            for page in pages[:5]:  # åªæª¢æŸ¥å‰5é 
                text_for_extraction += page.page_content + "\n"
            
            # æå–å…¬å¸åç¨±
            company_name = self._extract_company_name(text_for_extraction)
            
            # æå–å ±å‘Šå¹´åº¦
            report_year = self._extract_report_year(text_for_extraction)
            
            # å¦‚æœç„¡æ³•å¾æ–‡æª”ä¸­æå–ï¼Œå˜—è©¦å¾æ–‡ä»¶åæå–
            if not company_name or not report_year:
                filename_metadata = self._extract_from_filename(Path(pdf_path).name)
                if not company_name:
                    company_name = filename_metadata.get('company_name', 'æœªçŸ¥å…¬å¸')
                if not report_year:
                    report_year = filename_metadata.get('report_year', 'æœªçŸ¥å¹´åº¦')
            
            result = {
                'company_name': company_name,
                'report_year': report_year
            }
            
            print(f"âœ… æå–åˆ°ï¼š{company_name} - {report_year}")
            return result
            
        except Exception as e:
            print(f"âš ï¸ å…ƒæ•¸æ“šæå–å¤±æ•—: {e}")
            return {
                'company_name': Path(pdf_path).stem,
                'report_year': 'æœªçŸ¥å¹´åº¦'
            }
    
    def _extract_company_name(self, text: str) -> str:
        """æå–å…¬å¸åç¨±"""
        text_clean = re.sub(r'\s+', ' ', text[:2000])  # åªæª¢æŸ¥å‰2000å­—ç¬¦
        
        for pattern in self.company_patterns:
            matches = re.findall(pattern, text_clean, re.IGNORECASE)
            if matches:
                company_name = matches[0].strip()
                # æ¸…ç†å…¬å¸åç¨±
                company_name = re.sub(r'^[\s\d\-\.]+', '', company_name)
                company_name = re.sub(r'[\s\-\.]+$', '', company_name)
                if len(company_name) >= 2 and len(company_name) <= 20:
                    return company_name
        
        return ""
    
    def _extract_report_year(self, text: str) -> str:
        """æå–å ±å‘Šå¹´åº¦"""
        text_clean = re.sub(r'\s+', ' ', text[:2000])
        
        for pattern in self.year_patterns:
            matches = re.findall(pattern, text_clean)
            if matches:
                year = matches[0]
                # é©—è­‰å¹´ä»½åˆç†æ€§
                if 2015 <= int(year) <= 2030:
                    return year
        
        return ""
    
    def _extract_from_filename(self, filename: str) -> Dict[str, str]:
        """å¾æ–‡ä»¶åæå–å…ƒæ•¸æ“šä½œç‚ºå‚™ç”¨"""
        result = {'company_name': '', 'report_year': ''}
        
        # æå–å¹´ä»½
        year_match = re.search(r'(20\d{2})', filename)
        if year_match:
            result['report_year'] = year_match.group(1)
        
        # ç°¡å–®çš„å…¬å¸åç¨±æå–ï¼ˆå»é™¤å¹´ä»½ã€å‰¯æª”åç­‰ï¼‰
        company_part = re.sub(r'(20\d{2}|ESG|æ°¸çºŒ|å ±å‘Š|\.pdf)', '', filename, flags=re.IGNORECASE)
        company_part = re.sub(r'[_\-\s]+', '', company_part).strip()
        if company_part:
            result['company_name'] = company_part
        
        return result

class OptimizedTextSplitter:
    """å„ªåŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼Œå°ˆç‚ºè¡¨æ ¼æ•¸æ“šè¨­è¨ˆ"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # æ¨™æº–åˆ†å‰²å™¨
        self.standard_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "ã€‚", "ï¼Œ", " ", ""]
        )
        
        # è¡¨æ ¼å°ˆç”¨åˆ†å‰²å™¨ï¼ˆæ›´å¤§çš„å¡Šï¼‰
        self.table_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size * 2,  # è¡¨æ ¼ä½¿ç”¨æ›´å¤§çš„å¡Š
            chunk_overlap=chunk_overlap,
            separators=["\n\n\n", "\n\n", "ã€‚ã€‚", "\n", ""]
        )
    
    def split_documents_optimized(self, documents: List) -> List:
        """å„ªåŒ–çš„æ–‡æª”åˆ†å‰²"""
        all_chunks = []
        
        for doc in tqdm(documents, desc="å„ªåŒ–åˆ†å‰²æ–‡æª”"):
            content = doc.page_content
            
            # æª¢æ¸¬æ˜¯å¦ç‚ºè¡¨æ ¼å…§å®¹
            if self._is_table_content(content):
                # ä½¿ç”¨è¡¨æ ¼å°ˆç”¨åˆ†å‰²å™¨
                chunks = self.table_splitter.split_documents([doc])
                # ç‚ºè¡¨æ ¼å¡Šæ·»åŠ ç‰¹æ®Šæ¨™è¨˜
                for chunk in chunks:
                    chunk.metadata['content_type'] = 'table'
                    chunk.metadata['is_table'] = True
            else:
                # ä½¿ç”¨æ¨™æº–åˆ†å‰²å™¨
                chunks = self.standard_splitter.split_documents([doc])
                for chunk in chunks:
                    chunk.metadata['content_type'] = 'standard'
                    chunk.metadata['is_table'] = False
            
            all_chunks.extend(chunks)
        
        print(f"ğŸ“Š åˆ†å‰²çµ±è¨ˆ: ç¸½å…± {len(all_chunks)} å€‹æ–‡æœ¬å¡Š")
        table_chunks = sum(1 for chunk in all_chunks if chunk.metadata.get('is_table', False))
        print(f"   - è¡¨æ ¼å¡Š: {table_chunks} å€‹")
        print(f"   - æ¨™æº–å¡Š: {len(all_chunks) - table_chunks} å€‹")
        
        return all_chunks
    
    def _is_table_content(self, content: str) -> bool:
        """æª¢æ¸¬æ˜¯å¦ç‚ºè¡¨æ ¼å…§å®¹"""
        # è¡¨æ ¼ç‰¹å¾µæŒ‡æ¨™
        table_indicators = [
            # æ•¸å€¼æ’åˆ—
            re.search(r'\d+(?:,\d{3})*(?:\.\d+)?\s+\d+(?:,\d{3})*(?:\.\d+)?\s+\d+(?:,\d{3})*(?:\.\d+)?', content),
            
            # å¹´ä»½åºåˆ—
            re.search(r'20\d{2}\s+20\d{2}\s+20\d{2}', content),
            
            # è¡¨æ ¼é—œéµè©
            any(keyword in content.lower() for keyword in [
                'æ­·å¹´', 'å¹´ä»½', 'å›æ”¶æ•¸é‡', 'ç¢³æ’æ¸›å°‘é‡', 'æ•ˆç›Š',
                'å›æ”¶é‡å¯ç¹è¡Œ', 'å¤§å®‰æ£®æ—', 'å¸ç¢³é‡'
            ]),
            
            # å¤šåˆ—æ•¸æ“šæ¨¡å¼
            len(re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?', content)) >= 6,
            
            # ç‰¹å®šè¡¨æ ¼æ¨™è¨˜è©
            any(phrase in content for phrase in [
                'å„„æ”¯å¯¶ç‰¹ç“¶', 'è¬å™¸', 'å™¸/å¹´', 'å™¸/æœˆ',
                'åº§å¤§å®‰æ£®æ—å…¬åœ’', 'å¯ç¹è¡Œåœ°çƒ'
            ])
        ]
        
        # å¦‚æœæœ‰2å€‹ä»¥ä¸ŠæŒ‡æ¨™åŒ¹é…ï¼Œèªç‚ºæ˜¯è¡¨æ ¼å…§å®¹
        matched_indicators = sum(1 for indicator in table_indicators if indicator)
        return matched_indicators >= 2

def preprocess_documents_optimized(pdf_path: str, output_db_path: str = None, metadata: Dict[str, str] = None):
    """å„ªåŒ–çš„æ–‡æª”é è™•ç†"""
    
    if output_db_path is None:
        output_db_path = VECTOR_DB_PATH
    
    print(f"ğŸ”„ é–‹å§‹å„ªåŒ–é è™•ç†PDF: {pdf_path}")
    
    # 1. è¼‰å…¥PDF
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°PDFæ–‡ä»¶: {pdf_path}")
    
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    print(f"ğŸ“„ æˆåŠŸè¼‰å…¥ {len(pages)} é ")
    
    # 2. ç‚ºæ¯å€‹æ–‡æª”æ·»åŠ å…ƒæ•¸æ“š
    if metadata:
        for page in pages:
            page.metadata.update(metadata)
            page.metadata['source_file'] = Path(pdf_path).name
    
    # 3. å„ªåŒ–æ–‡æœ¬åˆ†å‰²
    print("ğŸ”§ æ­£åœ¨é€²è¡Œå„ªåŒ–åˆ†å‰²...")
    text_splitter = OptimizedTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    
    chunks = text_splitter.split_documents_optimized(pages)
    print(f"âœ… å„ªåŒ–åˆ†å‰²å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} å€‹æ™ºèƒ½æ–‡æœ¬å¡Š")
    
    # 4. åˆå§‹åŒ–embeddingæ¨¡å‹
    print(f"ğŸ§  è¼‰å…¥embeddingæ¨¡å‹: {EMBEDDING_MODEL}")
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}
    )
    
    # 5. å»ºç«‹å‘é‡è³‡æ–™åº«
    print("ğŸ”— å»ºç«‹å„ªåŒ–å‘é‡è³‡æ–™åº«...")
    db = FAISS.from_documents(chunks, embedding_model)
    
    # 6. ä¿å­˜è³‡æ–™åº«
    os.makedirs(os.path.dirname(output_db_path), exist_ok=True)
    db.save_local(output_db_path)
    print(f"ğŸ’¾ å‘é‡è³‡æ–™åº«å·²ä¿å­˜åˆ°: {output_db_path}")
    
    return db

def preprocess_multiple_documents_optimized(pdf_paths: List[str]) -> Dict[str, Dict]:
    """
    å„ªåŒ–çš„æ‰¹é‡é è™•ç†å¤šå€‹PDFæ–‡æª”
    
    Returns:
        Dict: {pdf_path: {'db_path': str, 'metadata': dict}}
    """
    print(f"ğŸš€ é–‹å§‹å„ªåŒ–æ‰¹é‡é è™•ç† {len(pdf_paths)} å€‹PDFæ–‡ä»¶")
    print("=" * 60)
    
    metadata_extractor = DocumentMetadataExtractor()
    results = {}
    
    for pdf_path in pdf_paths:
        try:
            print(f"\nğŸ“„ å„ªåŒ–è™•ç†æ–‡ä»¶: {Path(pdf_path).name}")
            
            # 1. æå–å…ƒæ•¸æ“š
            metadata = metadata_extractor.extract_metadata(pdf_path)
            
            # 2. ç‚ºæ¯å€‹æ–‡ä»¶å‰µå»ºç¨ç«‹çš„å‘é‡è³‡æ–™åº«
            pdf_name = Path(pdf_path).stem
            db_path = os.path.join(
                os.path.dirname(VECTOR_DB_PATH),
                f"esg_db_optimized_{pdf_name}"
            )
            
            # 3. å„ªåŒ–é è™•ç†æ–‡æª”
            preprocess_documents_optimized(pdf_path, db_path, metadata)
            
            results[pdf_path] = {
                'db_path': db_path,
                'metadata': metadata,
                'pdf_name': pdf_name
            }
            
            print(f"âœ… å„ªåŒ–å®Œæˆ: {metadata['company_name']} - {metadata['report_year']}")
            
        except Exception as e:
            print(f"âŒ å„ªåŒ–è™•ç†å¤±æ•— {Path(pdf_path).name}: {e}")
            continue
    
    print(f"\nğŸ‰ å„ªåŒ–æ‰¹é‡é è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(pdf_paths)} å€‹æ–‡ä»¶")
    return results

def test_table_detection():
    """æ¸¬è©¦è¡¨æ ¼æª¢æ¸¬åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦è¡¨æ ¼æª¢æ¸¬åŠŸèƒ½")
    print("=" * 50)
    
    # æ¸¬è©¦ç”¨ä¾‹
    test_cases = [
        {
            "name": "å¯¶ç‰¹ç“¶å›æ”¶è¡¨æ ¼",
            "content": """æ­·å¹´å›æ”¶æ•¸é‡èˆ‡ç¢³æ’æ¸›å°‘é‡
å¹´ä»½ 2021 2022 2023
å›æ”¶æ•¸é‡ ç¢³æ’æ¸›å°‘é‡ å›æ”¶æ•¸é‡ ç¢³æ’æ¸›å°‘é‡ å›æ”¶æ•¸é‡ ç¢³æ’æ¸›å°‘é‡
æ•ˆç›Š
109,008 å™¸
87 å„„æ”¯å¯¶ç‰¹ç“¶ 188,039 å™¸
88,140 å™¸
70 å„„æ”¯å¯¶ç‰¹ç“¶ 152,042 å™¸
80,904 å™¸
64 å„„æ”¯å¯¶ç‰¹ç“¶ 139,559 å™¸""",
            "expected": True
        },
        {
            "name": "æ¨™æº–æ–‡æœ¬æ®µè½",
            "content": """å—äºå…¬å¸ç§‰æŒä¿è­·åœ°çƒã€æ°¸çºŒç™¼å±•çš„ç¶“ç‡Ÿç†å¿µï¼Œè‡ª2007å¹´é–‹å§‹å³è‡´åŠ›æ–¼å›æ”¶ã€å†ç”Ÿæ¶ˆè²»è€…ä½¿ç”¨å¾Œçš„å¯¶ç‰¹ç“¶ç­‰èšé…¯è£½å“ï¼Œå…¨åŠ›ç™¼å±•ç’°ä¿æ°¸çºŒç”¢å“ã€‚""",
            "expected": False
        },
        {
            "name": "æ•¸å€¼å¯†é›†æ®µè½",
            "content": """2023å¹´å›æ”¶å¯¶ç‰¹ç“¶64å„„æ”¯ï¼Œæ¸›ç¢³æ’æ”¾13.9è¬å™¸/å¹´ï¼Œå¯¶ç‰¹ç“¶å›æ”¶é€ ç²’å¾Œå–ä»£åŸç”Ÿèšé…¯ç²’è¼ƒåŸè£½ç¨‹ç”Ÿç”¢ä¹‹ç¢³æ’æ”¾é‡å¯æ¸›å°‘72%ã€‚""",
            "expected": True
        }
    ]
    
    splitter = OptimizedTextSplitter()
    
    for i, case in enumerate(test_cases, 1):
        print(f"\næ¸¬è©¦æ¡ˆä¾‹ {i}: {case['name']}")
        result = splitter._is_table_content(case['content'])
        status = "âœ… æ­£ç¢º" if result == case['expected'] else "âŒ éŒ¯èª¤"
        print(f"é æœŸ: {case['expected']}, å¯¦éš›: {result} - {status}")

def main():
    """ä¸»å‡½æ•¸"""
    # æª¢æŸ¥dataç›®éŒ„ä¸­çš„PDFæ–‡ä»¶
    data_dir = Path(DATA_PATH)
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"éŒ¯èª¤: åœ¨ {DATA_PATH} ç›®éŒ„ä¸­æ‰¾ä¸åˆ°PDFæ–‡ä»¶")
        print("è«‹å°‡ESGå ±å‘ŠPDFæ–‡ä»¶æ”¾å…¥dataç›®éŒ„ä¸­")
        return
    
    print(f"æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶:")
    for pdf_file in pdf_files:
        print(f"  - {pdf_file.name}")
    
    # è©¢å•è™•ç†æ¨¡å¼
    if len(pdf_files) == 1:
        # å–®æ–‡ä»¶æ¨¡å¼
        pdf_path = pdf_files[0]
        print(f"\nå–®æ–‡ä»¶å„ªåŒ–æ¨¡å¼ï¼šè™•ç† {pdf_path.name}")
        
        try:
            metadata_extractor = DocumentMetadataExtractor()
            metadata = metadata_extractor.extract_metadata(str(pdf_path))
            preprocess_documents_optimized(str(pdf_path), metadata=metadata)
            print("âœ… å„ªåŒ–é è™•ç†å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ å„ªåŒ–é è™•ç†å¤±æ•—: {e}")
    else:
        # å¤šæ–‡ä»¶æ¨¡å¼
        print(f"\nå¤šæ–‡ä»¶å„ªåŒ–æ¨¡å¼ï¼šè™•ç† {len(pdf_files)} å€‹æ–‡ä»¶")
        
        print("é¸é …:")
        print("1. åŸ·è¡Œå„ªåŒ–é è™•ç†")
        print("2. æ¸¬è©¦è¡¨æ ¼æª¢æ¸¬åŠŸèƒ½")
        
        choice = input("è«‹é¸æ“‡ (1-2): ").strip()
        
        if choice == "1":
            confirm = input("ç¢ºå®šè¦æ‰¹é‡å„ªåŒ–è™•ç†æ‰€æœ‰æ–‡ä»¶å—ï¼Ÿ(y/n): ").strip().lower()
            
            if confirm == 'y':
                try:
                    results = preprocess_multiple_documents_optimized([str(f) for f in pdf_files])
                    print(f"âœ… å„ªåŒ–æ‰¹é‡é è™•ç†å®Œæˆï¼")
                    
                    # é¡¯ç¤ºè™•ç†çµæœæ‘˜è¦
                    print("\nğŸ“‹ å„ªåŒ–è™•ç†æ‘˜è¦:")
                    for pdf_path, result in results.items():
                        metadata = result['metadata']
                        print(f"  âœ“ {Path(pdf_path).name}: {metadata['company_name']} - {metadata['report_year']}")
                        
                except Exception as e:
                    print(f"âŒ å„ªåŒ–æ‰¹é‡é è™•ç†å¤±æ•—: {e}")
        
        elif choice == "2":
            test_table_detection()

if __name__ == "__main__":
    main()