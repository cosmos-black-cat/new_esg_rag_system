#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGå ±å‘Šæ›¸æå–å™¨æ ¸å¿ƒæ¨¡çµ„ v1.0
å°ˆé–€æå–ESGå ±å‘Šä¸­çš„å†ç”Ÿå¡‘è† ç›¸é—œæ•¸æ“š
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""
    keyword_distance: int = 0

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# é—œéµå­—é…ç½®
# =============================================================================

# åœ¨ esg_extractor.py ä¸­çš„ KeywordConfig é¡ï¼Œåªéœ€è¦ä¿®æ”¹é€™å€‹éƒ¨åˆ†

class KeywordConfig:
    """ESGå ±å‘Šæ›¸é—œéµå­—é…ç½® - æ“´å±•ç‰ˆ"""
    
    RECYCLED_PLASTIC_KEYWORDS = {
        # åŸæœ‰çš„å†ç”Ÿå¡‘è† é—œéµå­—ï¼ˆä¿æŒä¸è®Šï¼‰
        "high_relevance_continuous": [
            "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”ŸPET", "å†ç”ŸPP",
            "å›æ”¶å¡‘è† ", "å›æ”¶å¡‘æ–™", "å›æ”¶PP", "å›æ”¶PET", 
            "rPET", "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™",
            "å¯¶ç‰¹ç“¶å›æ”¶", "å»¢å¡‘è† å›æ”¶", "å¡‘è† å¾ªç’°",
            "å›æ”¶é€ ç²’", "å†ç”Ÿèšé…¯", "å›æ”¶èšé…¯",
            "å¾ªç’°ç¶“æ¿Ÿ", "ç‰©æ–™å›æ”¶", "ææ–™å›æ”¶"
        ],
        
        "medium_relevance_continuous": [
            "ç’°ä¿å¡‘è† ", "ç¶ è‰²ææ–™", "æ°¸çºŒææ–™",
            "å»¢æ–™å›æ”¶", "è³‡æºå›æ”¶", "å¾ªç’°åˆ©ç”¨"
        ],
        
        "high_relevance_discontinuous": [
            ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å¯¶ç‰¹ç“¶", "å†é€ "), ("å¯¶ç‰¹ç“¶", "å¾ªç’°"),
            ("å„„æ”¯", "å¯¶ç‰¹ç“¶"), ("è¬æ”¯", "å¯¶ç‰¹ç“¶"),
            ("PET", "å›æ”¶"), ("PET", "å†ç”Ÿ"), ("PP", "å›æ”¶"), ("PP", "å†ç”Ÿ"),
            ("å¡‘è† ", "å›æ”¶"), ("å¡‘æ–™", "å›æ”¶"), ("å¡‘è† ", "å¾ªç’°"),
            ("å›æ”¶", "é€ ç²’"), ("å›æ”¶", "ç”¢èƒ½"), ("å›æ”¶", "ææ–™"),
            ("å†ç”Ÿ", "ææ–™"), ("å»¢æ–™", "å›æ”¶"), ("MLCC", "å›æ”¶"),
            ("åŸç”Ÿ", "ææ–™"), ("ç¢³æ’æ”¾", "æ¸›å°‘"), ("æ¸›ç¢³", "æ•ˆç›Š"),
            ("æ­·å¹´", "å›æ”¶"), ("å›æ”¶", "æ•¸é‡"), ("å›æ”¶", "æ•ˆç›Š"),
            ("å¾ªç’°", "ç¶“æ¿Ÿ"), ("æ°¸çºŒ", "ç™¼å±•"), ("ç’°ä¿", "ç”¢å“")
        ]
    }
    
    # æ–°å¢ï¼šå¾ªç’°ç¶“æ¿Ÿå’Œèƒ½æºç›¸é—œé—œéµå­—
    CIRCULAR_ECONOMY_KEYWORDS = {
        # çµ„1ï¼šæ¯”ç‡å’Œæ•ˆç‡ç›¸é—œ
        "ratio_and_efficiency": [
            "ææ–™å¾ªç’°ç‡", "ææ–™å¯å›æ”¶ç‡", "å†ç”Ÿèƒ½æºä½¿ç”¨ç‡",
            "å–®ä½ç¶“æ¿Ÿæ•ˆç›Š", "å†ç”Ÿææ–™æ›¿ä»£ç‡", "ç¢³æ’æ¸›é‡æ¯”ç‡",
            "å†ç”Ÿå¡‘è† ä½¿ç”¨æ¯”ç‡"
        ],
        
        # çµ„2ï¼šææ–™å’Œèƒ½æºä½¿ç”¨é‡
        "material_and_energy_usage": [
            "å†ç”Ÿææ–™ä½¿ç”¨é‡", "ææ–™ç¸½ä½¿ç”¨é‡", "ç¶ é›»æ†‘è­‰",
            "å¤ªé™½èƒ½é›»åŠ›", "è³¼é›»å”è­°", "å†ç”Ÿèƒ½æº",
            "å†ç”Ÿææ–™ç¢³æ’æ¸›é‡", "å†ç”Ÿå¡‘è† æˆæœ¬", "å†ç”Ÿå¡‘è† çš„ä½¿ç”¨é‡",
            "å¡‘è† ä½¿ç”¨é‡", "ææ–™ä½¿ç”¨é‡"
        ],
        
        # çµ„3ï¼šæµç¨‹å’Œæ¦‚å¿µ
        "process_and_concepts": [
            "åˆ†é¸è¾¨è¦–", "æˆæœ¬å¢åŠ ", "ææ–™å›æ”¶", "æè³ªåˆ†é›¢",
            "ç¢³æ’æ”¾", "å–®ä¸€ææ–™"
        ],
        
        # çµ„4ï¼šä¸é€£çºŒé—œéµå­—çµ„åˆ
        "discontinuous_combinations": [
            ("ææ–™", "å¾ªç’°ç‡"), ("ææ–™", "å¯å›æ”¶ç‡"), ("å†ç”Ÿèƒ½æº", "ä½¿ç”¨ç‡"),
            ("ç¶“æ¿Ÿ", "æ•ˆç›Š"), ("ææ–™", "æ›¿ä»£ç‡"), ("ç¢³æ’", "æ¸›é‡"),
            ("å†ç”Ÿå¡‘è† ", "ä½¿ç”¨æ¯”ç‡"), ("å†ç”Ÿææ–™", "ä½¿ç”¨é‡"),
            ("ææ–™", "ç¸½ä½¿ç”¨é‡"), ("ç¶ é›»", "æ†‘è­‰"), ("å¤ªé™½èƒ½", "é›»åŠ›"),
            ("è³¼é›»", "å”è­°"), ("å†ç”Ÿ", "èƒ½æº"), ("åˆ†é¸", "è¾¨è¦–"),
            ("ç¢³æ’", "æ¸›é‡"), ("å¡‘è† ", "æˆæœ¬"), ("æˆæœ¬", "å¢åŠ "),
            ("æè³ª", "åˆ†é›¢"), ("å–®ä¸€", "ææ–™"), ("å¡‘è† ", "ä½¿ç”¨é‡"),
            ("ææ–™", "ä½¿ç”¨é‡")
        ]
    }
    
    # åŸæœ‰çš„æ’é™¤è¦å‰‡ä¿æŒä¸è®Š
    EXCLUSION_RULES = {
        "exclude_topics": [
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "è·ç½",
            "é¦¬æ‹‰æ¾", "è³½äº‹", "é¸æ‰‹", "æ¯”è³½", "è³½è¡£", "é‹å‹•",
            "é›¨æ°´å›æ”¶", "å»¢æ°´è™•ç†", "æ°´è³ªç›£æ¸¬",
            "æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ", "æ¡ˆä¾‹é¸æ‹”",
            "èƒ½æºè½‰å‹", "ç‡ƒæ²¹æ”¹ç‡ƒ", "é‹çˆæ”¹å–„", "å¤©ç„¶æ°£ç‡ƒç‡’",
            "ç¯€èƒ½ç”¢å“", "éš”ç†±æ¼†", "ç¯€èƒ½çª—", "éš”ç†±ç´™", "é…·æ¨‚æ¼†",
            "æ°£å¯†çª—", "éš”ç†±ç”¢å“", "ä¿æº«ææ–™", "å»ºæç”¢å“",
            "å¤ªé™½èƒ½", "é¢¨é›»", "ç¶ èƒ½", "å…‰é›»", "é›»æ± ææ–™"
        ],
        
        "exclude_contexts": [
            "å‚ç›´é¦¬æ‹‰æ¾", "å²ä¸Šæœ€ç’°ä¿è³½è¡£", "å„ç•Œå¥½æ‰‹",
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å®‰çµ±è¨ˆ", 
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "å„ªè‰¯æ¡ˆä¾‹",
            "é›¨æ°´å›æ”¶é‡æ¸›å°‘", "é™é›¨é‡æ¸›å°‘",
            "ç‡ƒæ²¹æ”¹ç‡ƒæ±½é‹çˆ", "å¤©ç„¶æ°£ç‡ƒç‡’æ©Ÿ", "é‹çˆæ”¹é€ ",
            "é…·æ¨‚æ¼†", "éš”ç†±æ¼†", "ç¯€èƒ½æ°£å¯†çª—", "å†°é…·éš”ç†±ç´™"
        ]
    }
    
    # åŸæœ‰çš„å¡‘è† æŒ‡æ¨™ä¿æŒä¸è®Š
    PLASTIC_INDICATORS = {
        "plastic_materials": [
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "èšåˆç‰©",
            "æ¨¹è„‚", "ç²’å­", "é¡†ç²’", "ææ–™", "å¡‘è† ç²’", "èšé…¯ç²’",
            "å¯¶ç‰¹ç“¶", "ç“¶ç‰‡", "å®¹å™¨", "åŒ…è£", "è†œæ", "çº–ç¶­"
        ],
        
        "recycling_specific": [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨",
            "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "å¾ªç’°ç¶“æ¿Ÿ",
            "å»¢æ–™", "å»¢æ£„", "å›æ”¶æ–™", "å†ç”Ÿæ–™", "PCR"
        ]
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­— - åŒ…å«æ–°å¢çš„å¾ªç’°ç¶“æ¿Ÿé—œéµå­—"""
        all_keywords = []
        
        # åŸæœ‰çš„å†ç”Ÿå¡‘è† é—œéµå­—
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_discontinuous"])
        
        # æ–°å¢çš„å¾ªç’°ç¶“æ¿Ÿé—œéµå­—
        all_keywords.extend(cls.CIRCULAR_ECONOMY_KEYWORDS["ratio_and_efficiency"])
        all_keywords.extend(cls.CIRCULAR_ECONOMY_KEYWORDS["material_and_energy_usage"])
        all_keywords.extend(cls.CIRCULAR_ECONOMY_KEYWORDS["process_and_concepts"])
        all_keywords.extend(cls.CIRCULAR_ECONOMY_KEYWORDS["discontinuous_combinations"])
        
        return all_keywords

# =============================================================================
# åŒ¹é…å¼•æ“
# =============================================================================

# åœ¨ esg_extractor.py ä¸­çš„ ESGMatcher é¡ï¼Œåªéœ€è¦æ›´æ–° __init__ æ–¹æ³•ä¸­çš„åŒ¹é…æ¨¡å¼

class ESGMatcher:
    """ESGæ•¸æ“šåŒ¹é…å¼•æ“"""
    
    def __init__(self):
        self.config = KeywordConfig()
        self.max_distance = 300
        
        # æ“´å±•çš„æ•¸å€¼åŒ¹é…æ¨¡å¼ï¼ˆå¢åŠ æ–°çš„å–®ä½å’Œæ ¼å¼ï¼‰
        self.number_patterns = [
            # åŸæœ‰çš„åŒ¹é…æ¨¡å¼
            r'\d+(?:\.\d+)?\s*å„„æ”¯',
            r'\d+(?:\.\d+)?\s*è¬æ”¯',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|åƒ)?å™¸',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kg|KG|å…¬æ–¤)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æœˆ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/å¹´',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æ—¥',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ä»¶|å€‹|æ‰¹|å°|å¥—)',
            
            # æ–°å¢ï¼šèƒ½æºå’Œææ–™ç›¸é—œå–®ä½
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kW|MW|GW|åƒç“¦|è¬ç“¦)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kWh|MWh|GWh|åº¦)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å¼µ',  # ç¶ é›»æ†‘è­‰
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å„„å…ƒ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*è¬å…ƒ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*åƒå…ƒ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å…ƒ',
            
            # æ–°å¢ï¼šç”¨é‡å’Œä½¿ç”¨é‡ç›¸é—œ
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*ç«‹æ–¹ç±³',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*mÂ³',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å…¬å‡',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*L',
        ]
        
        # æ“´å±•çš„ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%',
            r'\d+(?:\.\d+)?\s*ï¼…',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
            
            # æ–°å¢ï¼šæ¯”ç‡ç›¸é—œè¡¨é”
            r'\d+(?:\.\d+)?\s*æ¯”ç‡',
            r'\d+(?:\.\d+)?\s*æ•ˆç‡',
            r'\d+(?:\.\d+)?\s*ä½¿ç”¨ç‡',
            r'\d+(?:\.\d+)?\s*å¾ªç’°ç‡',
            r'\d+(?:\.\d+)?\s*å›æ”¶ç‡',
            r'\d+(?:\.\d+)?\s*æ›¿ä»£ç‡',
        ]

# =============================================================================
# ESGæå–å™¨ä¸»é¡
# =============================================================================

class ESGExtractor:
    """ESGå ±å‘Šæ›¸æå–å™¨ä¸»é¡"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = ESGMatcher()
        self.keyword_config = KeywordConfig()
        
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… ESGå ±å‘Šæ›¸æå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 400) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """è™•ç†å–®å€‹æ–‡æª”"""
        start_time = datetime.now()
        print(f"\nğŸ“Š è™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. æ–‡æª”æª¢ç´¢
        documents = self._document_retrieval(db, max_documents)
        
        # 3. æ•¸æ“šæå–
        extractions = self._extract_data(documents, doc_info)
        
        # 4. å¾Œè™•ç†
        extractions = self._post_process_extractions(extractions)
        
        # 5. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            total_documents=len(documents),
            stage1_passed=len(documents),
            stage2_passed=len(extractions),
            total_extractions=len(extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 6. åŒ¯å‡ºçµæœ
        excel_path = self._export_to_excel(extractions, summary, doc_info)
        
        return extractions, summary, excel_path
    
    def process_multiple_documents(self, docs_info: Dict[str, DocumentInfo], max_documents: int = 400) -> Dict[str, Tuple]:
        """æ‰¹é‡è™•ç†å¤šå€‹æ–‡æª”"""
        print(f"ğŸ“Š é–‹å§‹æ‰¹é‡è™•ç† {len(docs_info)} å€‹æ–‡æª”")
        print("=" * 60)
        
        results = {}
        
        for pdf_path, doc_info in docs_info.items():
            try:
                print(f"\nğŸ“„ è™•ç†: {doc_info.company_name} - {doc_info.report_year}")
                
                extractions, summary, excel_path = self.process_single_document(doc_info, max_documents)
                
                results[pdf_path] = (extractions, summary, excel_path)
                
                print(f"âœ… å®Œæˆ: ç”Ÿæˆ {len(extractions)} å€‹çµæœ -> {Path(excel_path).name}")
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•— {doc_info.company_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\nğŸ‰ æ‰¹é‡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(docs_info)} å€‹æ–‡æª”")
        return results
    
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _document_retrieval(self, db, max_docs: int) -> List[Document]:
        """æ–‡æª”æª¢ç´¢"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # é—œéµå­—æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œé—œéµå­—æª¢ç´¢...")
        for keyword in keywords[:20]:
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = db.similarity_search(search_term, k=15)
            all_docs.extend(docs)
        
        # ä¸»é¡Œæª¢ç´¢
        print("   ğŸ” åŸ·è¡Œä¸»é¡Œæª¢ç´¢...")
        topic_queries = [
            "å¡‘è†  å›æ”¶ ææ–™",
            "å¯¶ç‰¹ç“¶ å¾ªç’° ç¶“æ¿Ÿ",
            "å†ç”Ÿ ç’°ä¿ æ°¸çºŒ",
            "å»¢æ–™ è™•ç† åˆ©ç”¨"
        ]
        
        for query in topic_queries:
            docs = db.similarity_search(query, k=20)
            all_docs.extend(docs)
        
        # æ•¸å€¼æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œæ•¸å€¼æª¢ç´¢...")
        number_queries = [
            "å„„æ”¯", "è¬å™¸", "åƒå™¸", "ç”¢èƒ½", "å›æ”¶é‡",
            "æ¸›ç¢³", "ç™¾åˆ†æ¯”", "æ•ˆç›Š", "æ•¸é‡"
        ]
        
        for query in number_queries:
            docs = db.similarity_search(query, k=10)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹å€™é¸æ–‡æª”")
        return result_docs
    
    def _extract_data(self, documents: List[Document], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """æ•¸æ“šæå–"""
        print("ğŸ¯ åŸ·è¡Œæ•¸æ“šæå–...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="æ•¸æ“šæå–"):
            # æ®µè½åˆ†å‰²
            paragraphs = self._split_paragraphs(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 15:
                    continue
                
                # å°æ¯å€‹é—œéµå­—é€²è¡ŒåŒ¹é…
                for keyword in keywords:
                    # æª¢æŸ¥ç›¸é—œæ€§
                    is_relevant, relevance_score, details = self.matcher.comprehensive_relevance_check(paragraph, keyword)
                    
                    if is_relevant and relevance_score > 0.55:
                        # æå–æ•¸å€¼é…å°
                        value_pairs = self.matcher.extract_keyword_value_pairs(paragraph, keyword)
                        
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ•¸å€¼ä½†ç›¸é—œæ€§å¾ˆé«˜ï¼Œä¿ç•™ä½œç‚ºæè¿°
                        if not value_pairs and relevance_score > 0.75:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value="[ç›¸é—œæè¿°]",
                                value_type='description',
                                unit='',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                keyword_distance=0
                            )
                            extractions.append(extraction)
                        
                        # è™•ç†æ‰¾åˆ°çš„æ•¸å€¼é…å°
                        for value, value_type, association_score, distance in value_pairs:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            final_confidence = (relevance_score * 0.4 + association_score * 0.6)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=value,
                                value_type=value_type,
                                unit=self._extract_unit(value) if value_type == 'number' else '%',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=final_confidence,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                keyword_distance=distance
                            )
                            extractions.append(extraction)
        
        print(f"âœ… æ•¸æ“šæå–å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹çµæœ")
        return extractions
    
    def _post_process_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å¾Œè™•ç†å’Œå»é‡"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”§ å¾Œè™•ç† {len(extractions)} å€‹æå–çµæœ...")
        
        # ç²¾ç¢ºå»é‡
        unique_extractions = []
        seen_combinations = set()
        
        for extraction in extractions:
            identifier = (
                extraction.keyword,
                extraction.value,
                extraction.value_type,
                extraction.paragraph[:100]
            )
            
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_extractions.append(extraction)
        
        print(f"ğŸ“Š å»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™2ç­†ï¼‰
        page_filtered_extractions = self._apply_per_page_filtering(unique_extractions, max_per_page=2)
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        page_filtered_extractions.sort(key=lambda x: x.confidence, reverse=True)
        
        print(f"âœ… å¾Œè™•ç†å®Œæˆ: ä¿ç•™ {len(page_filtered_extractions)} å€‹æœ€çµ‚çµæœ")
        return page_filtered_extractions
    
    def _apply_per_page_filtering(self, extractions: List[NumericExtraction], max_per_page: int = 2) -> List[NumericExtraction]:
        """æŒ‰é é¢å»é‡"""
        if not extractions:
            return extractions
        
        print(f"ğŸ“„ åŸ·è¡ŒæŒ‰é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™ {max_per_page} ç­†ï¼‰...")
        
        # æŒ‰é ç¢¼åˆ†çµ„
        page_groups = {}
        for extraction in extractions:
            page_key = str(extraction.page_number).strip()
            if page_key not in page_groups:
                page_groups[page_key] = []
            page_groups[page_key].append(extraction)
        
        # æ¯é é¢å…§æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åºä¸¦ä¿ç•™æœ€ä½³çµæœ
        filtered_extractions = []
        
        for page_key, page_extractions in page_groups.items():
            page_extractions.sort(key=lambda x: x.confidence, reverse=True)
            kept_extractions = page_extractions[:max_per_page]
            filtered_extractions.extend(kept_extractions)
        
        print(f"   âœ… é é¢å»é‡å®Œæˆ: {len(filtered_extractions)} ç­†æœ€çµ‚çµæœ")
        return filtered_extractions
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excel"""
        company_safe = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
        
        # æ ¹æ“šæå–çµæœæ•¸é‡æ±ºå®šæª”å
        if len(extractions) == 0:
            output_filename = f"ESGæå–çµæœ_ç„¡æå–_{company_safe}_{doc_info.report_year}.xlsx"
            status_message = "ç„¡æå–çµæœ"
        else:
            output_filename = f"ESGæå–çµæœ_{company_safe}_{doc_info.report_year}.xlsx"
            status_message = f"æå–çµæœ: {len(extractions)} é …"
        
        output_path = os.path.join(RESULTS_PATH, output_filename)
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºçµæœåˆ°Excel: {output_filename}")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        
        # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯
        header_row = {
            'é—œéµå­—': f"å…¬å¸: {doc_info.company_name}",
            'æå–æ•¸å€¼': f"å ±å‘Šå¹´åº¦: {doc_info.report_year}",
            'æ•¸æ“šé¡å‹': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            'å–®ä½': '',
            'æ®µè½å…§å®¹': f"{status_message}ï¼ˆESGå ±å‘Šæ›¸æå–å™¨ v1.0ï¼‰",
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': '',
            'ä¸Šä¸‹æ–‡': ''
        }
        main_data.append(header_row)
        
        # ç©ºè¡Œåˆ†éš”
        main_data.append({col: '' for col in header_row.keys()})
        
        # å¦‚æœæœ‰æå–çµæœï¼Œæ·»åŠ çµæœæ•¸æ“š
        if len(extractions) > 0:
            for extraction in extractions:
                main_data.append({
                    'é—œéµå­—': extraction.keyword,
                    'æå–æ•¸å€¼': extraction.value,
                    'æ•¸æ“šé¡å‹': extraction.value_type,
                    'å–®ä½': extraction.unit,
                    'æ®µè½å…§å®¹': extraction.paragraph,
                    'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                    'é ç¢¼': extraction.page_number,
                    'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                    'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
                })
        else:
            # å¦‚æœæ²’æœ‰æå–çµæœï¼Œæ·»åŠ èªªæ˜è¡Œ
            no_result_row = {
                'é—œéµå­—': 'ç„¡ç›¸é—œé—œéµå­—åŒ¹é…',
                'æå–æ•¸å€¼': 'N/A',
                'æ•¸æ“šé¡å‹': 'no_data',
                'å–®ä½': '',
                'æ®µè½å…§å®¹': 'åœ¨æ­¤ä»½ESGå ±å‘Šä¸­æœªæ‰¾åˆ°å†ç”Ÿå¡‘è† ç›¸é—œçš„æ•¸å€¼æ•¸æ“š',
                'æ®µè½ç·¨è™Ÿ': '',
                'é ç¢¼': '',
                'ä¿¡å¿ƒåˆ†æ•¸': 0.0,
                'ä¸Šä¸‹æ–‡': 'å¯èƒ½çš„åŸå› ï¼š1) è©²å…¬å¸æœªæ¶‰åŠå†ç”Ÿå¡‘è† æ¥­å‹™ 2) å ±å‘Šä¸­æœªè©³ç´°æŠ«éœ²ç›¸é—œæ•¸æ“š 3) é—œéµå­—åŒ¹é…ç¯„åœéœ€è¦èª¿æ•´'
            }
            main_data.append(no_result_row)
        
        # çµ±è¨ˆæ•¸æ“š
        stats_data = []
        if len(extractions) > 0:
            for keyword, count in summary.keywords_found.items():
                keyword_extractions = [e for e in extractions if e.keyword == keyword]
                
                stats_data.append({
                    'é—œéµå­—': keyword,
                    'æå–æ•¸é‡': count,
                    'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                    'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
                })
        else:
            stats_data.append({
                'é—œéµå­—': 'æœå°‹æ‘˜è¦',
                'æå–æ•¸é‡': 0,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': 0.0,
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': 0.0
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ä¸»è¦çµæœå·¥ä½œè¡¨
            sheet_name = 'æå–çµæœ' if len(extractions) > 0 else 'ç„¡æå–çµæœ'
            pd.DataFrame(main_data).to_excel(writer, sheet_name=sheet_name, index=False)
            
            # çµ±è¨ˆå·¥ä½œè¡¨
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='çµ±è¨ˆæ‘˜è¦', index=False)
            
            # è™•ç†æ‘˜è¦
            summary_data = [{
                'å…¬å¸åç¨±': summary.company_name,
                'å ±å‘Šå¹´åº¦': summary.report_year,
                'ç¸½æ–‡æª”æ•¸': summary.total_documents,
                'ç¸½æå–çµæœ': summary.total_extractions,
                'è™•ç†ç‹€æ…‹': 'æˆåŠŸæå–' if len(extractions) > 0 else 'ç„¡ç›¸é—œæ•¸æ“š',
                'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
                'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'æå–å™¨ç‰ˆæœ¬': 'ESGå ±å‘Šæ›¸æå–å™¨ v1.0'
            }]
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        if len(extractions) > 0:
            print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼ŒåŒ…å« {len(extractions)} é …æå–çµæœ")
        else:
            print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼Œæ¨™è¨˜ç‚ºç„¡æå–çµæœ")
        
        return output_path
    
    # è¼”åŠ©æ–¹æ³•
    def _split_paragraphs(self, text: str) -> List[str]:
        """æ®µè½åˆ†å‰²"""
        paragraphs = []
        
        # æ¨™æº–åˆ†å‰²
        standard_paras = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs.extend([p.strip() for p in standard_paras if len(p.strip()) >= 15])
        
        # å¥è™Ÿåˆ†å‰²
        sentence_paras = re.split(r'ã€‚{2,}|\.{2,}', text)
        paragraphs.extend([p.strip() for p in sentence_paras if len(p.strip()) >= 30])
        
        # ä¿æŒåŸæ–‡
        if len(text.strip()) >= 50:
            paragraphs.append(text.strip())
        
        # å»é‡
        unique_paragraphs = []
        seen = set()
        for para in paragraphs:
            para_hash = hash(para[:100])
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)
        
        return unique_paragraphs
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 150) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:300]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:300]

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦ç”¨"""
    print("ğŸ“Š ESGå ±å‘Šæ›¸æå–å™¨æ¸¬è©¦æ¨¡å¼")
    
    extractor = ESGExtractor(enable_llm=False)
    print("âœ… ESGå ±å‘Šæ›¸æå–å™¨åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    main()