#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGå ±å‘Šæ›¸æå–å™¨æ ¸å¿ƒæ¨¡çµ„ v2.0
æ”¯æŒæ–°é—œéµå­—é…ç½®å’ŒWordæ–‡ä»¶è¼¸å‡º
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# Wordæ–‡ä»¶è™•ç†
from docx import Document as WordDocument
from docx.shared import Inches, Cm, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement, qn

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str
    stock_code: str = ""  # æ–°å¢è‚¡ç¥¨ä»£è™Ÿ

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number', 'percentage', 'description'
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""
    stock_code: str = ""  # æ–°å¢è‚¡ç¥¨ä»£è™Ÿ
    keyword_distance: int = 0
    full_section: str = ""  # æ–°å¢å®Œæ•´æ®µè½å…§å®¹

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    stock_code: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# æ–°é—œéµå­—é…ç½®
# =============================================================================

class EnhancedKeywordConfig:
    """å¢å¼·çš„é—œéµå­—é…ç½® - æ”¯æŒæ–°çš„ESGé—œéµå­—"""
    
    # çµ„1ï¼šæ¯”ç‡é¡é—œéµå­—ï¼ˆé€šå¸¸èˆ‡ç™¾åˆ†æ¯”æ•¸å€¼é—œè¯ï¼‰
    RATIO_KEYWORDS = {
        "high_relevance_continuous": [
            "ææ–™å¾ªç’°ç‡", "ææ–™å¯å›æ”¶ç‡", "å†ç”Ÿèƒ½æºä½¿ç”¨ç‡", 
            "å–®ä½ç¶“æ¿Ÿæ•ˆç›Š", "å†ç”Ÿææ–™æ›¿ä»£ç‡", "ç¢³æ’æ¸›é‡æ¯”ç‡", "å†ç”Ÿå¡‘è† ä½¿ç”¨æ¯”ç‡",
            "å›æ”¶åˆ©ç”¨ç‡", "è³‡æºå¾ªç’°ç‡", "ç¶ è‰²ææ–™æ¯”ç‡", "ç’°ä¿ææ–™å æ¯”",
            "å†ç”Ÿæ–™ä½¿ç”¨ç‡", "å¾ªç’°ç¶“æ¿Ÿæ•ˆç‡", "å»¢æ£„ç‰©è³‡æºåŒ–æ¯”ä¾‹"
        ],
        
        "medium_relevance_continuous": [
            "ä½¿ç”¨ç‡", "æ›¿ä»£ç‡", "å›æ”¶ç‡", "åˆ©ç”¨ç‡", "æ¸›é‡æ¯”ç‡",
            "å¾ªç’°ç‡", "æ•ˆç›Šæ¯”", "å æ¯”", "æ¯”ä¾‹", "æ•ˆç‡"
        ],
        
        "discontinuous": [
            ("ææ–™", "å¾ªç’°ç‡"), ("ææ–™", "å¯å›æ”¶ç‡"), ("å†ç”Ÿèƒ½æº", "ä½¿ç”¨ç‡"),
            ("å†ç”Ÿææ–™", "æ›¿ä»£ç‡"), ("ç¢³æ’", "æ¸›é‡", "æ¯”ç‡"), ("å†ç”Ÿå¡‘è† ", "ä½¿ç”¨", "æ¯”ç‡"),
            ("å›æ”¶", "åˆ©ç”¨ç‡"), ("è³‡æº", "å¾ªç’°ç‡"), ("å»¢æ£„ç‰©", "è³‡æºåŒ–", "æ¯”ä¾‹")
        ]
    }
    
    # çµ„2ï¼šæ•¸é‡é¡é—œéµå­—ï¼ˆé€šå¸¸èˆ‡æ•¸å€¼å–®ä½é—œè¯ï¼‰
    QUANTITY_KEYWORDS = {
        "high_relevance_continuous": [
            "å†ç”Ÿææ–™ä½¿ç”¨é‡", "ææ–™ç¸½ä½¿ç”¨é‡", "ç¶ é›»æ†‘è­‰", "å¤ªé™½èƒ½é›»åŠ›",
            "è³¼é›»å”è­°", "å†ç”Ÿèƒ½æº", "å†ç”Ÿææ–™ç¢³æ’æ¸›é‡", "å†ç”Ÿå¡‘è† æˆæœ¬",
            "å†ç”Ÿå¡‘è† çš„ä½¿ç”¨é‡", "æˆæœ¬å¢åŠ ", "ææ–™å›æ”¶", "æè³ªåˆ†é›¢",
            "ç¢³æ’æ”¾", "å¡‘è† ä½¿ç”¨é‡", "ææ–™ä½¿ç”¨é‡", "å›æ”¶è™•ç†é‡", "å»¢æ–™å›æ”¶é‡",
            "å¯¶ç‰¹ç“¶å›æ”¶é‡", "å¾ªç’°ææ–™ä½¿ç”¨é‡", "ç’°ä¿ææ–™ä½¿ç”¨é‡"
        ],
        
        "medium_relevance_continuous": [
            "ä½¿ç”¨é‡", "è™•ç†é‡", "å›æ”¶é‡", "æ¸›é‡", "ç”¢é‡", "æ¶ˆè€—é‡",
            "æŠ•å…¥é‡", "ç”¢å‡ºé‡", "ç¯€ç´„é‡", "æ›¿ä»£é‡", "å¾ªç’°é‡"
        ],
        
        "discontinuous": [
            ("å†ç”Ÿææ–™", "ä½¿ç”¨é‡"), ("ææ–™", "ç¸½ä½¿ç”¨é‡"), ("ç¶ é›»", "æ†‘è­‰"),
            ("å¤ªé™½èƒ½", "é›»åŠ›"), ("è³¼é›»", "å”è­°"), ("å†ç”Ÿ", "èƒ½æº"),
            ("å†ç”Ÿææ–™", "ç¢³æ’", "æ¸›é‡"), ("å†ç”Ÿå¡‘è† ", "æˆæœ¬"), ("å†ç”Ÿå¡‘è† ", "ä½¿ç”¨é‡"),
            ("ææ–™", "å›æ”¶"), ("æè³ª", "åˆ†é›¢"), ("ç¢³", "æ’æ”¾"),
            ("å¡‘è† ", "ä½¿ç”¨é‡"), ("ææ–™", "ä½¿ç”¨é‡"), ("MLCC", "å›æ”¶"),
            ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å»¢æ–™", "è™•ç†"), ("å¾ªç’°", "ææ–™")
        ]
    }
    
    # ç‰¹æ®Šé—œéµå­—ï¼ˆåˆ†é¸è¾¨è¦–ã€å–®ä¸€ææ–™ç­‰ï¼‰
    SPECIAL_KEYWORDS = {
        "process_related": [
            "åˆ†é¸è¾¨è¦–", "å–®ä¸€ææ–™", "ææ–™ç´”åº¦", "å“è³ªæ§åˆ¶", "åˆ†é¡è™•ç†",
            "ææ–™è­˜åˆ¥", "è‡ªå‹•åˆ†é¸", "äººå·¥æ™ºèƒ½åˆ†é¸", "å…‰å­¸åˆ†é¸", "å¯†åº¦åˆ†é¸"
        ],
        
        "technology_related": [
            "å›æ”¶æŠ€è¡“", "è™•ç†å·¥è—", "å¾ªç’°æŠ€è¡“", "å†ç”Ÿå·¥è—", "åˆ†é›¢æŠ€è¡“",
            "ç´”åŒ–æŠ€è¡“", "æ”¹è³ªæŠ€è¡“", "é€ ç²’æŠ€è¡“", "ç†±è§£æŠ€è¡“"
        ]
    }
    
    # æ’é™¤è¦å‰‡ - æ›´ç²¾ç¢º
    EXCLUSION_RULES = {
        "exclude_topics": [
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "è·ç½", "å“¡å·¥å‚·äº¡",
            "é¦¬æ‹‰æ¾", "è³½äº‹", "é¸æ‰‹", "æ¯”è³½", "è³½è¡£", "é‹å‹•", "é«”è‚²æ´»å‹•",
            "å»¢æ°´è™•ç†", "æ°´è³ªç›£æ¸¬", "æ±¡æ°´è™•ç†", "æ°´è™•ç†ç³»çµ±",
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ", "æ¡ˆä¾‹é¸æ‹”", "å„ªè‰¯æ¡ˆä¾‹", "è¡¨æšå¤§æœƒ",
            "é‹çˆæ”¹å–„", "å¤©ç„¶æ°£ç‡ƒç‡’", "ç‡ƒæ²¹æ”¹ç‡ƒ", "è¨­å‚™æ”¹å–„", "æ©Ÿå°æ›´æ–°"
        ],
        
        "exclude_contexts": [
            "å‚ç›´é¦¬æ‹‰æ¾", "å²ä¸Šæœ€ç’°ä¿è³½è¡£", "å„ç•Œå¥½æ‰‹", "åƒèˆ‡ç››æœƒ",
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å®‰çµ±è¨ˆ", "å®‰å…¨æŒ‡æ¨™", "äº‹æ•…ç‡",
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "å„ªè‰¯æ¡ˆä¾‹é¸æ‹”", "ç¸¾å„ªéƒ¨é–€è¡¨æš",
            "é›¨æ°´å›æ”¶é‡æ¸›å°‘", "é™é›¨é‡æ¸›å°‘", "æœˆå¹³å‡é™é›¨", "æ°£è±¡è³‡æ–™",
            "ç‡ƒæ²¹æ”¹ç‡ƒæ±½é‹çˆ", "å¤©ç„¶æ°£ç‡ƒç‡’æ©Ÿ", "é‹çˆæ”¹é€ ", "è¨­å‚™æ±°æ›",
            "ç›£æ¸¬æ¬¡æ•¸", "æª¢æ¸¬é »ç‡", "åŸ·è¡Œæ¬¡æ•¸", "æŸ¥æ ¸å ´æ¬¡"
        ],
        
        "exclude_number_patterns": [
            r'è·æ¥­ç½å®³.*?\d+(?:\.\d+)?%',
            r'ç½å®³æ¯”ç‡.*?\d+(?:\.\d+)?%',
            r'é™é›¨é‡.*?\d+(?:\.\d+)?%',
            r'é›¨æ°´.*?\d+(?:\.\d+)?å™¸/æ—¥',
            r'\d+\s*ä»¶.*?æ”¹å–„æ¡ˆ',
            r'æ”¹å–„æ¡ˆ.*?\d+\s*ä»¶',
            r'\d+\s*æ¬¡.*?ç›£æ¸¬',
            r'ç›£æ¸¬.*?\d+\s*æ¬¡'
        ]
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—"""
        all_keywords = []
        
        # æ¯”ç‡é¡é—œéµå­—
        all_keywords.extend(cls.RATIO_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(cls.RATIO_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(cls.RATIO_KEYWORDS["discontinuous"])
        
        # æ•¸é‡é¡é—œéµå­—
        all_keywords.extend(cls.QUANTITY_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(cls.QUANTITY_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(cls.QUANTITY_KEYWORDS["discontinuous"])
        
        # ç‰¹æ®Šé—œéµå­—
        all_keywords.extend(cls.SPECIAL_KEYWORDS["process_related"])
        all_keywords.extend(cls.SPECIAL_KEYWORDS["technology_related"])
        
        return all_keywords

# =============================================================================
# è‚¡ç¥¨ä»£è™Ÿè­˜åˆ¥å™¨
# =============================================================================

class StockCodeExtractor:
    """è‚¡ç¥¨ä»£è™Ÿæå–å™¨"""
    
    def __init__(self):
        # å°ç£è‚¡ç¥¨ä»£è™Ÿæ¨¡å¼
        self.stock_patterns = [
            r'è‚¡ç¥¨ä»£è™Ÿ[ï¼š:]\s*(\d{4})',
            r'ä»£è™Ÿ[ï¼š:]\s*(\d{4})',
            r'è­‰åˆ¸ä»£è™Ÿ[ï¼š:]\s*(\d{4})',
            r'ä¸Šå¸‚ä»£è™Ÿ[ï¼š:]\s*(\d{4})',
            r'è‚¡ä»½ä»£è™Ÿ[ï¼š:]\s*(\d{4})',
            r'å…¬å¸ä»£è™Ÿ[ï¼š:]\s*(\d{4})',
            r'çµ±ä¸€ç·¨è™Ÿ[ï¼š:]\s*(\d{8})',  # çµ±ä¸€ç·¨è™Ÿä½œç‚ºå‚™ç”¨
        ]
        
        # å¸¸è¦‹å…¬å¸ä»£è™Ÿæ˜ å°„ï¼ˆå¯æ‰‹å‹•ç¶­è­·ï¼‰
        self.known_mappings = {
            "å°ç©é›»": "2330",
            "å—äº": "1303", 
            "å°å¡‘": "1301",
            "è¯é›»": "2303",
            "é´»æµ·": "2317",
            "å°åŒ–": "1326",
            "ä¸­æ²¹": "å°ç£ä¸­æ²¹å…¬å¸"
        }
    
    def extract_stock_code(self, text: str, company_name: str) -> str:
        """æå–è‚¡ç¥¨ä»£è™Ÿ"""
        
        # æ–¹æ³•1ï¼šå¾æ–‡æœ¬ä¸­ç›´æ¥æå–
        for pattern in self.stock_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        # æ–¹æ³•2ï¼šå¾å·²çŸ¥æ˜ å°„ä¸­æŸ¥æ‰¾
        for known_company, code in self.known_mappings.items():
            if known_company in company_name:
                return code
        
        # æ–¹æ³•3ï¼šå¾å…¬å¸åç¨±ä¸­æ¨æ¸¬ï¼ˆå•Ÿç™¼å¼ï¼‰
        if "å°ç©é›»" in company_name or "TSMC" in company_name.upper():
            return "2330"
        elif "å—äº" in company_name:
            return "1303"
        elif "å°å¡‘" in company_name and "å—äº" not in company_name:
            return "1301"
        elif "å°åŒ–" in company_name:
            return "1326"
        
        return ""  # ç„¡æ³•è­˜åˆ¥æ™‚è¿”å›ç©ºå­—ç¬¦ä¸²

# =============================================================================
# å¢å¼·çš„åŒ¹é…å¼•æ“
# =============================================================================

class EnhancedESGMatcher:
    """å¢å¼·çš„ESGæ•¸æ“šåŒ¹é…å¼•æ“"""
    
    def __init__(self):
        self.config = EnhancedKeywordConfig()
        self.max_distance = 200  # å¢åŠ æœç´¢è·é›¢
        
        # æ›´å…¨é¢çš„æ•¸å€¼åŒ¹é…æ¨¡å¼
        self.number_patterns = [
            # åŸºæœ¬æ•¸å€¼æ¨¡å¼
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|åƒ)?(?:å™¸|å…¬æ–¤|kg|KG)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:å„„|è¬|åƒ)?\s*(?:æ”¯|ä»¶|å€‹|å°|å¥—|ç­†)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:å™¸/æœˆ|å™¸/å¹´|å™¸/æ—¥|kg/æœˆ|kg/å¹´)',
            
            # èƒ½æºç›¸é—œ
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kWh|MWh|GWh|åº¦)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kW|MW|GW)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å¼µ.*?æ†‘è­‰',
            
            # æˆæœ¬ç›¸é—œ
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|å„„)?(?:å…ƒ|æ–°å°å¹£|TWD)',
            
            # ä¸€èˆ¬æ•¸å€¼
            r'\d+(?:,\d{3})*(?:\.\d+)?'
        ]
        
        # ç™¾åˆ†æ¯”æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%',
            r'\d+(?:\.\d+)?\s*ï¼…',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
            r'\d+(?:\.\d+)?\s*å€‹ç™¾åˆ†é»'
        ]
        
        # æ¯”ç‡æ¨¡å¼
        self.ratio_patterns = [
            r'\d+(?:\.\d+)?\s*:\s*\d+(?:\.\d+)?',
            r'\d+(?:\.\d+)?\s*æ¯”\s*\d+(?:\.\d+)?',
            r'\d+(?:\.\d+)?\s*å€'
        ]
    
    def comprehensive_relevance_check(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """ç¶œåˆç›¸é—œæ€§æª¢æŸ¥ - å¢å¼·ç‰ˆ"""
        text_lower = text.lower()
        
        # 1. å¼·æ’é™¤æª¢æŸ¥
        if self._is_strongly_excluded(text_lower):
            return False, 0.0, "å¼·æ’é™¤å…§å®¹"
        
        # 2. é—œéµå­—åŒ¹é…æª¢æŸ¥  
        keyword_match, keyword_confidence, keyword_details = self._match_keyword(text, keyword)
        if not keyword_match:
            return False, 0.0, "é—œéµå­—ä¸åŒ¹é…"
        
        # 3. ä¸Šä¸‹æ–‡ç›¸é—œæ€§æª¢æŸ¥
        context_score = self._enhanced_context_check(text_lower, keyword)
        
        # 4. æ•¸å€¼ç›¸é—œæ€§æª¢æŸ¥
        value_score = self._enhanced_value_check(text_lower, keyword)
        
        # 5. èªç¾©ä¸€è‡´æ€§æª¢æŸ¥
        semantic_score = self._semantic_consistency_check(text_lower, keyword)
        
        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        final_score = (
            keyword_confidence * 0.25 +
            context_score * 0.30 +
            value_score * 0.25 +
            semantic_score * 0.20
        )
        
        # å‹•æ…‹é–¾å€¼
        threshold = self._get_dynamic_threshold(keyword)
        is_relevant = final_score >= threshold
        
        details = f"é—œéµå­—:{keyword_confidence:.2f}, ä¸Šä¸‹æ–‡:{context_score:.2f}, æ•¸å€¼:{value_score:.2f}, èªç¾©:{semantic_score:.2f}"
        
        return is_relevant, final_score, details
    
    def extract_keyword_value_pairs(self, text: str, keyword: Union[str, tuple]) -> List[Tuple[str, str, float, int]]:
        """æå–é—œéµå­—èˆ‡æ•¸å€¼çš„é…å° - å¢å¼·ç‰ˆ"""
        text_lower = text.lower()
        
        # 1. æª¢æŸ¥é—œéµå­—æ˜¯å¦å­˜åœ¨
        keyword_match, keyword_confidence, _ = self._match_keyword(text, keyword)
        if not keyword_match:
            return []
        
        # 2. æ‰¾åˆ°é—œéµå­—ä½ç½®
        keyword_positions = self._get_keyword_positions(text_lower, keyword)
        if not keyword_positions:
            return []
        
        # 3. æ™ºèƒ½æ•¸å€¼æœç´¢
        valid_pairs = []
        
        for kw_start, kw_end in keyword_positions:
            # å‹•æ…‹æœç´¢çª—å£
            search_window_size = self._get_search_window_size(keyword)
            search_start = max(0, kw_start - search_window_size)
            search_end = min(len(text), kw_end + search_window_size)
            search_window = text[search_start:search_end]
            
            # æå–ä¸åŒé¡å‹çš„æ•¸å€¼
            all_values = []
            
            # æ•¸å€¼
            numbers = self._extract_enhanced_numbers(search_window)
            for num in numbers:
                all_values.append((num, 'number'))
            
            # ç™¾åˆ†æ¯”
            percentages = self._extract_enhanced_percentages(search_window)  
            for pct in percentages:
                all_values.append((pct, 'percentage'))
            
            # æ¯”ç‡
            ratios = self._extract_ratios(search_window)
            for ratio in ratios:
                all_values.append((ratio, 'ratio'))
            
            # é©—è­‰æ¯å€‹æ•¸å€¼
            for value, value_type in all_values:
                value_pos = search_window.find(value)
                if value_pos != -1:
                    actual_value_pos = search_start + value_pos
                    distance = min(abs(actual_value_pos - kw_start), abs(actual_value_pos - kw_end))
                    
                    # è¨ˆç®—é—œè¯åˆ†æ•¸
                    association_score = self._calculate_enhanced_association(
                        text, keyword, value, value_type, kw_start, kw_end, actual_value_pos
                    )
                    
                    if association_score > 0.5 and distance <= search_window_size:
                        valid_pairs.append((value, value_type, association_score, distance))
        
        # å»é‡å’Œæ’åº
        unique_pairs = self._deduplicate_value_pairs(valid_pairs)
        
        return unique_pairs[:5]  # è¿”å›æœ€å¤š5å€‹æœ€ä½³çµæœ
    
    # è¼”åŠ©æ–¹æ³•å¯¦ç¾
    def _is_strongly_excluded(self, text: str) -> bool:
        """å¼·æ’é™¤æª¢æŸ¥"""
        # æª¢æŸ¥æ’é™¤ä¸»é¡Œ
        for topic in self.config.EXCLUSION_RULES["exclude_topics"]:
            if topic in text:
                return True
        
        # æª¢æŸ¥æ’é™¤ä¸Šä¸‹æ–‡
        for context in self.config.EXCLUSION_RULES["exclude_contexts"]:
            if context in text:
                return True
        
        # æª¢æŸ¥æ’é™¤æ•¸å€¼æ¨¡å¼
        for pattern in self.config.EXCLUSION_RULES["exclude_number_patterns"]:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        return False
    
    def _enhanced_context_check(self, text: str, keyword: Union[str, tuple]) -> float:
        """å¢å¼·çš„ä¸Šä¸‹æ–‡æª¢æŸ¥"""
        context_indicators = {
            "material_related": ["ææ–™", "ç‰©æ–™", "åŸæ–™", "å¡‘è† ", "å¡‘æ–™", "èšåˆç‰©", "æ¨¹è„‚"],
            "recycling_related": ["å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "é‡è¤‡ä½¿ç”¨", "å»¢æ–™"],
            "sustainability_related": ["æ°¸çºŒ", "ç’°ä¿", "ç¶ è‰²", "æ¸›ç¢³", "ç¯€èƒ½", "ESG", "å¾ªç’°ç¶“æ¿Ÿ"],
            "quantitative_related": ["ä½¿ç”¨", "æ¶ˆè€—", "ç”¢ç”Ÿ", "è™•ç†", "è£½é€ ", "ç”Ÿç”¢", "æ‡‰ç”¨"],
            "performance_related": ["æ•ˆç‡", "æ•ˆç›Š", "æ¯”ç‡", "æ¯”ä¾‹", "æˆæ•ˆ", "ç¸¾æ•ˆ", "æ”¹å–„"]
        }
        
        total_score = 0.0
        category_weights = {
            "material_related": 0.25,
            "recycling_related": 0.25,
            "sustainability_related": 0.20,
            "quantitative_related": 0.15,
            "performance_related": 0.15
        }
        
        for category, indicators in context_indicators.items():
            category_score = 0.0
            for indicator in indicators:
                if indicator in text:
                    category_score += 1.0
            
            # æ­£è¦åŒ–åˆ†æ•¸
            normalized_score = min(category_score / len(indicators), 1.0)
            total_score += normalized_score * category_weights[category]
        
        return total_score
    
    def _enhanced_value_check(self, text: str, keyword: Union[str, tuple]) -> float:
        """å¢å¼·çš„æ•¸å€¼æª¢æŸ¥"""
        # å°‹æ‰¾æ•¸å€¼
        all_numbers = re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?', text)
        all_percentages = re.findall(r'\d+(?:\.\d+)?\s*%', text)
        
        if not all_numbers and not all_percentages:
            return 0.0
        
        value_score = 0.0
        
        # æª¢æŸ¥æ•¸å€¼åˆç†æ€§å’Œç›¸é—œæ€§
        keyword_str = keyword if isinstance(keyword, str) else " ".join(keyword)
        
        # æ ¹æ“šé—œéµå­—é¡å‹èª¿æ•´æœŸæœ›çš„æ•¸å€¼é¡å‹
        expected_value_type = self._get_expected_value_type(keyword_str)
        
        if expected_value_type == "percentage" and all_percentages:
            value_score += 0.6
        elif expected_value_type == "quantity" and all_numbers:
            value_score += 0.6
        elif expected_value_type == "mixed" and (all_numbers or all_percentages):
            value_score += 0.5
        
        # é¡å¤–åŠ åˆ†ï¼šæ•¸å€¼åœ¨åˆç†ç¯„åœå…§
        if all_numbers:
            for num_str in all_numbers[:3]:  # åªæª¢æŸ¥å‰3å€‹æ•¸å€¼
                try:
                    num = float(num_str.replace(',', ''))
                    if self._is_reasonable_value(num, keyword_str):
                        value_score += 0.1
                except:
                    continue
        
        return min(value_score, 1.0)
    
    def _semantic_consistency_check(self, text: str, keyword: Union[str, tuple]) -> float:
        """èªç¾©ä¸€è‡´æ€§æª¢æŸ¥"""
        keyword_str = keyword if isinstance(keyword, str) else " ".join(keyword)
        
        # æª¢æŸ¥èªç¾©ä¸€è‡´æ€§æŒ‡æ¨™
        consistency_score = 0.0
        
        # æª¢æŸ¥å‹•è©ä¸€è‡´æ€§
        if any(verb in text for verb in ["ä½¿ç”¨", "æ‡‰ç”¨", "æ¡ç”¨", "å¯¦æ–½"]):
            if any(term in keyword_str for term in ["ä½¿ç”¨", "æ‡‰ç”¨"]):
                consistency_score += 0.3
        
        # æª¢æŸ¥é‡è©ä¸€è‡´æ€§  
        if any(measure in text for measure in ["å™¸", "å…¬æ–¤", "ä»¶", "å¼µ"]):
            if "é‡" in keyword_str or "ä½¿ç”¨" in keyword_str:
                consistency_score += 0.3
        
        # æª¢æŸ¥æ¯”ç‡ä¸€è‡´æ€§
        if any(ratio in text for ratio in ["%", "ï¼…", "æ¯”ç‡", "æ¯”ä¾‹"]):
            if any(term in keyword_str for term in ["ç‡", "æ¯”", "æ•ˆç‡"]):
                consistency_score += 0.4
        
        return min(consistency_score, 1.0)
    
    def _get_dynamic_threshold(self, keyword: Union[str, tuple]) -> float:
        """å‹•æ…‹é–¾å€¼è¨ˆç®—"""
        keyword_str = keyword if isinstance(keyword, str) else " ".join(keyword)
        
        # é«˜å„ªå…ˆç´šé—œéµå­—è¼ƒä½é–¾å€¼
        if any(high_priority in keyword_str for high_priority in 
               ["å†ç”Ÿææ–™", "å¾ªç’°ç‡", "å›æ”¶ç‡", "ç¢³æ’æ¸›é‡", "ä½¿ç”¨é‡"]):
            return 0.6
        
        # ä¸­å„ªå…ˆç´šé—œéµå­—ä¸­ç­‰é–¾å€¼
        elif any(mid_priority in keyword_str for mid_priority in 
                 ["ææ–™", "èƒ½æº", "æ•ˆç›Š", "æˆæœ¬"]):
            return 0.65
        
        # ä¸€èˆ¬é—œéµå­—è¼ƒé«˜é–¾å€¼
        else:
            return 0.7
    
    def _extract_enhanced_numbers(self, text: str) -> List[str]:
        """å¢å¼·çš„æ•¸å€¼æå–"""
        numbers = []
        
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        # å»é‡
        return list(set(numbers))
    
    def _extract_enhanced_percentages(self, text: str) -> List[str]:
        """å¢å¼·çš„ç™¾åˆ†æ¯”æå–"""
        percentages = []
        
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            percentages.extend(matches)
        
        return list(set(percentages))
    
    def _extract_ratios(self, text: str) -> List[str]:
        """æå–æ¯”ç‡"""
        ratios = []
        
        for pattern in self.ratio_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            ratios.extend(matches)
        
        return list(set(ratios))
    
    def _get_expected_value_type(self, keyword: str) -> str:
        """æ ¹æ“šé—œéµå­—æ¨æ¸¬æœŸæœ›çš„æ•¸å€¼é¡å‹"""
        if any(ratio_term in keyword for ratio_term in ["ç‡", "æ¯”", "%", "æ•ˆç‡", "æ¯”ä¾‹"]):
            return "percentage"
        elif any(quantity_term in keyword for quantity_term in ["é‡", "ä½¿ç”¨", "ç”¢ç”Ÿ", "è™•ç†", "æˆæœ¬"]):
            return "quantity"
        else:
            return "mixed"
    
    def _is_reasonable_value(self, value: float, keyword: str) -> bool:
        """æª¢æŸ¥æ•¸å€¼åˆç†æ€§"""
        # æ ¹æ“šé—œéµå­—é¡å‹æª¢æŸ¥æ•¸å€¼ç¯„åœ
        if "ç‡" in keyword or "æ¯”" in keyword:
            return 0 <= value <= 100  # æ¯”ç‡é€šå¸¸0-100%
        elif "ä½¿ç”¨é‡" in keyword or "è™•ç†é‡" in keyword:
            return 0 < value < 1000000  # ä½¿ç”¨é‡æ‡‰ç‚ºæ­£æ•¸ä¸”åˆç†
        elif "æˆæœ¬" in keyword:
            return 0 < value  # æˆæœ¬æ‡‰ç‚ºæ­£æ•¸
        else:
            return value >= 0  # ä¸€èˆ¬æƒ…æ³ä¸‹éè² æ•¸
    
    def _get_search_window_size(self, keyword: Union[str, tuple]) -> int:
        """å‹•æ…‹æœç´¢çª—å£å¤§å°"""
        keyword_str = keyword if isinstance(keyword, str) else " ".join(keyword)
        
        # è¤‡é›œé—œéµå­—éœ€è¦æ›´å¤§çš„æœç´¢çª—å£
        if isinstance(keyword, tuple) or len(keyword_str) > 10:
            return 150
        else:
            return 100
    
    def _calculate_enhanced_association(self, text: str, keyword: Union[str, tuple], 
                                      value: str, value_type: str,
                                      kw_start: int, kw_end: int, value_pos: int) -> float:
        """å¢å¼·çš„é—œè¯åº¦è¨ˆç®—"""
        
        # è·é›¢å› å­
        distance = min(abs(value_pos - kw_start), abs(value_pos - kw_end))
        distance_score = max(0, 1.0 - distance / 100.0)
        
        # é¡å‹åŒ¹é…åˆ†æ•¸
        keyword_str = keyword if isinstance(keyword, str) else " ".join(keyword)
        expected_type = self._get_expected_value_type(keyword_str)
        
        type_score = 1.0
        if expected_type == "percentage" and value_type != "percentage":
            type_score = 0.7
        elif expected_type == "quantity" and value_type == "percentage":
            type_score = 0.8
        
        # ä¸Šä¸‹æ–‡åˆ†æ•¸
        context_start = min(kw_start, value_pos) - 50
        context_end = max(kw_end, value_pos + len(value)) + 50
        context_start = max(0, context_start)
        context_end = min(len(text), context_end)
        context = text[context_start:context_end].lower()
        
        context_score = self._calculate_local_context_score(context, keyword_str, value_type)
        
        # ç¶œåˆåˆ†æ•¸
        final_score = (
            distance_score * 0.4 +
            type_score * 0.3 +
            context_score * 0.3
        )
        
        return final_score
    
    def _calculate_local_context_score(self, context: str, keyword: str, value_type: str) -> float:
        """è¨ˆç®—å±€éƒ¨ä¸Šä¸‹æ–‡åˆ†æ•¸"""
        score = 0.0
        
        # æª¢æŸ¥æ”¯æŒæ€§è©å½™
        support_words = ["ç‚º", "é”", "ç´„", "å…±", "ç¸½è¨ˆ", "åˆè¨ˆ", "é”åˆ°", "å¯¦ç¾"]
        for word in support_words:
            if word in context:
                score += 0.1
        
        # æª¢æŸ¥ç›¸é—œå‹•è©
        relevant_verbs = ["ä½¿ç”¨", "ç”¢ç”Ÿ", "è™•ç†", "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "æ›¿ä»£", "æ¸›å°‘", "å¢åŠ "]
        for verb in relevant_verbs:
            if verb in context:
                score += 0.15
        
        # æ ¹æ“šæ•¸å€¼é¡å‹èª¿æ•´
        if value_type == "percentage":
            if any(pct_word in context for pct_word in ["æå‡", "æ”¹å–„", "å¢é•·", "ä¸‹é™", "æ¸›å°‘"]):
                score += 0.2
        
        return min(score, 1.0)
    
    def _deduplicate_value_pairs(self, pairs: List[Tuple[str, str, float, int]]) -> List[Tuple[str, str, float, int]]:
        """å»é‡æ•¸å€¼é…å°"""
        unique_pairs = []
        seen_values = set()
        
        # æŒ‰é—œè¯åˆ†æ•¸æ’åº
        sorted_pairs = sorted(pairs, key=lambda x: x[2], reverse=True)
        
        for value, value_type, score, distance in sorted_pairs:
            # æ¨™æº–åŒ–æ•¸å€¼ç”¨æ–¼å»é‡æ¯”è¼ƒ
            normalized_value = re.sub(r'\s+', '', value.lower())
            
            if normalized_value not in seen_values:
                seen_values.add(normalized_value)
                unique_pairs.append((value, value_type, score, distance))
        
        return unique_pairs
    
    # å…¶ä»–å¿…è¦çš„è¼”åŠ©æ–¹æ³•ä¿æŒä¸è®Š
    def _match_keyword(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """é—œéµå­—åŒ¹é…"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            if keyword.lower() in text_lower:
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {keyword}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            positions = []
            
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            distance = max(positions) - min(positions)
            
            if distance <= 80:
                return True, 0.9, f"è¿‘è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 150:
                return True, 0.8, f"ä¸­è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= self.max_distance:
                return True, 0.7, f"é è·é›¢åŒ¹é…({distance}å­—)"
            else:
                return True, 0.5, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—)"
        
        return False, 0.0, ""
    
    def _get_keyword_positions(self, text: str, keyword: Union[str, tuple]) -> List[Tuple[int, int]]:
        """ç²å–é—œéµå­—ä½ç½®"""
        positions = []
        
        if isinstance(keyword, str):
            keyword_lower = keyword.lower()
            start = 0
            while True:
                pos = text.find(keyword_lower, start)
                if pos == -1:
                    break
                positions.append((pos, pos + len(keyword_lower)))
                start = pos + 1
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            component_positions = {}
            
            for comp in components:
                comp_positions = []
                start = 0
                while True:
                    pos = text.find(comp, start)
                    if pos == -1:
                        break
                    comp_positions.append((pos, pos + len(comp)))
                    start = pos + 1
                component_positions[comp] = comp_positions
            
            # æ‰¾åˆ°æ‰€æœ‰çµ„ä»¶éƒ½åœ¨åˆç†è·é›¢å…§çš„çµ„åˆ
            for comp1_pos in component_positions.get(components[0], []):
                for comp2_pos in component_positions.get(components[1], []):
                    distance = abs(comp1_pos[0] - comp2_pos[0])
                    if distance <= self.max_distance:
                        start_pos = min(comp1_pos[0], comp2_pos[0])
                        end_pos = max(comp1_pos[1], comp2_pos[1])
                        positions.append((start_pos, end_pos))
        
        return positions

# =============================================================================
# Wordæ–‡ä»¶è¼¸å‡ºå™¨
# =============================================================================

class WordDocumentExporter:
    """Wordæ–‡æª”è¼¸å‡ºå™¨"""
    
    def __init__(self):
        self.font_name = "æ¨™æ¥·é«”"
        self.font_size_title = 16
        self.font_size_heading = 14
        self.font_size_body = 12
    
    def create_word_document(self, extractions: List[NumericExtraction], 
                           doc_info: DocumentInfo, summary: ProcessingSummary) -> str:
        """å‰µå»ºWordæ–‡æª”"""
        
        # å‰µå»ºæ–‡æª”
        doc = WordDocument()
        
        # è¨­ç½®é é¢æ ¼å¼
        sections = doc.sections
        for section in sections:
            section.page_height = Cm(29.7)  # A4é«˜åº¦
            section.page_width = Cm(21.0)   # A4å¯¬åº¦
            section.left_margin = Cm(2.5)
            section.right_margin = Cm(2.5)
            section.top_margin = Cm(2.5)
            section.bottom_margin = Cm(2.5)
        
        # æ¨™é¡Œ
        title = f"{doc_info.stock_code}_{doc_info.company_name}_{doc_info.report_year}_æå–çµ±æ•´"
        title_para = doc.add_heading(title, level=0)
        title_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ‘˜è¦ä¿¡æ¯
        summary_para = doc.add_paragraph()
        summary_para.add_run(f"æå–æ—¥æœŸï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n").bold = True
        summary_para.add_run(f"æå–çµæœç¸½æ•¸ï¼š{len(extractions)} é …\n").bold = True  
        summary_para.add_run(f"è™•ç†æ™‚é–“ï¼š{summary.processing_time:.2f} ç§’\n").bold = True
        
        # æ·»åŠ åˆ†éš”ç·š
        doc.add_paragraph("=" * 60)
        
        if not extractions:
            # å¦‚æœæ²’æœ‰æå–çµæœ
            no_data_para = doc.add_paragraph()
            no_data_para.add_run("æœªæ‰¾åˆ°ç›¸é—œæ•¸æ“š").bold = True
            no_data_para.add_run("\n\nå¯èƒ½çš„åŸå› ï¼š\n")
            no_data_para.add_run("1. è©²å…¬å¸ESGå ±å‘Šä¸­æœªåŒ…å«ç›¸é—œçš„å†ç”Ÿææ–™æˆ–å¾ªç’°ç¶“æ¿Ÿæ•¸æ“š\n")
            no_data_para.add_run("2. ç›¸é—œæ•¸æ“šå­˜åœ¨ä½†é—œéµå­—åŒ¹é…æœªèƒ½è­˜åˆ¥\n")  
            no_data_para.add_run("3. æ•¸æ“šæ ¼å¼ç‰¹æ®Šï¼Œéœ€è¦èª¿æ•´æå–è¦å‰‡\n")
        else:
            # æŒ‰é ç¢¼æ’åº
            sorted_extractions = sorted(extractions, key=lambda x: (x.page_number, x.paragraph_number))
            
            for i, extraction in enumerate(sorted_extractions, 1):
                # æ¯å€‹æå–çµæœä½œç‚ºä¸€å€‹æ®µè½çµ„
                
                # é ç¢¼
                page_para = doc.add_paragraph()
                page_run = page_para.add_run(f"é ç¢¼ï¼š{extraction.page_number}")
                page_run.bold = True
                page_run.font.size = 14
                
                # é—œéµå­—
                keyword_para = doc.add_paragraph()
                keyword_run = keyword_para.add_run(f"é—œéµå­—ï¼š{extraction.keyword}")
                keyword_run.bold = True
                
                # æ•¸å€¼
                value_para = doc.add_paragraph()
                if extraction.value == "[ç›¸é—œæè¿°]":
                    value_run = value_para.add_run("æ•¸å€¼ï¼šç›¸é—œæè¿°å…§å®¹ï¼ˆç„¡å…·é«”æ•¸å€¼ï¼‰")
                else:
                    value_run = value_para.add_run(f"æ•¸å€¼ï¼š{extraction.value} {extraction.unit}")
                value_run.font.color.rgb = RGBColor(0, 0, 255)  # è—è‰²
                
                # ä¿¡å¿ƒåˆ†æ•¸
                confidence_para = doc.add_paragraph()
                confidence_run = confidence_para.add_run(f"ä¿¡å¿ƒåˆ†æ•¸ï¼š{extraction.confidence:.3f}")
                if extraction.confidence >= 0.8:
                    confidence_run.font.color.rgb = RGBColor(0, 128, 0)  # ç¶ è‰²
                elif extraction.confidence >= 0.6:
                    confidence_run.font.color.rgb = RGBColor(255, 165, 0)  # æ©™è‰²
                else:
                    confidence_run.font.color.rgb = RGBColor(255, 0, 0)  # ç´…è‰²
                
                # æ•´å€‹æ®µè½å…§å®¹
                content_para = doc.add_paragraph()
                content_run = content_para.add_run("æ•´å€‹æ®µè½å…§å®¹ï¼š")
                content_run.bold = True
                
                # æ®µè½å…§å®¹ï¼ˆç¸®æ’é¡¯ç¤ºï¼‰
                paragraph_content = extraction.full_section if extraction.full_section else extraction.paragraph
                content_detail_para = doc.add_paragraph(paragraph_content)
                content_detail_para.style.paragraph_format.left_indent = Cm(1)  # ç¸®æ’1å…¬åˆ†
                
                # åˆ†éš”ç·šï¼ˆé™¤äº†æœ€å¾Œä¸€é …ï¼‰
                if i < len(sorted_extractions):
                    doc.add_paragraph("-" * 80)
        
        # ä¿å­˜æ–‡æª”
        safe_company = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
        filename = f"{doc_info.stock_code}_{safe_company}_{doc_info.report_year}_æå–çµ±æ•´.docx"
        output_path = os.path.join(RESULTS_PATH, filename)
        
        doc.save(output_path)
        
        return output_path

# =============================================================================
# ä¸»æå–å™¨é¡æ›´æ–°
# =============================================================================

class EnhancedESGExtractor:
    """å¢å¼·çš„ESGå ±å‘Šæ›¸æå–å™¨"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = EnhancedESGMatcher()
        self.keyword_config = EnhancedKeywordConfig()
        self.stock_extractor = StockCodeExtractor()
        self.word_exporter = WordDocumentExporter()
        
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 400) -> Tuple[List[NumericExtraction], ProcessingSummary, str, str]:
        """è™•ç†å–®å€‹æ–‡æª” - è¿”å›Excelå’ŒWordæ–‡ä»¶è·¯å¾‘"""
        start_time = datetime.now()
        print(f"\nğŸ“Š è™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. æ–‡æª”æª¢ç´¢ï¼ˆä½¿ç”¨æ–°é—œéµå­—ï¼‰
        documents = self._enhanced_document_retrieval(db, max_documents)
        
        # 3. è‚¡ç¥¨ä»£è™Ÿè­˜åˆ¥
        if not doc_info.stock_code:
            stock_code = self._extract_stock_code_from_documents(documents, doc_info.company_name)
            doc_info.stock_code = stock_code
        
        # 4. æ•¸æ“šæå–
        extractions = self._enhanced_extract_data(documents, doc_info)
        
        # 5. å¾Œè™•ç†
        extractions = self._enhanced_post_process(extractions)
        
        # 6. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            stock_code=doc_info.stock_code,
            total_documents=len(documents),
            stage1_passed=len(documents),
            stage2_passed=len(extractions),
            total_extractions=len(extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 7. åŒ¯å‡ºçµæœåˆ°Excel
        excel_path = self._export_to_excel(extractions, summary, doc_info)
        
        # 8. åŒ¯å‡ºçµæœåˆ°Word (æ–°å¢)
        word_path = self._export_to_word(extractions, summary, doc_info)
        
        return extractions, summary, excel_path, word_path
    
    def _enhanced_document_retrieval(self, db, max_docs: int) -> List[Document]:
        """å¢å¼·çš„æ–‡æª”æª¢ç´¢ - ä½¿ç”¨æ–°é—œéµå­—"""
        print("   ğŸ” åŸ·è¡Œå¢å¼·é—œéµå­—æª¢ç´¢...")
        
        all_docs = []
        keywords = self.keyword_config.get_all_keywords()
        
        # åˆ†çµ„æª¢ç´¢
        ratio_keywords = (self.keyword_config.RATIO_KEYWORDS["high_relevance_continuous"] + 
                         self.keyword_config.RATIO_KEYWORDS["medium_relevance_continuous"])
        quantity_keywords = (self.keyword_config.QUANTITY_KEYWORDS["high_relevance_continuous"] +
                           self.keyword_config.QUANTITY_KEYWORDS["medium_relevance_continuous"])
        
        # æ¯”ç‡é¡é—œéµå­—æª¢ç´¢
        for keyword in ratio_keywords[:15]:
            search_term = keyword
            docs = db.similarity_search(search_term, k=12)
            all_docs.extend(docs)
        
        # æ•¸é‡é¡é—œéµå­—æª¢ç´¢
        for keyword in quantity_keywords[:15]:
            search_term = keyword
            docs = db.similarity_search(search_term, k=12)
            all_docs.extend(docs)
        
        # ç‰¹æ®Šé—œéµå­—æª¢ç´¢
        special_keywords = (self.keyword_config.SPECIAL_KEYWORDS["process_related"] +
                          self.keyword_config.SPECIAL_KEYWORDS["technology_related"])
        for keyword in special_keywords[:10]:
            docs = db.similarity_search(keyword, k=8)
            all_docs.extend(docs)
        
        # ä¸»é¡Œæª¢ç´¢ï¼ˆæ›´æ–°ä¸»é¡Œï¼‰
        topic_queries = [
            "ææ–™å¾ªç’° å›æ”¶åˆ©ç”¨",
            "å†ç”Ÿèƒ½æº ä½¿ç”¨é‡",
            "ç¢³æ’æ”¾ æ¸›é‡ æ•ˆç›Š",
            "ç¶ é›»æ†‘è­‰ å¤ªé™½èƒ½",
            "ææ–™å›æ”¶ åˆ†é¸æŠ€è¡“"
        ]
        
        for query in topic_queries:
            docs = db.similarity_search(query, k=15)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹å€™é¸æ–‡æª”")
        return result_docs
    
    def _extract_stock_code_from_documents(self, documents: List[Document], company_name: str) -> str:
        """å¾æ–‡æª”ä¸­æå–è‚¡ç¥¨ä»£è™Ÿ"""
        print(f"   ğŸ” æå–è‚¡ç¥¨ä»£è™Ÿ...")
        
        # æª¢æŸ¥å‰å¹¾å€‹æ–‡æª”çš„å…§å®¹
        for doc in documents[:5]:
            stock_code = self.stock_extractor.extract_stock_code(doc.page_content, company_name)
            if stock_code:
                print(f"   âœ… æ‰¾åˆ°è‚¡ç¥¨ä»£è™Ÿ: {stock_code}")
                return stock_code
        
        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œä½¿ç”¨å…¬å¸åç¨±æ¨æ¸¬
        stock_code = self.stock_extractor.extract_stock_code("", company_name)
        if stock_code:
            print(f"   âœ… æ¨æ¸¬è‚¡ç¥¨ä»£è™Ÿ: {stock_code}")
        else:
            print(f"   âš ï¸ ç„¡æ³•è­˜åˆ¥è‚¡ç¥¨ä»£è™Ÿ")
            
        return stock_code
    
    def _enhanced_extract_data(self, documents: List[Document], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """å¢å¼·çš„æ•¸æ“šæå–"""
        print("ğŸ¯ åŸ·è¡Œå¢å¼·æ•¸æ“šæå–...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="å¢å¼·æ•¸æ“šæå–"):
            # æ®µè½åˆ†å‰² - æ›´æ™ºèƒ½çš„åˆ†å‰²
            paragraphs = self._enhanced_split_paragraphs(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 20:  # æé«˜æœ€å°é•·åº¦è¦æ±‚
                    continue
                
                # å°æ¯å€‹é—œéµå­—é€²è¡ŒåŒ¹é…
                for keyword in keywords:
                    # æª¢æŸ¥ç›¸é—œæ€§ - ä½¿ç”¨å¢å¼·åŒ¹é…å™¨
                    is_relevant, relevance_score, details = self.matcher.comprehensive_relevance_check(paragraph, keyword)
                    
                    if is_relevant and relevance_score > 0.6:  # ç¨å¾®é™ä½é–¾å€¼
                        # æå–æ•¸å€¼é…å°
                        value_pairs = self.matcher.extract_keyword_value_pairs(paragraph, keyword)
                        
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ•¸å€¼ä½†ç›¸é—œæ€§å¾ˆé«˜ï¼Œä¿ç•™ä½œç‚ºæè¿°
                        if not value_pairs and relevance_score > 0.8:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            # ç²å–å®Œæ•´æ®µè½å…§å®¹
                            full_section = self._get_full_section(doc.page_content, paragraph)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value="[ç›¸é—œæè¿°]",
                                value_type='description',
                                unit='',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                stock_code=doc_info.stock_code,
                                keyword_distance=0,
                                full_section=full_section
                            )
                            extractions.append(extraction)
                        
                        # è™•ç†æ‰¾åˆ°çš„æ•¸å€¼é…å°
                        for value, value_type, association_score, distance in value_pairs:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            final_confidence = (relevance_score * 0.4 + association_score * 0.6)
                            
                            # ç²å–å®Œæ•´æ®µè½å…§å®¹
                            full_section = self._get_full_section(doc.page_content, paragraph)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=value,
                                value_type=value_type,
                                unit=self._extract_unit(value) if value_type in ['number', 'ratio'] else '%' if value_type == 'percentage' else '',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=final_confidence,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                stock_code=doc_info.stock_code,
                                keyword_distance=distance,
                                full_section=full_section
                            )
                            extractions.append(extraction)
        
        print(f"âœ… å¢å¼·æ•¸æ“šæå–å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹çµæœ")
        return extractions
    
    def _enhanced_split_paragraphs(self, text: str) -> List[str]:
        """å¢å¼·çš„æ®µè½åˆ†å‰²"""
        paragraphs = []
        
        # æ–¹æ³•1ï¼šæ¨™æº–åˆ†å‰²
        standard_paras = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs.extend([p.strip() for p in standard_paras if len(p.strip()) >= 20])
        
        # æ–¹æ³•2ï¼šå¥è™Ÿåˆ†å‰²
        sentence_paras = re.split(r'ã€‚{2,}|\.{2,}', text)
        paragraphs.extend([p.strip() for p in sentence_paras if len(p.strip()) >= 30])
        
        # æ–¹æ³•3ï¼šé …ç›®ç¬¦è™Ÿåˆ†å‰²
        bullet_paras = re.split(r'\n\s*[â€¢â–¶â– â–ª]\s*', text)
        paragraphs.extend([p.strip() for p in bullet_paras if len(p.strip()) >= 25])
        
        # æ–¹æ³•4ï¼šç·¨è™Ÿåˆ†å‰²
        number_paras = re.split(r'\n\s*\d+[\.\)]\s*', text)
        paragraphs.extend([p.strip() for p in number_paras if len(p.strip()) >= 25])
        
        # ä¿æŒåŸæ–‡
        if len(text.strip()) >= 50:
            paragraphs.append(text.strip())
        
        # å»é‡
        unique_paragraphs = []
        seen = set()
        for para in paragraphs:
            para_hash = hash(para[:100])  # ç”¨å‰100å­—ç¬¦ä½œç‚ºå»é‡ä¾æ“š
            if para_hash not in seen and len(para.strip()) >= 20:
                seen.add(para_hash)
                unique_paragraphs.append(para)
        
        return unique_paragraphs
    
    def _get_full_section(self, full_text: str, target_paragraph: str, expand_size: int = 300) -> str:
        """ç²å–å®Œæ•´æ®µè½å…§å®¹ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph
            
            # å‘å‰æ“´å±•åˆ°æ®µè½é–‹å§‹
            start = pos
            while start > 0 and full_text[start-1] not in ['\n', '\r']:
                start -= 1
            
            # å‘å¾Œæ“´å±•åˆ°æ®µè½çµæŸ
            end = pos + len(target_paragraph)
            while end < len(full_text) and full_text[end] not in ['\n', '\r']:
                end += 1
            
            # é€²ä¸€æ­¥æ“´å±•ä¸Šä¸‹æ–‡
            start = max(0, start - expand_size)
            end = min(len(full_text), end + expand_size)
            
            return full_text[start:end].strip()
        except:
            return target_paragraph
    
    def _enhanced_post_process(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å¢å¼·çš„å¾Œè™•ç†"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”§ å¢å¼·å¾Œè™•ç† {len(extractions)} å€‹æå–çµæœ...")
        
        # 1. ç²¾ç¢ºå»é‡ï¼ˆæ›´åš´æ ¼ï¼‰
        unique_extractions = []
        seen_combinations = set()
        
        for extraction in extractions:
            identifier = (
                extraction.keyword,
                extraction.value,
                extraction.value_type,
                extraction.paragraph[:150],  # å¢åŠ æ¯”è¼ƒé•·åº¦
                extraction.page_number
            )
            
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_extractions.append(extraction)
        
        print(f"ğŸ“Š ç²¾ç¢ºå»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # 2. ä¿¡å¿ƒåˆ†æ•¸ç¯©é¸ï¼ˆå‹•æ…‹é–¾å€¼ï¼‰
        filtered_extractions = []
        for extraction in unique_extractions:
            dynamic_threshold = self._get_post_process_threshold(extraction.keyword)
            if extraction.confidence >= dynamic_threshold:
                filtered_extractions.append(extraction)
        
        print(f"ğŸ“Š ä¿¡å¿ƒåˆ†æ•¸ç¯©é¸å¾Œ: {len(filtered_extractions)} å€‹çµæœ")
        
        # 3. é é¢ç´šå»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™3ç­†é«˜è³ªé‡çµæœï¼‰
        page_filtered_extractions = self._enhanced_per_page_filtering(filtered_extractions, max_per_page=3)
        
        # 4. æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        page_filtered_extractions.sort(key=lambda x: x.confidence, reverse=True)
        
        print(f"âœ… å¢å¼·å¾Œè™•ç†å®Œæˆ: ä¿ç•™ {len(page_filtered_extractions)} å€‹æœ€çµ‚çµæœ")
        return page_filtered_extractions
    
    def _get_post_process_threshold(self, keyword: str) -> float:
        """ç²å–å¾Œè™•ç†çš„å‹•æ…‹é–¾å€¼"""
        # é«˜åƒ¹å€¼é—œéµå­—è¼ƒä½é–¾å€¼
        high_value_keywords = ["ä½¿ç”¨é‡", "å¾ªç’°ç‡", "å›æ”¶ç‡", "ç¢³æ’æ¸›é‡", "å†ç”Ÿææ–™"]
        if any(hvk in keyword for hvk in high_value_keywords):
            return 0.65
        
        # ä¸­åƒ¹å€¼é—œéµå­—ä¸­ç­‰é–¾å€¼
        mid_value_keywords = ["ææ–™", "èƒ½æº", "æ•ˆç›Š", "æˆæœ¬", "è™•ç†"]
        if any(mvk in keyword for mvk in mid_value_keywords):
            return 0.7
        
        # å…¶ä»–é—œéµå­—è¼ƒé«˜é–¾å€¼
        return 0.75
    
    def _enhanced_per_page_filtering(self, extractions: List[NumericExtraction], max_per_page: int = 3) -> List[NumericExtraction]:
        """å¢å¼·çš„æŒ‰é é¢å»é‡"""
        if not extractions:
            return extractions
        
        print(f"ğŸ“„ åŸ·è¡Œå¢å¼·æŒ‰é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™ {max_per_page} ç­†ï¼‰...")
        
        # æŒ‰é ç¢¼åˆ†çµ„
        page_groups = {}
        for extraction in extractions:
            page_key = str(extraction.page_number).strip()
            if page_key not in page_groups:
                page_groups[page_key] = []
            page_groups[page_key].append(extraction)
        
        # æ¯é é¢å…§æŒ‰è³ªé‡æ’åºä¸¦ä¿ç•™æœ€ä½³çµæœ
        filtered_extractions = []
        
        for page_key, page_extractions in page_groups.items():
            # æŒ‰ä¿¡å¿ƒåˆ†æ•¸å’Œé—œéµå­—é‡è¦æ€§æ’åº
            page_extractions.sort(key=lambda x: (
                x.confidence,  # ä¿¡å¿ƒåˆ†æ•¸
                self._get_keyword_importance(x.keyword),  # é—œéµå­—é‡è¦æ€§
                len(x.value) if x.value != "[ç›¸é—œæè¿°]" else 0  # æ•¸å€¼é•·åº¦
            ), reverse=True)
            
            # ä¿ç•™é ‚éƒ¨çµæœï¼Œä½†è¦é¿å…ç›¸åŒé—œéµå­—é‡è¤‡
            kept_extractions = []
            used_keywords = set()
            
            for extraction in page_extractions:
                if len(kept_extractions) >= max_per_page:
                    break
                
                # æª¢æŸ¥é—œéµå­—æ˜¯å¦é‡è¤‡ï¼ˆå…è¨±å°‘é‡é‡è¤‡ï¼‰
                if extraction.keyword not in used_keywords or len(used_keywords) < max_per_page // 2:
                    kept_extractions.append(extraction)
                    used_keywords.add(extraction.keyword)
            
            filtered_extractions.extend(kept_extractions)
        
        print(f"   âœ… å¢å¼·é é¢å»é‡å®Œæˆ: {len(filtered_extractions)} ç­†æœ€çµ‚çµæœ")
        return filtered_extractions
    
    def _get_keyword_importance(self, keyword: str) -> float:
        """ç²å–é—œéµå­—é‡è¦æ€§åˆ†æ•¸"""
        # é«˜é‡è¦æ€§é—œéµå­—
        high_importance = ["å†ç”Ÿææ–™ä½¿ç”¨é‡", "ææ–™å¾ªç’°ç‡", "ç¢³æ’æ¸›é‡", "å›æ”¶ç‡"]
        if any(hi in keyword for hi in high_importance):
            return 1.0
        
        # ä¸­é‡è¦æ€§é—œéµå­—
        mid_importance = ["ä½¿ç”¨é‡", "è™•ç†é‡", "æ•ˆç›Š", "æˆæœ¬"]
        if any(mi in keyword for mi in mid_importance):
            return 0.8
        
        # ä¸€èˆ¬é‡è¦æ€§
        return 0.6
    
    def _export_to_word(self, extractions: List[NumericExtraction], 
                       summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
        """åŒ¯å‡ºåˆ°Wordæ–‡æª”"""
        print(f"ğŸ“„ åŒ¯å‡ºçµæœåˆ°Wordæ–‡æª”...")
        
        try:
            word_path = self.word_exporter.create_word_document(extractions, doc_info, summary)
            print(f"âœ… Wordæ–‡æª”å·²ä¿å­˜: {Path(word_path).name}")
            return word_path
        except Exception as e:
            print(f"âŒ Wordæ–‡æª”åŒ¯å‡ºå¤±æ•—: {e}")
            return ""
    
    # å…¶ä»–å¿…è¦æ–¹æ³•ä¿æŒä¸è®Šæˆ–ç•¥ä½œèª¿æ•´...
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excelï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        company_safe = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
        
        # æ ¹æ“šæå–çµæœæ•¸é‡æ±ºå®šæª”å
        if len(extractions) == 0:
            output_filename = f"ESGæå–çµæœ_ç„¡æå–_{company_safe}_{doc_info.report_year}.xlsx"
            status_message = "ç„¡æå–çµæœ"
        else:
            output_filename = f"ESGæå–çµæœ_{company_safe}_{doc_info.report_year}.xlsx"
            status_message = f"æå–çµæœ: {len(extractions)} é …"
        
        output_path = os.path.join(RESULTS_PATH, output_filename)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºçµæœåˆ°Excel: {output_filename}")
        
        # ExcelåŒ¯å‡ºé‚è¼¯ï¼ˆèˆ‡åŸç‰ˆæœ¬ç›¸ä¼¼ï¼Œä½†åŒ…å«è‚¡ç¥¨ä»£è™Ÿï¼‰
        main_data = []
        
        # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯
        header_row = {
            'è‚¡ç¥¨ä»£è™Ÿ': doc_info.stock_code,
            'å…¬å¸åç¨±': doc_info.company_name,
            'å ±å‘Šå¹´åº¦': doc_info.report_year,
            'é—œéµå­—': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            'æå–æ•¸å€¼': f"{status_message}ï¼ˆå¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨ v2.0ï¼‰",
            'æ•¸æ“šé¡å‹': '',
            'å–®ä½': '',
            'æ®µè½å…§å®¹': '',
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': ''
        }
        main_data.append(header_row)
        
        # ç©ºè¡Œåˆ†éš”
        main_data.append({col: '' for col in header_row.keys()})
        
        # å¦‚æœæœ‰æå–çµæœï¼Œæ·»åŠ çµæœæ•¸æ“š
        if len(extractions) > 0:
            for extraction in extractions:
                main_data.append({
                    'è‚¡ç¥¨ä»£è™Ÿ': extraction.stock_code,
                    'å…¬å¸åç¨±': extraction.company_name,
                    'å ±å‘Šå¹´åº¦': extraction.report_year,
                    'é—œéµå­—': extraction.keyword,
                    'æå–æ•¸å€¼': extraction.value,
                    'æ•¸æ“šé¡å‹': extraction.value_type,
                    'å–®ä½': extraction.unit,
                    'æ®µè½å…§å®¹': extraction.paragraph,
                    'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                    'é ç¢¼': extraction.page_number,
                    'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3)
                })
        else:
            # ç„¡çµæœèªªæ˜
            no_result_row = {
                'è‚¡ç¥¨ä»£è™Ÿ': doc_info.stock_code,
                'å…¬å¸åç¨±': doc_info.company_name,
                'å ±å‘Šå¹´åº¦': doc_info.report_year,
                'é—œéµå­—': 'ç„¡ç›¸é—œé—œéµå­—åŒ¹é…',
                'æå–æ•¸å€¼': 'N/A',
                'æ•¸æ“šé¡å‹': 'no_data',
                'å–®ä½': '',
                'æ®µè½å…§å®¹': 'åœ¨æ­¤ä»½ESGå ±å‘Šä¸­æœªæ‰¾åˆ°ç›¸é—œçš„æ•¸å€¼æ•¸æ“š',
                'æ®µè½ç·¨è™Ÿ': '',
                'é ç¢¼': '',
                'ä¿¡å¿ƒåˆ†æ•¸': 0.0
            }
            main_data.append(no_result_row)
        
        # çµ±è¨ˆæ•¸æ“š
        stats_data = []
        if len(extractions) > 0:
            for keyword, count in summary.keywords_found.items():
                keyword_extractions = [e for e in extractions if e.keyword == keyword]
                
                stats_data.append({
                    'é—œéµå­—': keyword,
                    'æå–æ•¸é‡': count,
                    'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                    'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
                })
        else:
            stats_data.append({
                'é—œéµå­—': 'æœå°‹æ‘˜è¦',
                'æå–æ•¸é‡': 0,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': 0.0,
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': 0.0
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ä¸»è¦çµæœå·¥ä½œè¡¨
            sheet_name = 'å¢å¼·æå–çµæœ' if len(extractions) > 0 else 'ç„¡æå–çµæœ'
            pd.DataFrame(main_data).to_excel(writer, sheet_name=sheet_name, index=False)
            
            # çµ±è¨ˆå·¥ä½œè¡¨
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='çµ±è¨ˆæ‘˜è¦', index=False)
            
            # è™•ç†æ‘˜è¦
            summary_data = [{
                'è‚¡ç¥¨ä»£è™Ÿ': summary.stock_code,
                'å…¬å¸åç¨±': summary.company_name,
                'å ±å‘Šå¹´åº¦': summary.report_year,
                'ç¸½æ–‡æª”æ•¸': summary.total_documents,
                'ç¸½æå–çµæœ': summary.total_extractions,
                'è™•ç†ç‹€æ…‹': 'æˆåŠŸæå–' if len(extractions) > 0 else 'ç„¡ç›¸é—œæ•¸æ“š',
                'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
                'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'æå–å™¨ç‰ˆæœ¬': 'å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨ v2.0'
            }]
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        if len(extractions) > 0:
            print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼ŒåŒ…å« {len(extractions)} é …æå–çµæœ")
        else:
            print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼Œæ¨™è¨˜ç‚ºç„¡æå–çµæœ")
        
        return output_path
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 200) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:400]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:400]

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦ç”¨"""
    print("ğŸ“Š å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨æ¸¬è©¦æ¨¡å¼")
    
    extractor = EnhancedESGExtractor(enable_llm=False)
    print("âœ… å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    main()