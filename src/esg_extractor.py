#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ v2.0
æ•´åˆå…©æ®µå¼ç¯©é¸ã€ä¸é€£çºŒé—œéµå­—åŒ¹é…ã€LLMå¢å¼·ã€æ™ºèƒ½å»é‡
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from tqdm import tqdm
import numpy as np
from difflib import SequenceMatcher

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class ExtractionMatch:
    """å–®å€‹åŒ¹é…çµæœ"""
    keyword: str
    keyword_type: str  # 'continuous' or 'discontinuous'
    confidence: float
    matched_text: str
    
@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# é—œéµå­—é…ç½®é¡
# =============================================================================

class KeywordConfig:
    """é—œéµå­—é…ç½®ç®¡ç†é¡"""
    
    # ç°¡åŒ–çš„å››å€‹æ ¸å¿ƒé—œéµå­—
    CORE_KEYWORDS = {
        "å†ç”Ÿå¡‘è† ææ–™": {
            "continuous": [
                "å†ç”Ÿå¡‘è† ",
                "å†ç”Ÿå¡‘æ–™", 
                "å†ç”Ÿæ–™",
                "å†ç”Ÿpp"
            ],
            "discontinuous": [
                ("å†ç”Ÿ", "å¡‘è† "),
                ("å†ç”Ÿ", "å¡‘æ–™"),
                ("å†ç”Ÿ", "PP"),
                ("PP", "å›æ”¶"),
                ("PP", "å†ç”Ÿ"),
                ("PP", "æ£§æ¿", "å›æ”¶"),
                ("å¡‘è† ", "å›æ”¶"),
                ("å¡‘æ–™", "å›æ”¶"),
                ("PCR", "å¡‘è† "),
                ("PCR", "å¡‘æ–™"),
                ("PCR", "ææ–™"),
                ("å›æ”¶", "å¡‘è† "),
                ("å›æ”¶", "å¡‘æ–™"),
                ("rPET", "å«é‡"),
                ("å†ç”Ÿ", "ææ–™"),
                ("MLCC", "å›æ”¶"),
                ("å›æ”¶", "ç”¢èƒ½")
            ]
        }
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—ï¼ˆé€£çºŒ+ä¸é€£çºŒï¼‰"""
        all_keywords = []
        for category in cls.CORE_KEYWORDS.values():
            all_keywords.extend(category["continuous"])
            all_keywords.extend(category["discontinuous"])
        return all_keywords
    
    @classmethod
    def get_keyword_category(cls, keyword: Union[str, tuple]) -> str:
        """ç²å–é—œéµå­—æ‰€å±¬é¡åˆ¥"""
        for category_name, category_data in cls.CORE_KEYWORDS.items():
            if keyword in category_data["continuous"] or keyword in category_data["discontinuous"]:
                return category_name
        return "æœªçŸ¥é¡åˆ¥"

# =============================================================================
# å¢å¼·åŒ¹é…å¼•æ“
# =============================================================================

class EnhancedMatcher:
    """å¢å¼·çš„é—œéµå­—åŒ¹é…å¼•æ“"""
    
    def __init__(self, max_distance: int = 150):
        self.max_distance = max_distance
        
        # æ•¸å€¼åŒ¹é…æ¨¡å¼ï¼ˆæ›´å…¨é¢ï¼‰
        self.number_patterns = [
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:kg|KG|å…¬æ–¤|å™¸|å…‹|g|G|å…¬å…‹|è¬å™¸|åƒå™¸))',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*å™¸/æœˆ)',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*è¬|åƒ)?(?:å™¸|å…¬æ–¤|kg|g)',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:å€‹|ä»¶|æ‰¹|å°|å¥—|æ¬¡|å€))',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*ç«‹æ–¹ç±³|mÂ³)',
        ]
        
        # ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?(?:\s*%|ï¼…|ç™¾åˆ†æ¯”)',
            r'\d+(?:\.\d+)?(?:\s*æˆ)',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
        ]
    
    def match_keyword(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """
        åŒ¹é…é—œéµå­—
        
        Returns:
            Tuple[æ˜¯å¦åŒ¹é…, ä¿¡å¿ƒåˆ†æ•¸, åŒ¹é…è©³æƒ…]
        """
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            # é€£çºŒé—œéµå­—åŒ¹é…
            if keyword.lower() in text_lower:
                # å°‹æ‰¾ç²¾ç¢ºåŒ¹é…ä½ç½®ï¼Œæä¾›ä¸Šä¸‹æ–‡
                pos = text_lower.find(keyword.lower())
                start = max(0, pos - 20)
                end = min(len(text), pos + len(keyword) + 20)
                context = text[start:end]
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {context}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            # ä¸é€£çºŒé—œéµå­—åŒ¹é…
            components = [comp.lower() for comp in keyword]
            positions = []
            
            # æ‰¾åˆ°æ¯å€‹çµ„ä»¶çš„ä½ç½®
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            # è¨ˆç®—è·é›¢å’Œä¿¡å¿ƒåˆ†æ•¸
            min_pos = min(positions)
            max_pos = max(positions)
            distance = max_pos - min_pos
            
            # æä¾›åŒ¹é…ä¸Šä¸‹æ–‡
            start = max(0, min_pos - 30)
            end = min(len(text), max_pos + 30)
            context = text[start:end]
            
            if distance <= 30:
                return True, 0.95, f"è¿‘è·é›¢åŒ¹é…({distance}å­—): {context}"
            elif distance <= 80:
                return True, 0.85, f"ä¸­è·é›¢åŒ¹é…({distance}å­—): {context}"
            elif distance <= self.max_distance:
                return True, 0.7, f"é è·é›¢åŒ¹é…({distance}å­—): {context}"
            else:
                return True, 0.5, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—): {context}"
        
        return False, 0.0, ""
    
    def extract_numbers_and_percentages(self, text: str) -> Tuple[List[str], List[str]]:
        """æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”"""
        numbers = []
        percentages = []
        
        # æå–æ•¸å€¼
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        # æå–ç™¾åˆ†æ¯”
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            percentages.extend(matches)
        
        # å»é‡ä¸¦æ’åº
        numbers = list(set(numbers))
        percentages = list(set(percentages))
        
        return numbers, percentages

# =============================================================================
# æ™ºèƒ½å»é‡å™¨
# =============================================================================

class ESGResultDeduplicator:
    """ESGæå–çµæœå»é‡å™¨"""
    
    def __init__(self, similarity_threshold: float = 0.8):
        self.similarity_threshold = similarity_threshold
        self.value_match_threshold = 0.95
    
    def deduplicate_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å»é‡æå–çµæœåˆ—è¡¨"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”„ é–‹å§‹å»é‡è™•ç†: {len(extractions)} å€‹çµæœ")
        
        # å°‡æå–çµæœè½‰æ›ç‚ºDataFrameé€²è¡Œè™•ç†
        data = []
        for i, extraction in enumerate(extractions):
            data.append({
                'index': i,
                'keyword': extraction.keyword,
                'value': extraction.value,
                'value_type': extraction.value_type,
                'paragraph': extraction.paragraph,
                'page_number': extraction.page_number,
                'confidence': extraction.confidence,
                'context_window': extraction.context_window
            })
        
        df = pd.DataFrame(data)
        
        # è­˜åˆ¥é‡è¤‡çµ„
        groups = self._group_similar_results(df)
        
        if not groups:
            print("âœ… æœªç™¼ç¾é‡è¤‡æ•¸æ“š")
            return extractions
        
        # åŸ·è¡Œå»é‡
        deduplicated_extractions = self._merge_duplicate_groups(extractions, df, groups)
        
        print(f"âœ… å»é‡å®Œæˆ: {len(extractions)} â†’ {len(deduplicated_extractions)} å€‹çµæœ")
        
        return deduplicated_extractions
    
    def deduplicate_excel_file(self, file_path: str) -> str:
        """å»é‡Excelæ–‡ä»¶"""
        print(f"ğŸ“Š è™•ç†Excelæ–‡ä»¶: {file_path}")
        
        try:
            # è¼‰å…¥Excelæ•¸æ“š
            df = self._load_excel_data(file_path)
            if df is None:
                return None
            
            # æ¨™æº–åŒ–åˆ—å
            df = self._standardize_excel_columns(df)
            
            # è­˜åˆ¥é‡è¤‡çµ„
            groups = self._group_similar_excel_results(df)
            
            if not groups:
                print("âœ… Excelæ–‡ä»¶ä¸­æœªç™¼ç¾é‡è¤‡æ•¸æ“š")
                return file_path
            
            # å‰µå»ºå»é‡å¾Œçš„DataFrame
            deduplicated_df = self._create_deduplicated_dataframe(df, groups)
            
            # ç”Ÿæˆçµ±è¨ˆæ‘˜è¦
            summary_df = self._create_summary_statistics(df, deduplicated_df)
            
            # å°å‡ºçµæœ
            output_path = self._export_deduplicated_excel(deduplicated_df, summary_df, file_path)
            
            # é¡¯ç¤ºè™•ç†æ‘˜è¦
            self._print_excel_dedup_summary(df, deduplicated_df, groups)
            
            return output_path
            
        except Exception as e:
            print(f"âŒ Excelå»é‡è™•ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _group_similar_results(self, df: pd.DataFrame) -> List[List[int]]:
        """è­˜åˆ¥ç›¸ä¼¼çš„æå–çµæœ"""
        groups = []
        processed = set()
        
        for i, row1 in df.iterrows():
            if i in processed:
                continue
            
            current_group = [i]
            value1 = self._normalize_value(row1['value'])
            paragraph1 = str(row1['paragraph'])
            
            for j, row2 in df.iterrows():
                if j <= i or j in processed:
                    continue
                
                value2 = self._normalize_value(row2['value'])
                paragraph2 = str(row2['paragraph'])
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºç›¸ä¼¼çµæœ
                is_similar = False
                
                # æ¢ä»¶1: ç›¸åŒæ•¸å€¼ + ç›¸ä¼¼æ–‡æœ¬
                if value1 == value2 and value1 != "N/A":
                    text_similarity = self._calculate_text_similarity(paragraph1, paragraph2)
                    if text_similarity > self.similarity_threshold:
                        is_similar = True
                
                # æ¢ä»¶2: å®Œå…¨ç›¸åŒçš„æ®µè½æ–‡æœ¬
                if self._calculate_text_similarity(paragraph1, paragraph2) > 0.95:
                    is_similar = True
                
                if is_similar:
                    current_group.append(j)
                    processed.add(j)
            
            if len(current_group) > 1:
                groups.append(current_group)
                for idx in current_group:
                    processed.add(idx)
        
        return groups
    
    def _group_similar_excel_results(self, df: pd.DataFrame) -> List[List[int]]:
        """è­˜åˆ¥Excelä¸­çš„ç›¸ä¼¼çµæœ"""
        groups = []
        processed = set()
        
        for i, row1 in df.iterrows():
            if i in processed:
                continue
            
            current_group = [i]
            value1 = self._normalize_value(row1['value'])
            paragraph1 = str(row1.get('paragraph', ''))
            
            for j, row2 in df.iterrows():
                if j <= i or j in processed:
                    continue
                
                value2 = self._normalize_value(row2['value'])
                paragraph2 = str(row2.get('paragraph', ''))
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºç›¸ä¼¼çµæœ
                is_similar = False
                
                # æ¢ä»¶1: ç›¸åŒæ•¸å€¼ + ç›¸ä¼¼é—œéµå­—
                if value1 == value2 and value1 not in ["N/A", "æœªæåŠ", ""]:
                    keyword_similarity = self._calculate_text_similarity(
                        str(row1.get('keyword', '')), str(row2.get('keyword', ''))
                    )
                    if keyword_similarity > 0.6:  # é—œéµå­—ç›¸ä¼¼åº¦è¼ƒä½çš„é–¾å€¼
                        is_similar = True
                
                # æ¢ä»¶2: ç›¸ä¼¼çš„æ®µè½æ–‡æœ¬ + ç›¸åŒæ•¸å€¼
                if value1 == value2 and paragraph1 and paragraph2:
                    text_similarity = self._calculate_text_similarity(paragraph1, paragraph2)
                    if text_similarity > self.similarity_threshold:
                        is_similar = True
                
                if is_similar:
                    current_group.append(j)
                    processed.add(j)
            
            if len(current_group) > 1:
                groups.append(current_group)
                for idx in current_group:
                    processed.add(idx)
        
        return groups
    
    def _merge_duplicate_groups(self, extractions: List[NumericExtraction], 
                               df: pd.DataFrame, groups: List[List[int]]) -> List[NumericExtraction]:
        """åˆä½µé‡è¤‡çš„æå–çµæœçµ„"""
        # æ”¶é›†è¢«åˆä½µçš„ç´¢å¼•
        merged_indices = set()
        for group in groups:
            merged_indices.update(group)
        
        # ä¿ç•™æœªè¢«åˆä½µçš„çµæœ
        deduplicated = []
        for i, extraction in enumerate(extractions):
            if i not in merged_indices:
                deduplicated.append(extraction)
        
        # è™•ç†æ¯å€‹é‡è¤‡çµ„
        for group in groups:
            group_extractions = [extractions[i] for i in group]
            merged_extraction = self._merge_extraction_group(group_extractions)
            deduplicated.append(merged_extraction)
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        deduplicated.sort(key=lambda x: x.confidence, reverse=True)
        
        return deduplicated
    
    def _merge_extraction_group(self, group_extractions: List[NumericExtraction]) -> NumericExtraction:
        """åˆä½µä¸€çµ„é‡è¤‡çš„æå–çµæœ"""
        # é¸æ“‡ä¿¡å¿ƒåˆ†æ•¸æœ€é«˜çš„ä½œç‚ºåŸºç¤
        best_extraction = max(group_extractions, key=lambda x: x.confidence)
        
        # åˆä½µé—œéµå­—
        keywords = [e.keyword for e in group_extractions]
        unique_keywords = list(dict.fromkeys(keywords))  # ä¿æŒé †åºå»é‡
        primary_keyword = self._select_primary_keyword(unique_keywords)
        
        # åˆä½µé ç¢¼
        pages = [e.page_number for e in group_extractions if e.page_number]
        unique_pages = list(dict.fromkeys(pages))
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
        avg_confidence = np.mean([e.confidence for e in group_extractions])
        
        # åˆä½µä¸Šä¸‹æ–‡
        contexts = [e.context_window for e in group_extractions if e.context_window]
        merged_context = "\n---åˆä½µçµæœ---\n".join(set(contexts))
        
        # å‰µå»ºåˆä½µå¾Œçš„çµæœ
        merged_extraction = NumericExtraction(
            keyword=primary_keyword,
            value=best_extraction.value,
            value_type=best_extraction.value_type,
            unit=best_extraction.unit,
            paragraph=best_extraction.paragraph,
            paragraph_number=best_extraction.paragraph_number,
            page_number=" | ".join(unique_pages),
            confidence=avg_confidence,
            context_window=f"{merged_context}\n[åˆä½µäº†{len(group_extractions)}å€‹çµæœ: {', '.join(unique_keywords)}]"
        )
        
        return merged_extraction
    
    def _select_primary_keyword(self, keywords: List[str]) -> str:
        """é¸æ“‡ä¸»è¦é—œéµå­—ï¼ˆå„ªå…ˆä¸­æ–‡ã€ç°¡æ½”ï¼‰"""
        if not keywords:
            return 'N/A'
        
        # å„ªå…ˆç´šï¼šä¸­æ–‡é—œéµå­— > è‹±æ–‡é—œéµå­—
        chinese_keywords = [k for k in keywords if re.search(r'[\u4e00-\u9fff]', k)]
        english_keywords = [k for k in keywords if not re.search(r'[\u4e00-\u9fff]', k)]
        
        if chinese_keywords:
            # é¸æ“‡æœ€çŸ­çš„ä¸­æ–‡é—œéµå­—
            return min(chinese_keywords, key=len)
        else:
            # é¸æ“‡æœ€çŸ­çš„è‹±æ–‡é—œéµå­—
            return min(english_keywords, key=len)
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if pd.isna(text1) or pd.isna(text2):
            return 0.0
        
        text1 = str(text1).strip()
        text2 = str(text2).strip()
        
        if not text1 or not text2:
            return 0.0
        
        return SequenceMatcher(None, text1, text2).ratio()
    
    def _normalize_value(self, value: str) -> str:
        """æ¨™æº–åŒ–æ•¸å€¼æ ¼å¼"""
        if pd.isna(value):
            return "N/A"
        
        value_str = str(value).strip()
        
        if not value_str or value_str.lower() in ['nan', 'none', 'null']:
            return "N/A"
        
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        value_str = re.sub(r'\s+', ' ', value_str)
        
        # æ¨™æº–åŒ–ç™¾åˆ†æ¯”
        if '%' in value_str or 'ï¼…' in value_str:
            numbers = re.findall(r'\d+\.?\d*', value_str)
            if numbers:
                return f"{numbers[0]}%"
        
        # æ¨™æº–åŒ–æ•¸å€¼å¸¶å–®ä½
        number_match = re.search(r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*([^\d\s]+)', value_str)
        if number_match:
            number, unit = number_match.groups()
            return f"{number}{unit}"
        
        return value_str
    
    def _load_excel_data(self, file_path: str) -> Optional[pd.DataFrame]:
        """è¼‰å…¥Excelæ•¸æ“š"""
        try:
            # å˜—è©¦è®€å–ä¸åŒçš„å·¥ä½œè¡¨
            possible_sheets = ['æå–çµæœ', 'extraction_results', 'results', 'Sheet1']
            
            for sheet_name in possible_sheets:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    print(f"âœ… è®€å–å·¥ä½œè¡¨: {sheet_name}")
                    return df
                except:
                    continue
            
            # å¦‚æœéƒ½å¤±æ•—ï¼Œè®€å–ç¬¬ä¸€å€‹å·¥ä½œè¡¨
            df = pd.read_excel(file_path)
            print("âœ… è®€å–ç¬¬ä¸€å€‹å·¥ä½œè¡¨")
            return df
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥Excelå¤±æ•—: {e}")
            return None
    
    def _standardize_excel_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¨™æº–åŒ–Excelåˆ—å"""
        column_mapping = {
            'é—œéµå­—': 'keyword',
            'æå–æ•¸å€¼': 'value', 
            'æ•¸æ“šé¡å‹': 'data_type',
            'æ®µè½å…§å®¹': 'paragraph',
            'æ®µè½ç·¨è™Ÿ': 'paragraph_number',
            'é ç¢¼': 'page_number',
            'ä¿¡å¿ƒåˆ†æ•¸': 'confidence',
            'ä¸Šä¸‹æ–‡': 'context',
            'æŒ‡æ¨™é¡åˆ¥': 'indicator',
            'æå–å€¼': 'value',
            'ä¾†æºé é¢': 'page_number',
            'ä¾†æºæ–‡æœ¬': 'paragraph',
            'èªªæ˜': 'explanation'
        }
        
        df_renamed = df.rename(columns=column_mapping)
        
        # ç¢ºä¿å¿…è¦åˆ—å­˜åœ¨
        required_columns = ['keyword', 'value']
        for col in required_columns:
            if col not in df_renamed.columns:
                df_renamed[col] = 'N/A'
        
        return df_renamed
    
    def _create_deduplicated_dataframe(self, df: pd.DataFrame, groups: List[List[int]]) -> pd.DataFrame:
        """å‰µå»ºå»é‡å¾Œçš„DataFrame"""
        # æ”¶é›†è¢«åˆä½µçš„ç´¢å¼•
        merged_indices = set()
        for group in groups:
            merged_indices.update(group)
        
        # æœªè¢«åˆä½µçš„è¨˜éŒ„
        unmerged_df = df[~df.index.isin(merged_indices)].copy()
        
        # å‰µå»ºåˆä½µå¾Œçš„è¨˜éŒ„
        merged_records = []
        for group in groups:
            merged_record = self._merge_excel_group(df, group)
            merged_records.append(merged_record)
        
        # åˆä½µæ•¸æ“š
        if merged_records:
            merged_df = pd.DataFrame(merged_records)
            
            # æ¨™æº–åŒ–æœªåˆä½µæ•¸æ“š
            unmerged_standardized = []
            for _, row in unmerged_df.iterrows():
                record = {
                    'keyword': str(row.get('keyword', 'N/A')),
                    'alternative_keywords': '',
                    'value': str(row.get('value', 'N/A')),
                    'data_type': str(row.get('data_type', 'N/A')),
                    'confidence': float(row.get('confidence', 0.5)) if pd.notna(row.get('confidence')) else 0.5,
                    'paragraph': str(row.get('paragraph', 'N/A')),
                    'page_number': str(row.get('page_number', 'N/A')),
                    'merged_count': 1,
                    'original_indices': str([row.name])
                }
                unmerged_standardized.append(record)
            
            if unmerged_standardized:
                unmerged_std_df = pd.DataFrame(unmerged_standardized)
                final_df = pd.concat([merged_df, unmerged_std_df], ignore_index=True)
            else:
                final_df = merged_df
        else:
            # æ²’æœ‰åˆä½µè¨˜éŒ„çš„æƒ…æ³
            unmerged_standardized = []
            for _, row in unmerged_df.iterrows():
                record = {
                    'keyword': str(row.get('keyword', 'N/A')),
                    'alternative_keywords': '',
                    'value': str(row.get('value', 'N/A')),
                    'data_type': str(row.get('data_type', 'N/A')),
                    'confidence': float(row.get('confidence', 0.5)) if pd.notna(row.get('confidence')) else 0.5,
                    'paragraph': str(row.get('paragraph', 'N/A')),
                    'page_number': str(row.get('page_number', 'N/A')),
                    'merged_count': 1,
                    'original_indices': str([row.name])
                }
                unmerged_standardized.append(record)
            
            final_df = pd.DataFrame(unmerged_standardized)
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        final_df = final_df.sort_values('confidence', ascending=False).reset_index(drop=True)
        
        return final_df
    
    def _merge_excel_group(self, df: pd.DataFrame, group_indices: List[int]) -> Dict:
        """åˆä½µExcelä¸­çš„ä¸€çµ„é‡è¤‡è¨˜éŒ„"""
        group_data = df.iloc[group_indices]
        
        # é¸æ“‡æœ€ä½³è¨˜éŒ„
        if 'confidence' in group_data.columns:
            best_idx = group_data['confidence'].idxmax()
        else:
            best_idx = group_indices[0]
        
        best_record = group_data.loc[best_idx]
        
        # åˆä½µé—œéµå­—
        keywords = [str(row.get('keyword', 'N/A')) for _, row in group_data.iterrows() 
                   if pd.notna(row.get('keyword'))]
        unique_keywords = list(dict.fromkeys(keywords))
        primary_keyword = self._select_primary_keyword(unique_keywords)
        secondary_keywords = [k for k in unique_keywords if k != primary_keyword]
        
        # åˆä½µå…¶ä»–ä¿¡æ¯
        pages = [str(row.get('page_number', 'N/A')) for _, row in group_data.iterrows() 
                if pd.notna(row.get('page_number'))]
        unique_pages = list(dict.fromkeys(pages))
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
        confidences = []
        for _, row in group_data.iterrows():
            conf = row.get('confidence')
            if pd.notna(conf) and conf != 'N/A':
                try:
                    confidences.append(float(conf))
                except:
                    pass
        
        avg_confidence = np.mean(confidences) if confidences else 0.5
        
        return {
            'keyword': primary_keyword,
            'alternative_keywords': ' | '.join(secondary_keywords) if secondary_keywords else '',
            'value': str(best_record.get('value', 'N/A')),
            'data_type': str(best_record.get('data_type', 'N/A')),
            'confidence': round(avg_confidence, 3),
            'paragraph': str(best_record.get('paragraph', 'N/A')),
            'page_number': ' | '.join(unique_pages),
            'merged_count': len(group_indices),
            'original_indices': str(group_indices)
        }
    
    def _create_summary_statistics(self, original_df: pd.DataFrame, deduplicated_df: pd.DataFrame) -> pd.DataFrame:
        """å‰µå»ºçµ±è¨ˆæ‘˜è¦"""
        stats = []
        
        # æ•´é«”çµ±è¨ˆ
        stats.append({
            'é …ç›®': 'ç¸½è¨˜éŒ„æ•¸',
            'åŸå§‹': len(original_df),
            'å»é‡å¾Œ': len(deduplicated_df),
            'æ¸›å°‘æ•¸é‡': len(original_df) - len(deduplicated_df),
            'æ¸›å°‘æ¯”ä¾‹': f"{((len(original_df) - len(deduplicated_df)) / len(original_df) * 100):.1f}%"
        })
        
        # æ•¸æ“šé¡å‹çµ±è¨ˆ
        if 'data_type' in original_df.columns:
            for data_type in original_df['data_type'].unique():
                if pd.isna(data_type):
                    continue
                
                original_count = len(original_df[original_df['data_type'] == data_type])
                deduplicated_count = len(deduplicated_df[deduplicated_df['data_type'] == data_type])
                
                stats.append({
                    'é …ç›®': f'{data_type}é¡å‹',
                    'åŸå§‹': original_count,
                    'å»é‡å¾Œ': deduplicated_count,
                    'æ¸›å°‘æ•¸é‡': original_count - deduplicated_count,
                    'æ¸›å°‘æ¯”ä¾‹': f"{((original_count - deduplicated_count) / original_count * 100):.1f}%" if original_count > 0 else "0%"
                })
        
        return pd.DataFrame(stats)
    
    def _export_deduplicated_excel(self, deduplicated_df: pd.DataFrame, 
                                  summary_df: pd.DataFrame, original_file_path: str) -> str:
        """å°å‡ºå»é‡å¾Œçš„Excel"""
        original_path = Path(original_file_path)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"{original_path.stem}_deduplicated_{timestamp}.xlsx"
        output_path = original_path.parent / output_filename
        
        # æº–å‚™å±•ç¤ºç”¨çš„DataFrame
        display_columns = {
            'keyword': 'ä¸»è¦é—œéµå­—',
            'alternative_keywords': 'å…¶ä»–ç›¸é—œé—œéµå­—',
            'value': 'æå–æ•¸å€¼',
            'data_type': 'æ•¸æ“šé¡å‹',
            'confidence': 'ä¿¡å¿ƒåˆ†æ•¸',
            'paragraph': 'æ®µè½å…§å®¹',
            'page_number': 'é ç¢¼',
            'merged_count': 'åˆä½µæ•¸é‡'
        }
        
        # é¸æ“‡å’Œé‡å‘½ååˆ—
        final_columns = ['keyword', 'alternative_keywords', 'value', 'data_type', 
                        'confidence', 'paragraph', 'page_number', 'merged_count']
        
        # ç¢ºä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
        for col in final_columns:
            if col not in deduplicated_df.columns:
                deduplicated_df[col] = 'N/A'
        
        display_df = deduplicated_df[final_columns].rename(columns=display_columns)
        
        # æˆªçŸ­éé•·æ–‡æœ¬
        if 'æ®µè½å…§å®¹' in display_df.columns:
            display_df['æ®µè½å…§å®¹'] = display_df['æ®µè½å…§å®¹'].apply(
                lambda x: str(x)[:200] + "..." if len(str(x)) > 200 else str(x)
            )
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            display_df.to_excel(writer, sheet_name='å»é‡çµæœ', index=False)
            summary_df.to_excel(writer, sheet_name='å»é‡çµ±è¨ˆ', index=False)
        
        return str(output_path)
    
    def _print_excel_dedup_summary(self, original_df: pd.DataFrame, 
                                  deduplicated_df: pd.DataFrame, groups: List[List[int]]):
        """æ‰“å°Excelå»é‡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š Excelå»é‡è™•ç†æ‘˜è¦")
        print("="*60)
        
        print(f"åŸå§‹è¨˜éŒ„æ•¸: {len(original_df)}")
        print(f"å»é‡å¾Œè¨˜éŒ„æ•¸: {len(deduplicated_df)}")
        print(f"åˆªé™¤é‡è¤‡è¨˜éŒ„: {len(original_df) - len(deduplicated_df)}")
        
        if len(original_df) > 0:
            reduction_rate = ((len(original_df) - len(deduplicated_df)) / len(original_df) * 100)
            print(f"å»é‡æ¯”ä¾‹: {reduction_rate:.1f}%")
        
        print(f"\nğŸ“‹ ç™¼ç¾ {len(groups)} å€‹é‡è¤‡çµ„")
        
        # é¡¯ç¤ºé‡è¤‡ç¨‹åº¦æœ€é«˜çš„å¹¾çµ„
        group_sizes = [len(group) for group in groups]
        if group_sizes:
            print(f"æœ€å¤§é‡è¤‡çµ„: {max(group_sizes)} å€‹è¨˜éŒ„")
            print(f"å¹³å‡é‡è¤‡çµ„å¤§å°: {np.mean(group_sizes):.1f} å€‹è¨˜éŒ„")

# =============================================================================
# ä¸»è¦æå–å™¨é¡
# =============================================================================

class ESGExtractor:
    """ESGè³‡æ–™æå–å™¨ä¸»é¡"""
    
    def __init__(self, vector_db_path: str = None, enable_llm: bool = True, auto_dedupe: bool = True):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            vector_db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘
            enable_llm: æ˜¯å¦å•Ÿç”¨LLMå¢å¼·
            auto_dedupe: æ˜¯å¦è‡ªå‹•å»é‡
        """
        self.vector_db_path = vector_db_path or VECTOR_DB_PATH
        self.enable_llm = enable_llm
        self.auto_dedupe = auto_dedupe
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.matcher = EnhancedMatcher()
        self.keyword_config = KeywordConfig()
        self.deduplicator = ESGResultDeduplicator()
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        self._load_vector_database()
        
        # åˆå§‹åŒ–LLMï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… ESGæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        if self.auto_dedupe:
            print("âœ… è‡ªå‹•å»é‡åŠŸèƒ½å·²å•Ÿç”¨")
    
    def _load_vector_database(self):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(self.vector_db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {self.vector_db_path}")
        
        print(f"ğŸ“š è¼‰å…¥å‘é‡è³‡æ–™åº«: {self.vector_db_path}")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        self.db = FAISS.load_local(
            self.vector_db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        print(f"âœ… å‘é‡è³‡æ–™åº«è¼‰å…¥å®Œæˆ")
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print(f"ğŸ¤– åˆå§‹åŒ–Geminiæ¨¡å‹: {GEMINI_MODEL}")
            self.llm = ChatGoogleGenerativeAI(
                model=GEMINI_MODEL,
                google_api_key=GOOGLE_API_KEY,
                temperature=0.1,
                max_tokens=1024
            )
            print("âœ… Geminiæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def stage1_filtering(self, documents: List[Document]) -> Tuple[List[Document], List[ExtractionMatch]]:
        """ç¬¬ä¸€éšæ®µç¯©é¸ï¼šæª¢æŸ¥æ–‡æª”æ˜¯å¦åŒ…å«ç›®æ¨™é—œéµå­—"""
        print("ğŸ” åŸ·è¡Œç¬¬ä¸€éšæ®µç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        passed_docs = []
        all_matches = []
        
        for doc in tqdm(documents, desc="ç¬¬ä¸€éšæ®µç¯©é¸"):
            doc_matches = []
            
            for keyword in keywords:
                is_match, confidence, details = self.matcher.match_keyword(doc.page_content, keyword)
                
                if is_match:
                    keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                    match = ExtractionMatch(
                        keyword=keyword_str,
                        keyword_type='continuous' if isinstance(keyword, str) else 'discontinuous',
                        confidence=confidence,
                        matched_text=details
                    )
                    doc_matches.append(match)
            
            if doc_matches:
                passed_docs.append(doc)
                all_matches.extend(doc_matches)
        
        print(f"âœ… ç¬¬ä¸€éšæ®µå®Œæˆ: {len(passed_docs)}/{len(documents)} æ–‡æª”é€šé")
        return passed_docs, all_matches
    
    def stage2_filtering(self, documents: List[Document]) -> List[NumericExtraction]:
        """ç¬¬äºŒéšæ®µç¯©é¸ï¼šæå–åŒ…å«æ•¸å€¼æˆ–ç™¾åˆ†æ¯”çš„å…§å®¹"""
        print("ğŸ”¢ åŸ·è¡Œç¬¬äºŒéšæ®µç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="ç¬¬äºŒéšæ®µç¯©é¸"):
            # åˆ†å‰²æˆæ®µè½
            paragraphs = self._split_into_paragraphs(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 10:
                    continue
                
                # æª¢æŸ¥æ®µè½ä¸­çš„é—œéµå­—
                para_matches = []
                for keyword in keywords:
                    is_match, confidence, details = self.matcher.match_keyword(paragraph, keyword)
                    if is_match:
                        para_matches.append((keyword, confidence, details))
                
                if para_matches:
                    # æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”
                    numbers, percentages = self.matcher.extract_numbers_and_percentages(paragraph)
                    
                    if numbers or percentages:
                        # ç‚ºæ¯å€‹æ‰¾åˆ°çš„æ•¸å€¼å‰µå»ºæå–çµæœ
                        for number in numbers:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=number,
                                    value_type='number',
                                    unit=self._extract_unit(number),
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph)
                                )
                                extractions.append(extraction)
                        
                        # ç‚ºæ¯å€‹æ‰¾åˆ°çš„ç™¾åˆ†æ¯”å‰µå»ºæå–çµæœ
                        for percentage in percentages:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=percentage,
                                    value_type='percentage',
                                    unit='%',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph)
                                )
                                extractions.append(extraction)
        
        print(f"âœ… ç¬¬äºŒéšæ®µå®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹æ•¸å€¼æå–çµæœ")
        return extractions
    
    def llm_enhancement(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """LLMå¢å¼·ï¼šé©—è­‰å’Œè±å¯Œæå–çµæœ"""
        if not self.enable_llm or not extractions:
            return extractions
        
        print("ğŸ¤– åŸ·è¡ŒLLMå¢å¼·...")
        
        enhanced_extractions = []
        
        for extraction in tqdm(extractions, desc="LLMå¢å¼·"):
            try:
                # æ§‹å»ºé©—è­‰æç¤º
                prompt = self._build_verification_prompt(extraction)
                
                # å‘¼å«LLM
                response = self.llm.invoke(prompt)
                llm_result = self._parse_llm_response(response.content)
                
                # æ›´æ–°æå–çµæœ
                if llm_result and llm_result.get("is_relevant", True):
                    # æ›´æ–°ä¿¡å¿ƒåˆ†æ•¸
                    llm_confidence = llm_result.get("confidence", extraction.confidence)
                    extraction.confidence = min(
                        (extraction.confidence + llm_confidence) / 2, 
                        1.0
                    )
                    
                    # æ·»åŠ LLMçš„è§£é‡‹
                    extraction.context_window += f"\n[LLMé©—è­‰]: {llm_result.get('explanation', '')}"
                
                enhanced_extractions.append(extraction)
                
            except Exception as e:
                print(f"âš ï¸ LLMå¢å¼·å¤±æ•—: {e}")
                enhanced_extractions.append(extraction)  # ä¿ç•™åŸå§‹çµæœ
        
        return enhanced_extractions
    
    def export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(RESULTS_PATH, f"esg_extraction_results_{timestamp}.xlsx")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºçµæœåˆ°Excel: {output_path}")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        for extraction in extractions:
            main_data.append({
                'é—œéµå­—': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'æ®µè½å…§å®¹': extraction.paragraph,
                'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                'é ç¢¼': extraction.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
            })
        
        # æº–å‚™çµ±è¨ˆæ•¸æ“š
        stats_data = []
        for keyword, count in summary.keywords_found.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            numbers = [e for e in keyword_extractions if e.value_type == 'number']
            percentages = [e for e in keyword_extractions if e.value_type == 'percentage']
            
            stats_data.append({
                'é—œéµå­—': keyword,
                'ç¸½æå–æ•¸': len(keyword_extractions),
                'æ•¸å€¼é¡å‹': len(numbers),
                'ç™¾åˆ†æ¯”é¡å‹': len(percentages),
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
            })
        
        # æº–å‚™è™•ç†æ‘˜è¦
        process_summary = [{
            'é …ç›®': 'è™•ç†æ‘˜è¦',
            'ç¸½æ–‡æª”æ•¸': summary.total_documents,
            'ç¬¬ä¸€éšæ®µé€šé': summary.stage1_passed,
            'ç¬¬äºŒéšæ®µé€šé': summary.stage2_passed,
            'ç¸½æå–çµæœ': summary.total_extractions,
            'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
            'è‡ªå‹•å»é‡': 'å·²å•Ÿç”¨' if self.auto_dedupe else 'æœªå•Ÿç”¨'
        }]
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ä¸»è¦çµæœ
            pd.DataFrame(main_data).to_excel(writer, sheet_name='æå–çµæœ', index=False)
            
            # çµ±è¨ˆæ‘˜è¦
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='é—œéµå­—çµ±è¨ˆ', index=False)
            
            # è™•ç†æ‘˜è¦
            pd.DataFrame(process_summary).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜")
        return output_path
    
    def run_complete_extraction(self, max_documents: int = 200) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """åŸ·è¡Œå®Œæ•´çš„è³‡æ–™æå–æµç¨‹ï¼ˆå«è‡ªå‹•å»é‡ï¼‰"""
        start_time = datetime.now()
        print("ğŸš€ é–‹å§‹å®Œæ•´çš„ESGè³‡æ–™æå–æµç¨‹")
        print("=" * 60)
        
        # 1. ç²å–ç›¸é—œæ–‡æª”
        print("ğŸ“„ æª¢ç´¢ç›¸é—œæ–‡æª”...")
        documents = self._retrieve_relevant_documents(max_documents)
        
        # 2. ç¬¬ä¸€éšæ®µç¯©é¸
        stage1_docs, stage1_matches = self.stage1_filtering(documents)
        
        # 3. ç¬¬äºŒéšæ®µç¯©é¸
        stage2_extractions = self.stage2_filtering(stage1_docs)
        
        # 4. LLMå¢å¼·ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        enhanced_extractions = self.llm_enhancement(stage2_extractions)
        
        # 5. è‡ªå‹•å»é‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.auto_dedupe:
            print("\nğŸ”„ åŸ·è¡Œè‡ªå‹•å»é‡...")
            final_extractions = self.deduplicator.deduplicate_extractions(enhanced_extractions)
        else:
            final_extractions = enhanced_extractions
        
        # 6. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in final_extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            total_documents=len(documents),
            stage1_passed=len(stage1_docs),
            stage2_passed=len([e for e in final_extractions]),
            total_extractions=len(final_extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 7. åŒ¯å‡ºçµæœ
        excel_path = self.export_to_excel(final_extractions, summary)
        
        # 8. é¡¯ç¤ºæœ€çµ‚æ‘˜è¦
        self._print_final_summary(summary, final_extractions)
        
        return final_extractions, summary, excel_path
    
    def manual_deduplicate_results(self, excel_path: str) -> str:
        """æ‰‹å‹•å»é‡ç¾æœ‰çš„Excelçµæœæ–‡ä»¶"""
        return self.deduplicator.deduplicate_excel_file(excel_path)
    
    # =============================================================================
    # è¼”åŠ©æ–¹æ³•
    # =============================================================================
    
    def _retrieve_relevant_documents(self, max_docs: int) -> List[Document]:
        """æª¢ç´¢ç›¸é—œæ–‡æª”"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # å°æ¯å€‹é—œéµå­—é€²è¡Œæª¢ç´¢
        for keyword in keywords:
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = self.db.similarity_search(search_term, k=30)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹ç›¸é—œæ–‡æª”")
        return result_docs
    
    def _split_into_paragraphs(self, text: str) -> List[str]:
        """å°‡æ–‡æœ¬åˆ†å‰²æˆæ®µè½"""
        paragraphs = re.split(r'\n{2,}|\r{2,}|ã€‚{2,}|\.{2,}', text)
        
        cleaned_paragraphs = []
        for para in paragraphs:
            para = para.strip()
            if len(para) >= 10:
                cleaned_paragraphs.append(para)
        
        return cleaned_paragraphs
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 100) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:200]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:200]
    
    def _build_verification_prompt(self, extraction: NumericExtraction) -> str:
        """æ§‹å»ºLLMé©—è­‰æç¤º"""
        return f"""
è«‹é©—è­‰ä»¥ä¸‹æ•¸æ“šæå–çµæœçš„æº–ç¢ºæ€§ï¼š

é—œéµå­—: {extraction.keyword}
æå–å€¼: {extraction.value}
æ•¸æ“šé¡å‹: {extraction.value_type}

æ®µè½å…§å®¹:
{extraction.paragraph}

è«‹åˆ¤æ–·ï¼š
1. æå–çš„æ•¸å€¼æ˜¯å¦èˆ‡é—œéµå­—ç›¸é—œï¼Ÿ
2. æ•¸å€¼æå–æ˜¯å¦æº–ç¢ºï¼Ÿ
3. æ•¸æ“šé¡å‹åˆ†é¡æ˜¯å¦æ­£ç¢ºï¼Ÿ

è«‹ä»¥JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_relevant": true/false,
    "is_accurate": true/false,
    "confidence": 0-1ä¹‹é–“çš„åˆ†æ•¸,
    "explanation": "ç°¡çŸ­è§£é‡‹"
}}
"""
    
    def _parse_llm_response(self, response_text: str) -> Optional[Dict]:
        """è§£æLLMå›æ‡‰"""
        try:
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        return None
    
    def _print_final_summary(self, summary: ProcessingSummary, extractions: List[NumericExtraction]):
        """æ‰“å°æœ€çµ‚æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ æå–å®Œæˆæ‘˜è¦")
        print("=" * 60)
        print(f"ğŸ“š è™•ç†æ–‡æª”æ•¸: {summary.total_documents}")
        print(f"ğŸ” ç¬¬ä¸€éšæ®µé€šé: {summary.stage1_passed}")
        print(f"ğŸ”¢ ç¬¬äºŒéšæ®µé€šé: {summary.stage2_passed}")
        print(f"ğŸ“Š ç¸½æå–çµæœ: {summary.total_extractions}")
        print(f"â±ï¸ è™•ç†æ™‚é–“: {summary.processing_time:.2f} ç§’")
        print(f"ğŸ§¹ è‡ªå‹•å»é‡: {'å·²å•Ÿç”¨' if self.auto_dedupe else 'æœªå•Ÿç”¨'}")
        
        print(f"\nğŸ“ˆ é—œéµå­—åˆ†å¸ƒ:")
        for keyword, count in summary.keywords_found.items():
            print(f"   {keyword}: {count} å€‹çµæœ")
        
        if extractions:
            numbers = [e for e in extractions if e.value_type == 'number']
            percentages = [e for e in extractions if e.value_type == 'percentage']
            
            print(f"\nğŸ”¢ æ•¸æ“šé¡å‹åˆ†å¸ƒ:")
            print(f"   æ•¸å€¼: {len(numbers)} å€‹")
            print(f"   ç™¾åˆ†æ¯”: {len(percentages)} å€‹")
            
            avg_confidence = np.mean([e.confidence for e in extractions])
            print(f"ğŸ“Š å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {avg_confidence:.3f}")

def main():
    """ä¸»å‡½æ•¸ - ç¨ç«‹é‹è¡Œæ¸¬è©¦"""
    try:
        print("ğŸš€ ESGè³‡æ–™æå–å™¨ - ç¨ç«‹æ¸¬è©¦æ¨¡å¼")
        print("=" * 50)
        
        # åˆå§‹åŒ–æå–å™¨ï¼ˆå•Ÿç”¨è‡ªå‹•å»é‡ï¼‰
        extractor = ESGExtractor(enable_llm=True, auto_dedupe=True)
        
        # åŸ·è¡Œå®Œæ•´æå–
        extractions, summary, excel_path = extractor.run_complete_extraction()
        
        if extractions:
            print(f"\nğŸ‰ æå–å®Œæˆï¼")
            print(f"ğŸ“ çµæœå·²ä¿å­˜è‡³: {excel_path}")
            
            # é¡¯ç¤ºå‰å¹¾å€‹çµæœä½œç‚ºæ¨£ä¾‹
            print(f"\nğŸ“‹ æ¨£ä¾‹çµæœ (å‰3å€‹):")
            for i, extraction in enumerate(extractions[:3], 1):
                print(f"\n{i}. é—œéµå­—: {extraction.keyword}")
                print(f"   æ•¸å€¼: {extraction.value}")
                print(f"   é¡å‹: {extraction.value_type}")
                print(f"   é ç¢¼: {extraction.page_number}")
                print(f"   ä¿¡å¿ƒ: {extraction.confidence:.2f}")
                print(f"   æ®µè½: {extraction.paragraph[:100]}...")
            
            # æ¸¬è©¦æ‰‹å‹•å»é‡åŠŸèƒ½
            print(f"\nğŸ§¹ æ¸¬è©¦æ‰‹å‹•å»é‡åŠŸèƒ½...")
            dedupe_path = extractor.manual_deduplicate_results(excel_path)
            if dedupe_path:
                print(f"âœ… æ‰‹å‹•å»é‡å®Œæˆ: {Path(dedupe_path).name}")
        
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æå–çµæœ")
    
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()