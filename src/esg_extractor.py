#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGå ±å‘Šæ›¸æå–å™¨æ ¸å¿ƒæ¨¡çµ„ v2.0 - å¢å¼·ç‰ˆ
æ”¯æŒæ–°é—œéµå­—é…ç½®å’ŒWordæ–‡æª”è¼¸å‡º
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np

# Wordæ–‡æª”æ”¯æŒ
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from docx.oxml.ns import qn

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document as LangchainDocument
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# å¢å¼·ç‰ˆé—œéµå­—é…ç½® - æ”¯æŒæ–°é—œéµå­—
# =============================================================================

class EnhancedKeywordConfig:
    """å¢å¼·ç‰ˆESGå ±å‘Šæ›¸é—œéµå­—é…ç½® - æ”¯æŒæ–°çš„é—œéµå­—çµ„"""
    
    def __init__(self):
        # åŸæœ‰çš„å†ç”Ÿå¡‘è† é—œéµå­—ï¼ˆä¿æŒé«˜æº–ç¢ºåº¦ï¼‰
        self.RECYCLED_PLASTIC_KEYWORDS = {
            "high_relevance_continuous": [
                "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”ŸPET", "å†ç”ŸPP",
                "å›æ”¶å¡‘è† ", "å›æ”¶å¡‘æ–™", "å›æ”¶PP", "å›æ”¶PET", 
                "rPET", "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™",
                "å¯¶ç‰¹ç“¶å›æ”¶", "å»¢å¡‘è† å›æ”¶", "å¡‘è† å¾ªç’°",
                "å›æ”¶é€ ç²’", "å†ç”Ÿèšé…¯", "å›æ”¶èšé…¯",
                "å¾ªç’°ç¶“æ¿Ÿ", "ç‰©æ–™å›æ”¶", "ææ–™å›æ”¶",
                # æ–°å¢é—œéµå­—
                "å†ç”Ÿå¡‘è† ä½¿ç”¨æ¯”ç‡", "å†ç”Ÿå¡‘è† çš„ä½¿ç”¨é‡", "å†ç”Ÿå¡‘è† æˆæœ¬"
            ],
            
            "medium_relevance_continuous": [
                "ç’°ä¿å¡‘è† ", "ç¶ è‰²ææ–™", "æ°¸çºŒææ–™",
                "å»¢æ–™å›æ”¶", "è³‡æºå›æ”¶", "å¾ªç’°åˆ©ç”¨",
                # æ–°å¢é—œéµå­—
                "ææ–™å¾ªç’°ç‡", "ææ–™å¯å›æ”¶ç‡", "å†ç”Ÿææ–™æ›¿ä»£ç‡",
                "å†ç”Ÿææ–™ä½¿ç”¨é‡", "ææ–™ç¸½ä½¿ç”¨é‡", "å¡‘è† ä½¿ç”¨é‡", "ææ–™ä½¿ç”¨é‡"
            ],
            
            "high_relevance_discontinuous": [
                ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å¯¶ç‰¹ç“¶", "å†é€ "), ("å¯¶ç‰¹ç“¶", "å¾ªç’°"),
                ("å„„æ”¯", "å¯¶ç‰¹ç“¶"), ("è¬æ”¯", "å¯¶ç‰¹ç“¶"),
                ("PET", "å›æ”¶"), ("PET", "å†ç”Ÿ"), ("PP", "å›æ”¶"), ("PP", "å†ç”Ÿ"),
                ("å¡‘è† ", "å›æ”¶"), ("å¡‘æ–™", "å›æ”¶"), ("å¡‘è† ", "å¾ªç’°"),
                ("å›æ”¶", "é€ ç²’"), ("å›æ”¶", "ç”¢èƒ½"), ("å›æ”¶", "ææ–™"),
                ("å†ç”Ÿ", "ææ–™"), ("å»¢æ–™", "å›æ”¶"), ("MLCC", "å›æ”¶"),
                ("åŸç”Ÿ", "ææ–™"), ("ç¢³æ’æ”¾", "æ¸›å°‘"), ("æ¸›ç¢³", "æ•ˆç›Š"),
                ("æ­·å¹´", "å›æ”¶"), ("å›æ”¶", "æ•¸é‡"), ("å›æ”¶", "æ•ˆç›Š"),
                ("å¾ªç’°", "ç¶“æ¿Ÿ"), ("æ°¸çºŒ", "ç™¼å±•"), ("ç’°ä¿", "ç”¢å“"),
                # æ–°å¢ä¸é€£çºŒé—œéµå­—çµ„åˆ
                ("ææ–™", "å¾ªç’°"), ("ææ–™", "å›æ”¶"), ("å†ç”Ÿ", "ä½¿ç”¨é‡"),
                ("ç¢³æ’", "æ¸›é‡"), ("æè³ª", "åˆ†é›¢"), ("å–®ä¸€", "ææ–™")
            ]
        }
        
        # æ–°å¢ï¼šæ“´å±•çš„ç’°ä¿å’Œæ°¸çºŒé—œéµå­—
        self.SUSTAINABILITY_KEYWORDS = {
            "high_relevance_continuous": [
                "å†ç”Ÿèƒ½æºä½¿ç”¨ç‡", "ç¶ é›»æ†‘è­‰", "å¤ªé™½èƒ½é›»åŠ›", "è³¼é›»å”è­°",
                "å†ç”Ÿèƒ½æº", "å†ç”Ÿææ–™ç¢³æ’æ¸›é‡", "ç¢³æ’æ¸›é‡æ¯”ç‡", 
                "å–®ä½ç¶“æ¿Ÿæ•ˆç›Š", "åˆ†é¸è¾¨è¦–", "æˆæœ¬å¢åŠ ", "ç¢³æ’æ”¾", "æè³ªåˆ†é›¢"
            ],
            
            "medium_relevance_continuous": [
                "ç¶ è‰²ä¾›æ‡‰éˆ", "æ°¸çºŒæ¡è³¼", "ç’°ä¿èªè­‰", "ä½ç¢³è£½ç¨‹",
                "ç¯€èƒ½æ¸›ç¢³", "æº«å®¤æ°£é«”", "ç¢³ä¸­å’Œ", "æ·¨é›¶æ’æ”¾"
            ],
            
            "high_relevance_discontinuous": [
                ("å†ç”Ÿ", "èƒ½æº"), ("å¤ªé™½èƒ½", "ç™¼é›»"), ("ç¶ é›»", "ä½¿ç”¨"),
                ("ç¢³æ’", "æ¸›é‡"), ("æº«å®¤", "æ°£é«”"), ("ç’°ä¿", "æ•ˆç›Š"),
                ("æ°¸çºŒ", "ç™¼å±•"), ("å¾ªç’°", "åˆ©ç”¨"), ("ç¶ è‰²", "è£½ç¨‹")
            ]
        }
        
        # åˆä½µæ‰€æœ‰é—œéµå­—
        self.ALL_KEYWORDS = {
            "plastic_recycling": self.RECYCLED_PLASTIC_KEYWORDS,
            "sustainability": self.SUSTAINABILITY_KEYWORDS
        }
    
    # æ’é™¤è¦å‰‡ - æ›´åŠ ç²¾ç¢º
    EXCLUSION_RULES = {
        "exclude_topics": [
            # è·æ¥­å®‰å…¨
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "è·ç½", "å·¥å‚·", "æ„å¤–äº‹æ•…",
            
            # é«”è‚²è³½äº‹
            "é¦¬æ‹‰æ¾", "è³½äº‹", "é¸æ‰‹", "æ¯”è³½", "è³½è¡£", "é‹å‹•", "ç«¶è³½", "é«”è‚²",
            
            # æ°´è³‡æºï¼ˆéææ–™ç›¸é—œï¼‰
            "é›¨æ°´å›æ”¶", "å»¢æ°´è™•ç†", "æ°´è³ªç›£æ¸¬", "ç”¨æ°´é‡", "ç¯€æ°´", "æ°´è³‡æº",
            
            # ä¸€èˆ¬æ”¹å–„æ¡ˆï¼ˆéææ–™ç›¸é—œï¼‰
            "æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ", "æ¡ˆä¾‹é¸æ‹”", "ç¸¾æ•ˆæ”¹å–„", "ç®¡ç†æ”¹å–„",
            
            # èƒ½æºè¨­å‚™ï¼ˆéææ–™ç›¸é—œï¼‰
            "é‹çˆæ”¹å–„", "å¤©ç„¶æ°£ç‡ƒç‡’", "ç‡ƒæ²¹æ”¹ç‡ƒ", "è¨­å‚™æ›´æ–°",
            
            # å»ºç¯‰ç”¢å“ï¼ˆéé‡é»ï¼‰
            "éš”ç†±æ¼†", "ç¯€èƒ½çª—", "éš”ç†±ç´™", "é…·æ¨‚æ¼†", "æ°£å¯†çª—", "å»ºæç”¢å“"
        ],
        
        "exclude_contexts": [
            # å…·é«”æ’é™¤ä¸Šä¸‹æ–‡
            "å‚ç›´é¦¬æ‹‰æ¾", "å²ä¸Šæœ€ç’°ä¿è³½è¡£", "å„ç•Œå¥½æ‰‹", "é‹å‹•è³½äº‹",
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å®‰çµ±è¨ˆ", "å®‰å…¨æŒ‡æ¨™",
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "å„ªè‰¯æ¡ˆä¾‹", "æ”¹å–„å°ˆæ¡ˆ",
            "é›¨æ°´å›æ”¶é‡æ¸›å°‘", "é™é›¨é‡æ¸›å°‘", "å»¢æ°´å›æ”¶",
            "ç‡ƒæ²¹æ”¹ç‡ƒæ±½é‹çˆ", "å¤©ç„¶æ°£ç‡ƒç‡’æ©Ÿ", "é‹çˆæ”¹é€ ",
            "é…·æ¨‚æ¼†", "éš”ç†±æ¼†", "ç¯€èƒ½æ°£å¯†çª—", "å†°é…·éš”ç†±ç´™",
            "å“¡å·¥è¨“ç·´", "æ•™è‚²è¨“ç·´", "æœƒè­°å ´æ¬¡", "æ´»å‹•åƒèˆ‡"
        ],
        
        "exclude_number_patterns": [
            # æ’é™¤ç‰¹å®šæ•¸å€¼æ¨¡å¼
            r'è·æ¥­ç½å®³.*?\d+(?:\.\d+)?(?:%|ï¼…|ä»¶|æ¬¡)',
            r'å·¥å®‰.*?\d+(?:\.\d+)?(?:ä»¶|æ¬¡|å°æ™‚)',
            r'è¨“ç·´.*?\d+(?:\.\d+)?(?:å°æ™‚|äººæ¬¡|å ´)',
            r'æœƒè­°.*?\d+(?:\.\d+)?(?:æ¬¡|å ´|å°æ™‚)',
            r'é™é›¨é‡.*?\d+(?:\.\d+)?(?:%|ï¼…|mm)',
            r'ç”¨æ°´é‡.*?\d+(?:\.\d+)?(?:å™¸|ç«‹æ–¹å…¬å°º)'
        ]
    }
    
    # ææ–™å’Œå¡‘è† ç›¸é—œæŒ‡æ¨™è© - æ›´ç²¾ç¢º
    MATERIAL_INDICATORS = {
        "plastic_materials": [
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "PE", "PS", "PVC",
            "æ¨¹è„‚", "èšåˆç‰©", "å¡‘è† ç²’", "èšé…¯ç²’", "å¡‘è† ææ–™",
            "å¯¶ç‰¹ç“¶", "ç“¶ç‰‡", "å®¹å™¨", "åŒ…è£æ", "è†œæ", "çº–ç¶­ææ–™"
        ],
        
        "recycling_process": [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨", "è³‡æºåŒ–",
            "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "åˆ†è§£", "ç´”åŒ–",
            "å¾ªç’°ç¶“æ¿Ÿ", "å»¢æ–™è™•ç†", "è³‡æºå¾ªç’°"
        ],
        
        "production_metrics": [
            "ç”¢èƒ½", "ç”¢é‡", "ç”Ÿç”¢", "è£½é€ ", "ä½¿ç”¨é‡", "æ¶ˆè€—é‡",
            "æ›¿ä»£ç‡", "ä½¿ç”¨ç‡", "å›æ”¶ç‡", "å¾ªç’°ç‡", "æ¯”ä¾‹", "æ¯”ç‡"
        ]
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—"""
        instance = cls()
        all_keywords = []
        
        # å¡‘è† å›æ”¶é—œéµå­—
        all_keywords.extend(instance.RECYCLED_PLASTIC_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(instance.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(instance.RECYCLED_PLASTIC_KEYWORDS["high_relevance_discontinuous"])
        
        # æ°¸çºŒç™¼å±•é—œéµå­—
        all_keywords.extend(instance.SUSTAINABILITY_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(instance.SUSTAINABILITY_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(instance.SUSTAINABILITY_KEYWORDS["high_relevance_discontinuous"])
        
        return all_keywords

# =============================================================================
# å¢å¼·ç‰ˆåŒ¹é…å¼•æ“ - æé«˜æº–ç¢ºåº¦
# =============================================================================

class EnhancedESGMatcher:
    """å¢å¼·ç‰ˆESGæ•¸æ“šåŒ¹é…å¼•æ“ - æé«˜æå–æº–ç¢ºåº¦"""
    
    def __init__(self):
        self.config = EnhancedKeywordConfig()
        self.max_distance = 200  # æ¸›å°‘æœ€å¤§è·é›¢ä»¥æé«˜æº–ç¢ºåº¦
        
        # å¢å¼·çš„æ•¸å€¼åŒ¹é…æ¨¡å¼
        self.number_patterns = [
            # åŸºæœ¬æ•¸é‡å–®ä½
            r'\d+(?:\.\d+)?\s*å„„æ”¯',
            r'\d+(?:\.\d+)?\s*è¬æ”¯',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|åƒ)?å™¸',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kg|KG|å…¬æ–¤)',
            
            # æ™‚é–“ç›¸é—œç”¢èƒ½
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æœˆ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/å¹´',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æ—¥',
            
            # é€šç”¨è¨ˆæ•¸å–®ä½
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ä»¶|å€‹|æ‰¹|å°|å¥—|é …)',
            
            # é‡‘é¡ç›¸é—œ
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:å„„|è¬|åƒ)?(?:å…ƒ|ç¾é‡‘|USD)',
            
            # æ–°å¢ï¼šæ¯”ç‡ç›¸é—œæ•¸å€¼
            r'\d+(?:\.\d+)?\s*å€',
            r'\d+(?:\.\d+)?\s*(?:ppm|PPM)',
        ]
        
        # å¢å¼·çš„ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%',
            r'\d+(?:\.\d+)?\s*ï¼…',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
            r'\d+(?:\.\d+)?\s*percent',
        ]
    
    def extract_keyword_value_pairs(self, text: str, keyword: Union[str, tuple]) -> List[Tuple[str, str, float, int]]:
        """æå–é—œéµå­—èˆ‡æ•¸å€¼çš„é…å° - å¢å¼·ç‰ˆ"""
        text_lower = text.lower()
        
        # 1. æª¢æŸ¥é—œéµå­—æ˜¯å¦å­˜åœ¨
        keyword_match, keyword_confidence, _ = self._match_keyword(text, keyword)
        if not keyword_match:
            return []
        
        # 2. æ‰¾åˆ°é—œéµå­—ä½ç½®
        keyword_positions = self._get_keyword_positions(text_lower, keyword)
        if not keyword_positions:
            return []
        
        # 3. åœ¨é—œéµå­—é™„è¿‘å°‹æ‰¾æ•¸å€¼
        valid_pairs = []
        
        for kw_start, kw_end in keyword_positions:
            # æ¸›å°æœç´¢ç¯„åœä»¥æé«˜æº–ç¢ºåº¦
            search_start = max(0, kw_start - 80)
            search_end = min(len(text), kw_end + 80)
            search_window = text[search_start:search_end]
            
            # æå–æ•¸å€¼
            numbers = self._extract_numbers_in_window(search_window)
            percentages = self._extract_percentages_in_window(search_window)
            
            # é©—è­‰æ•¸å€¼é—œè¯æ€§ï¼ˆæ›´åš´æ ¼çš„é©—è­‰ï¼‰
            for number in numbers:
                number_pos = search_window.find(number)
                if number_pos != -1:
                    actual_number_pos = search_start + number_pos
                    distance = min(abs(actual_number_pos - kw_start), abs(actual_number_pos - kw_end))
                    
                    association_score = self._calculate_association(
                        text, keyword, number, kw_start, kw_end, actual_number_pos
                    )
                    
                    # æé«˜é—œè¯åº¦é–¾å€¼
                    if association_score > 0.6 and distance <= 60:
                        valid_pairs.append((number, 'number', association_score, distance))
            
            for percentage in percentages:
                percentage_pos = search_window.find(percentage)
                if percentage_pos != -1:
                    actual_percentage_pos = search_start + percentage_pos
                    distance = min(abs(actual_percentage_pos - kw_start), abs(actual_percentage_pos - kw_end))
                    
                    association_score = self._calculate_association(
                        text, keyword, percentage, kw_start, kw_end, actual_percentage_pos
                    )
                    
                    # æé«˜é—œè¯åº¦é–¾å€¼
                    if association_score > 0.6 and distance <= 60:
                        valid_pairs.append((percentage, 'percentage', association_score, distance))
        
        # å»é‡ä¸¦æ’åºï¼Œåªä¿ç•™æœ€ä½³çµæœ
        unique_pairs = []
        seen_values = set()
        
        for value, value_type, score, distance in sorted(valid_pairs, key=lambda x: (-x[2], x[3])):
            if value not in seen_values:
                seen_values.add(value)
                unique_pairs.append((value, value_type, score, distance))
        
        return unique_pairs[:2]  # åªä¿ç•™å‰2å€‹æœ€ä½³åŒ¹é…
    
    def comprehensive_relevance_check(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """ç¶œåˆç›¸é—œæ€§æª¢æŸ¥ - å¢å¼·ç‰ˆ"""
        text_lower = text.lower()
        
        # 1. å¼·æ’é™¤æª¢æŸ¥ - æ›´åš´æ ¼
        exclusion_score = self._check_strong_exclusions(text_lower)
        if exclusion_score > 0.3:  # é™ä½æ’é™¤é–¾å€¼ï¼Œæ›´å®¹æ˜“æ’é™¤
            return False, 0.0, f"æ’é™¤å…§å®¹: {exclusion_score:.2f}"
        
        # 2. é—œéµå­—åŒ¹é…
        keyword_match, keyword_confidence, keyword_details = self._match_keyword(text, keyword)
        if not keyword_match:
            return False, 0.0, "é—œéµå­—ä¸åŒ¹é…"
        
        # 3. ææ–™ç›¸é—œæ€§æª¢æŸ¥ - æ›´åš´æ ¼
        material_relevance = self._check_material_relevance(text_lower)
        if material_relevance < 0.4:  # æé«˜ææ–™ç›¸é—œæ€§é–¾å€¼
            return False, 0.0, f"éææ–™ç›¸é—œ: {material_relevance:.2f}"
        
        # 4. ä¸Šä¸‹æ–‡è³ªé‡æª¢æŸ¥
        context_quality = self._check_context_quality(text_lower)
        if context_quality < 0.3:
            return False, 0.0, f"ä¸Šä¸‹æ–‡è³ªé‡ä¸è¶³: {context_quality:.2f}"
        
        # 5. è¨ˆç®—ç¶œåˆåˆ†æ•¸
        final_score = (
            keyword_confidence * 0.25 + 
            material_relevance * 0.30 + 
            context_quality * 0.25 +
            (1 - exclusion_score) * 0.20  # æ’é™¤æ‡²ç½°
        )
        
        # æé«˜ç›¸é—œæ€§é–¾å€¼
        is_relevant = final_score > 0.65
        
        details = f"é—œéµå­—:{keyword_confidence:.2f}, ææ–™:{material_relevance:.2f}, ä¸Šä¸‹æ–‡:{context_quality:.2f}, æ’é™¤:{exclusion_score:.2f}"
        
        return is_relevant, final_score, details
    
    # æ–°å¢è¼”åŠ©æ–¹æ³•
    def _check_strong_exclusions(self, text: str) -> float:
        """æª¢æŸ¥å¼·æ’é™¤æ¨¡å¼ - å¢å¼·ç‰ˆ"""
        exclusion_score = 0.0
        
        # æª¢æŸ¥æ’é™¤ä¸»é¡Œï¼ˆæ¬Šé‡æ›´é«˜ï¼‰
        for topic in self.config.EXCLUSION_RULES["exclude_topics"]:
            if topic in text:
                exclusion_score += 0.25
        
        # æª¢æŸ¥æ’é™¤ä¸Šä¸‹æ–‡ï¼ˆæ¬Šé‡æœ€é«˜ï¼‰
        for context in self.config.EXCLUSION_RULES["exclude_contexts"]:
            if context in text:
                exclusion_score += 0.35
        
        # æª¢æŸ¥æ’é™¤æ•¸å€¼æ¨¡å¼
        for pattern in self.config.EXCLUSION_RULES["exclude_number_patterns"]:
            if re.search(pattern, text, re.IGNORECASE):
                exclusion_score += 0.30
        
        return min(exclusion_score, 1.0)
    
    def _check_material_relevance(self, text: str) -> float:
        """æª¢æŸ¥ææ–™ç›¸é—œæ€§ - æ›´ç²¾ç¢º"""
        material_score = 0.0
        recycling_score = 0.0
        production_score = 0.0
        
        # ææ–™ç›¸é—œè©è¨ˆåˆ†
        material_count = 0
        for indicator in self.config.MATERIAL_INDICATORS["plastic_materials"]:
            if indicator in text:
                material_count += 1
        material_score = min(material_count / 3.0, 1.0)
        
        # å›æ”¶è™•ç†ç›¸é—œè©è¨ˆåˆ†
        recycling_count = 0
        for indicator in self.config.MATERIAL_INDICATORS["recycling_process"]:
            if indicator in text:
                recycling_count += 1
        recycling_score = min(recycling_count / 2.0, 1.0)
        
        # ç”Ÿç”¢æŒ‡æ¨™ç›¸é—œè©è¨ˆåˆ†
        production_count = 0
        for indicator in self.config.MATERIAL_INDICATORS["production_metrics"]:
            if indicator in text:
                production_count += 1
        production_score = min(production_count / 2.0, 1.0)
        
        # å¿…é ˆåŒæ™‚å…·å‚™ææ–™å’Œè™•ç†/ç”Ÿç”¢ç›¸é—œè©
        if material_score > 0 and (recycling_score > 0 or production_score > 0):
            return (material_score + recycling_score + production_score) / 3.0
        else:
            return 0.0
    
    def _check_context_quality(self, text: str) -> float:
        """æª¢æŸ¥ä¸Šä¸‹æ–‡è³ªé‡"""
        quality_indicators = [
            "ä½¿ç”¨", "ç”Ÿç”¢", "è£½é€ ", "æ‡‰ç”¨", "è™•ç†", "å›æ”¶",
            "æ•¸é‡", "æ¯”ä¾‹", "æ¯”ç‡", "ç”¢èƒ½", "æ•ˆç›Š", "æˆæœ¬",
            "æ¸›å°‘", "å¢åŠ ", "æé«˜", "é™ä½", "æ”¹å–„", "å„ªåŒ–"
        ]
        
        found_indicators = sum(1 for indicator in quality_indicators if indicator in text)
        quality_score = min(found_indicators / 4.0, 1.0)
        
        # æª¢æŸ¥æ•¸å€¼ç›¸é—œæ€§
        has_meaningful_numbers = bool(re.search(r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:å™¸|å„„|è¬|%|ï¼…)', text))
        if has_meaningful_numbers:
            quality_score += 0.3
        
        return min(quality_score, 1.0)
    
    # ä¿ç•™åŸæœ‰çš„è¼”åŠ©æ–¹æ³•ï¼ˆç°¡åŒ–é¡¯ç¤ºï¼‰
    def _match_keyword(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """é—œéµå­—åŒ¹é… - ä¿æŒåŸé‚è¼¯"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            if keyword.lower() in text_lower:
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {keyword}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            positions = []
            
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            distance = max(positions) - min(positions)
            
            if distance <= 50:
                return True, 0.9, f"è¿‘è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 100:
                return True, 0.8, f"ä¸­è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= self.max_distance:
                return True, 0.6, f"é è·é›¢åŒ¹é…({distance}å­—)"
            else:
                return True, 0.4, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—)"
        
        return False, 0.0, ""
    
    def _get_keyword_positions(self, text: str, keyword: Union[str, tuple]) -> List[Tuple[int, int]]:
        """ç²å–é—œéµå­—ä½ç½®"""
        positions = []
        
        if isinstance(keyword, str):
            keyword_lower = keyword.lower()
            start = 0
            while True:
                pos = text.find(keyword_lower, start)
                if pos == -1:
                    break
                positions.append((pos, pos + len(keyword_lower)))
                start = pos + 1
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            component_positions = {}
            
            for comp in components:
                comp_positions = []
                start = 0
                while True:
                    pos = text.find(comp, start)
                    if pos == -1:
                        break
                    comp_positions.append((pos, pos + len(comp)))
                    start = pos + 1
                component_positions[comp] = comp_positions
            
            for comp1_pos in component_positions.get(components[0], []):
                for comp2_pos in component_positions.get(components[1], []):
                    distance = abs(comp1_pos[0] - comp2_pos[0])
                    if distance <= self.max_distance:
                        start_pos = min(comp1_pos[0], comp2_pos[0])
                        end_pos = max(comp1_pos[1], comp2_pos[1])
                        positions.append((start_pos, end_pos))
        
        return positions
    
    def _extract_numbers_in_window(self, window_text: str) -> List[str]:
        """æå–æ•¸å€¼"""
        numbers = []
        for pattern in self.number_patterns:
            matches = re.findall(pattern, window_text, re.IGNORECASE)
            numbers.extend(matches)
        return list(set(numbers))
    
    def _extract_percentages_in_window(self, window_text: str) -> List[str]:
        """æå–ç™¾åˆ†æ¯”"""
        percentages = []
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, window_text, re.IGNORECASE)
            percentages.extend(matches)
        return list(set(percentages))
    
    def _calculate_association(self, text: str, keyword: Union[str, tuple], 
                             value: str, kw_start: int, kw_end: int, value_pos: int) -> float:
        """è¨ˆç®—é—œéµå­—èˆ‡æ•¸å€¼çš„é—œè¯åº¦ - å¢å¼·ç‰ˆ"""
        
        # è·é›¢å› å­ï¼ˆæ›´åš´æ ¼ï¼‰
        distance = min(abs(value_pos - kw_start), abs(value_pos - kw_end))
        if distance <= 15:
            distance_score = 1.0
        elif distance <= 30:
            distance_score = 0.8
        elif distance <= 60:
            distance_score = 0.6
        else:
            distance_score = 0.3
        
        # ä¸Šä¸‹æ–‡ç›¸é—œæ€§
        context_start = min(kw_start, value_pos) - 25
        context_end = max(kw_end, value_pos + len(value)) + 25
        context_start = max(0, context_start)
        context_end = min(len(text), context_end)
        context = text[context_start:context_end].lower()
        
        context_score = self._calculate_context_score(context)
        
        # æ•¸å€¼åˆç†æ€§
        value_score = self._calculate_value_score(value, context)
        
        # èªç¾©é€£æ¥è©æª¢æŸ¥
        connection_score = self._check_semantic_connection(context, keyword)
        
        final_score = (
            distance_score * 0.30 +
            context_score * 0.30 + 
            value_score * 0.25 +
            connection_score * 0.15
        )
        
        return final_score
    
    def _check_semantic_connection(self, context: str, keyword: Union[str, tuple]) -> float:
        """æª¢æŸ¥èªç¾©é€£æ¥è©"""
        connection_words = [
            "é”åˆ°", "ç‚º", "ç´„", "å…±", "ç¸½è¨ˆ", "ç´¯è¨ˆ", "å¯¦ç¾", "å®Œæˆ",
            "ä½¿ç”¨", "ç”Ÿç”¢", "è£½é€ ", "è™•ç†", "å›æ”¶", "ç¯€çœ", "æ¸›å°‘", "å¢åŠ "
        ]
        
        connection_score = 0.0
        for word in connection_words:
            if word in context:
                connection_score += 0.2
        
        return min(connection_score, 1.0)
    
    def _calculate_context_score(self, context: str) -> float:
        """è¨ˆç®—ä¸Šä¸‹æ–‡åˆ†æ•¸ - å¢å¼·ç‰ˆ"""
        high_relevance_words = [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "è£½é€ ", "ç”Ÿç”¢", "ç”¢èƒ½", "ä½¿ç”¨",
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "ææ–™", "å¯¶ç‰¹ç“¶", "æ¸›ç¢³", "æ•ˆç›Š",
            "æ¯”ä¾‹", "æ¯”ç‡", "æ•¸é‡", "ç”¢é‡", "ä½¿ç”¨é‡"
        ]
        
        negative_words = [
            "ç½å®³", "äº‹æ•…", "é¦¬æ‹‰æ¾", "è³½äº‹", "æ”¹å–„æ¡ˆ", "æ¡ˆä¾‹",
            "é›¨æ°´", "ç¯€èƒ½", "éš”ç†±", "é‹çˆ", "ç‡ƒæ²¹", "è¨“ç·´", "æœƒè­°"
        ]
        
        score = 0.0
        
        for word in high_relevance_words:
            if word in context:
                score += 0.12
        
        for word in negative_words:
            if word in context:
                score -= 0.25
        
        return max(0.0, min(1.0, score))
    
    def _calculate_value_score(self, value: str, context: str) -> float:
        """è¨ˆç®—æ•¸å€¼åˆç†æ€§åˆ†æ•¸ - å¢å¼·ç‰ˆ"""
        number_match = re.search(r'\d+(?:,\d{3})*(?:\.\d+)?', value)
        if not number_match:
            return 0.0
        
        try:
            number_str = number_match.group().replace(',', '')
            number = float(number_str)
        except ValueError:
            return 0.0
        
        # æ ¹æ“šå–®ä½è©•ä¼°åˆç†æ€§
        if "å„„æ”¯" in value:
            if 0.5 <= number <= 100:
                return 1.0
            elif 0.1 <= number <= 300:
                return 0.7
            else:
                return 0.3
        
        elif "è¬å™¸" in value or "åƒå™¸" in value:
            if 0.1 <= number <= 50:
                return 1.0
            elif 0.01 <= number <= 200:
                return 0.7
            else:
                return 0.3
        
        elif "å™¸" in value and "è¬" not in value and "åƒ" not in value:
            if 10 <= number <= 50000:
                return 1.0
            elif 1 <= number <= 100000:
                return 0.7
            else:
                return 0.3
        
        elif "%" in value or "ï¼…" in value:
            if 0.1 <= number <= 100:
                return 1.0
            else:
                return 0.2
        
        elif "ä»¶" in value:
            if 10 <= number <= 10000:
                return 1.0
            elif 1 <= number <= 50000:
                return 0.7
            else:
                return 0.3
        
        return 0.5

# =============================================================================
# Wordæ–‡æª”è¼¸å‡ºåŠŸèƒ½
# =============================================================================

class ESGWordExporter:
    """ESG Wordæ–‡æª”å°å‡ºå™¨"""
    
    def __init__(self):
        pass
    
    def create_word_document(self, extractions: List, doc_info, stock_code: str, short_company_name: str) -> str:
        """å‰µå»ºWordæ–‡æª”"""
        
        # ç”ŸæˆWordæª”æ¡ˆå
        if stock_code:
            word_filename = f"{stock_code}_{short_company_name}_{doc_info.report_year}_æå–çµ±æ•´.docx"
        else:
            company_safe = re.sub(r'[^\w\s-]', '', short_company_name).strip()
            word_filename = f"{company_safe}_{doc_info.report_year}_æå–çµ±æ•´.docx"
        
        word_path = os.path.join(RESULTS_PATH, word_filename)
        
        # å‰µå»ºWordæ–‡æª”
        doc = Document()
        
        # è¨­ç½®æ–‡æª”æ¨™é¡Œ
        title = doc.add_heading(f'{short_company_name} {doc_info.report_year}å¹´ ESGæå–çµ±æ•´å ±å‘Š', 0)
        title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
        
        # æ·»åŠ æ‘˜è¦ä¿¡æ¯
        summary_para = doc.add_paragraph()
        summary_para.add_run(f'è‚¡ç¥¨ä»£è™Ÿ: ').bold = True
        summary_para.add_run(f'{stock_code or "N/A"}\n')
        summary_para.add_run(f'å…¬å¸å…¨ç¨±: ').bold = True
        summary_para.add_run(f'{doc_info.company_name}\n')
        summary_para.add_run(f'å ±å‘Šå¹´åº¦: ').bold = True
        summary_para.add_run(f'{doc_info.report_year}\n')
        summary_para.add_run(f'æå–æ™‚é–“: ').bold = True
        summary_para.add_run(f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        summary_para.add_run(f'æå–çµæœæ•¸é‡: ').bold = True
        summary_para.add_run(f'{len(extractions)} é …')
        
        # æ·»åŠ åˆ†éš”ç·š
        doc.add_paragraph('=' * 80)
        
        if not extractions:
            doc.add_heading('æœªæ‰¾åˆ°ç›¸é—œæå–çµæœ', level=1)
            no_result_para = doc.add_paragraph()
            no_result_para.add_run('åœ¨æ­¤ä»½ESGå ±å‘Šä¸­æœªæ‰¾åˆ°å†ç”Ÿå¡‘è† æˆ–æ°¸çºŒææ–™ç›¸é—œçš„æ•¸å€¼æ•¸æ“šã€‚\n\n')
            no_result_para.add_run('å¯èƒ½çš„åŸå› ï¼š\n')
            no_result_para.add_run('1. è©²å…¬å¸æœªæ¶‰åŠç›¸é—œæ¥­å‹™\n')
            no_result_para.add_run('2. å ±å‘Šä¸­æœªè©³ç´°æŠ«éœ²ç›¸é—œæ•¸æ“š\n')
            no_result_para.add_run('3. é—œéµå­—åŒ¹é…ç¯„åœéœ€è¦èª¿æ•´')
        else:
            # æŒ‰é ç¢¼æ’åº
            sorted_extractions = sorted(extractions, key=lambda x: (
                int(re.findall(r'\d+', x.page_number)[0]) if re.findall(r'\d+', x.page_number) else 999,
                x.confidence
            ), reverse=False)
            
            for i, extraction in enumerate(sorted_extractions, 1):
                # æ·»åŠ åºè™Ÿæ¨™é¡Œ
                doc.add_heading(f'æå–çµæœ {i}', level=2)
                
                # é ç¢¼
                page_para = doc.add_paragraph()
                page_para.add_run('é ç¢¼: ').bold = True
                page_para.add_run(f'{extraction.page_number}')
                
                # é—œéµå­—
                keyword_para = doc.add_paragraph()
                keyword_para.add_run('é—œéµå­—: ').bold = True
                keyword_para.add_run(f'{extraction.keyword}')
                
                # æ•¸å€¼
                value_para = doc.add_paragraph()
                value_para.add_run('æ•¸å€¼: ').bold = True
                if extraction.value == "[ç›¸é—œæè¿°]":
                    value_para.add_run('ç›¸é—œæè¿°ï¼ˆç„¡å…·é«”æ•¸å€¼ï¼‰')
                else:
                    value_para.add_run(f'{extraction.value}')
                    if extraction.unit:
                        value_para.add_run(f' ({extraction.unit})')
                
                # ä¿¡å¿ƒåˆ†æ•¸
                confidence_para = doc.add_paragraph()
                confidence_para.add_run('ä¿¡å¿ƒåˆ†æ•¸: ').bold = True
                confidence_para.add_run(f'{extraction.confidence:.3f}')
                
                # æ•´å€‹æ®µè½å…§å®¹
                content_para = doc.add_paragraph()
                content_para.add_run('æ®µè½å…§å®¹: ').bold = True
                content_para.add_run('\n')
                
                # æ®µè½å…§å®¹ä½¿ç”¨ä¸åŒçš„å­—é«”
                content_run = content_para.add_run(extraction.paragraph)
                content_run.font.name = 'å¾®è»Ÿæ­£é»‘é«”'
                content_run.font.size = Pt(10)
                
                # æ·»åŠ åˆ†éš”ç·š
                if i < len(sorted_extractions):
                    doc.add_paragraph('-' * 60)
        
        # æ·»åŠ é å°¾ä¿¡æ¯
        footer_para = doc.add_paragraph()
        footer_para.add_run('\n\nç”Ÿæˆå·¥å…·: ').italic = True
        footer_para.add_run('ESGå ±å‘Šæ›¸æå–å™¨ v2.0').italic = True
        footer_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
        
        # ä¿å­˜æ–‡æª”
        doc.save(word_path)
        
        return word_path

# =============================================================================
# è‚¡ç¥¨ä»£è™Ÿæ˜ å°„å™¨ï¼ˆä¿æŒä¸è®Šï¼‰
# =============================================================================

class StockCodeMapper:
    """è‚¡ç¥¨ä»£è™Ÿèˆ‡å…¬å¸åç¨±å°ç…§æ˜ å°„å™¨"""
    
    def __init__(self):
        # å°ç£ä¸»è¦ä¸Šå¸‚å…¬å¸è‚¡ç¥¨ä»£è™Ÿå°ç…§è¡¨
        self.stock_code_mapping = {
            # çŸ³åŒ–å¡‘è† é¡
            "1301": "å°å¡‘",
            "1303": "å—äº", 
            "1326": "å°åŒ–",
            "1314": "ä¸­çŸ³åŒ–",
            "1605": "è¯æ–°",
            "1722": "å°è‚¥",
            "6505": "å°å¡‘åŒ–",
            
            # é›»å­ç§‘æŠ€é¡
            "2330": "å°ç©é›»",
            "2454": "è¯ç™¼ç§‘",
            "2317": "é´»æµ·",
            "2382": "å»£é”",
            "2308": "å°é”é›»",
            "3045": "å°ç£å¤§",
            "4904": "é å‚³",
            "3008": "å¤§ç«‹å…‰",
            "2474": "å¯æˆ",
            "6770": "åŠ›ç©é›»",
            
            # é‡‘èé¡
            "2884": "ç‰å±±é‡‘",
            "2885": "å…ƒå¤§é‡‘", 
            "2886": "å…†è±é‡‘",
            "2887": "å°æ–°é‡‘",
            "2888": "æ–°å…‰é‡‘",
            "2891": "ä¸­ä¿¡é‡‘",
            "2892": "ç¬¬ä¸€é‡‘",
            "2880": "è¯å—é‡‘",
            
            # å‚³çµ±ç”¢æ¥­
            "2002": "ä¸­é‹¼",
            "1216": "çµ±ä¸€",
            "1101": "å°æ³¥",
            "2408": "å—ç§‘",
            "2409": "å‹é”",
            "2412": "ä¸­è¯é›»",
            "3481": "ç¾¤å‰µ",
            "2207": "å’Œæ³°è»Š",
            "2912": "çµ±ä¸€è¶…",
            "6278": "å°è¡¨ç§‘",
        }
        
        # å…¬å¸åç¨±åå‘å°ç…§ï¼ˆç”¨æ–¼æŸ¥æ‰¾è‚¡ç¥¨ä»£è™Ÿï¼‰
        self.reverse_mapping = {}
        for code, name in self.stock_code_mapping.items():
            self.reverse_mapping[name] = code
            # æ·»åŠ å®Œæ•´å…¬å¸åç¨±çš„å°ç…§
            full_names = {
                "å°å¡‘": ["å°ç£å¡‘è† å·¥æ¥­", "å°å¡‘å·¥æ¥­", "å°ç£å¡‘è† ", "å°å¡‘å…¬å¸"],
                "å—äº": ["å—äºå¡‘è† å·¥æ¥­", "å—äºå¡‘è† ", "å—äºå…¬å¸"],
                "å°åŒ–": ["å°ç£åŒ–å­¸çº–ç¶­", "å°åŒ–å…¬å¸", "å°ç£åŒ–çº–"],
                "å°ç©é›»": ["å°ç£ç©é«”é›»è·¯", "å°ç£ç©é«”é›»è·¯è£½é€ ", "TSMC"],
                "é´»æµ·": ["é´»æµ·ç²¾å¯†", "é´»æµ·ç§‘æŠ€"],
                "ä¸­é‹¼": ["ä¸­åœ‹é‹¼éµ", "ä¸­é‹¼å…¬å¸"],
                "å°æ³¥": ["å°ç£æ°´æ³¥", "å°æ³¥å…¬å¸"],
                "çµ±ä¸€": ["çµ±ä¸€ä¼æ¥­", "çµ±ä¸€å…¬å¸"],
                "ä¸­è¯é›»": ["ä¸­è¯é›»ä¿¡", "ä¸­è¯é›»ä¿¡å…¬å¸"]
            }
            
            if name in full_names:
                for full_name in full_names[name]:
                    self.reverse_mapping[full_name] = code
    
    def extract_stock_info_from_vector_name(self, vector_name: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        å¾å‘é‡è³‡æ–™åº«åç¨±ä¸­æå–è‚¡ç¥¨ä»£è™Ÿã€å…¬å¸åç¨±ã€å¹´åº¦
        ä¾‹å¦‚ï¼šesg_db_1301_å°å¡‘_2024_esgå ±å‘Šæ›¸ -> ("1301", "å°å¡‘", "2024")
        """
        try:
            # ç§»é™¤å‰ç¶´å’Œå¾Œç¶´
            cleaned_name = vector_name.replace("esg_db_", "")
            
            # å˜—è©¦åŒ¹é…æ ¼å¼ï¼šè‚¡ç¥¨ä»£è™Ÿ_å…¬å¸åç¨±_å¹´åº¦_å…¶ä»–
            pattern = r'^(\d{4})_([^_]+)_(\d{4})(?:_.*)?$'
            match = re.match(pattern, cleaned_name)
            
            if match:
                stock_code = match.group(1)
                company_name = match.group(2)
                year = match.group(3)
                return stock_code, company_name, year
            
            # å¦‚æœæ²’æœ‰è‚¡ç¥¨ä»£è™Ÿï¼Œå˜—è©¦åŒ¹é…ï¼šå…¬å¸åç¨±_å¹´åº¦
            pattern2 = r'^([^_]+)_(\d{4})(?:_.*)?$'
            match2 = re.match(pattern2, cleaned_name)
            
            if match2:
                company_name = match2.group(1)
                year = match2.group(2)
                # å˜—è©¦æŸ¥æ‰¾è‚¡ç¥¨ä»£è™Ÿ
                stock_code = self.find_stock_code_by_company(company_name)
                return stock_code, company_name, year
            
            return None, None, None
            
        except Exception as e:
            print(f"âš ï¸ è§£æå‘é‡åç¨±å¤±æ•—: {e}")
            return None, None, None
    
    def find_stock_code_by_company(self, company_name: str) -> Optional[str]:
        """æ ¹æ“šå…¬å¸åç¨±æŸ¥æ‰¾è‚¡ç¥¨ä»£è™Ÿ"""
        if not company_name:
            return None
        
        # ç›´æ¥åŒ¹é…
        if company_name in self.reverse_mapping:
            return self.reverse_mapping[company_name]
        
        # æ¨¡ç³ŠåŒ¹é…
        company_clean = company_name.replace("è‚¡ä»½æœ‰é™å…¬å¸", "").replace("æœ‰é™å…¬å¸", "").replace("å…¬å¸", "").strip()
        
        for mapped_name, code in self.reverse_mapping.items():
            if company_clean in mapped_name or mapped_name in company_clean:
                return code
        
        # é—œéµå­—åŒ¹é…
        for mapped_name, code in self.reverse_mapping.items():
            if len(mapped_name) >= 2 and mapped_name in company_name:
                return code
        
        return None
    
    def get_short_company_name(self, company_name: str, stock_code: str = None) -> str:
        """ç²å–ç°¡åŒ–çš„å…¬å¸åç¨±"""
        if not company_name:
            return "æœªçŸ¥å…¬å¸"
        
        # å¦‚æœæœ‰è‚¡ç¥¨ä»£è™Ÿï¼Œç›´æ¥ä½¿ç”¨å°ç…§è¡¨ä¸­çš„ç°¡ç¨±
        if stock_code and stock_code in self.stock_code_mapping:
            return self.stock_code_mapping[stock_code]
        
        # æ‰‹å‹•ç°¡åŒ–å¸¸è¦‹å…¬å¸åç¨±
        simplifications = {
            "å°ç£å¡‘è† å·¥æ¥­è‚¡ä»½æœ‰é™å…¬å¸": "å°å¡‘",
            "å°ç£å¡‘è† å·¥æ¥­": "å°å¡‘", 
            "å°å¡‘å·¥æ¥­": "å°å¡‘",
            "å—äºå¡‘è† å·¥æ¥­è‚¡ä»½æœ‰é™å…¬å¸": "å—äº",
            "å—äºå¡‘è† å·¥æ¥­": "å—äº",
            "å—äºå¡‘è† ": "å—äº",
            "å°ç£åŒ–å­¸çº–ç¶­è‚¡ä»½æœ‰é™å…¬å¸": "å°åŒ–",
            "å°ç£åŒ–å­¸çº–ç¶­": "å°åŒ–",
            "å°åŒ–å…¬å¸": "å°åŒ–",
            "å°ç£ç©é«”é›»è·¯è£½é€ è‚¡ä»½æœ‰é™å…¬å¸": "å°ç©é›»",
            "å°ç£ç©é«”é›»è·¯": "å°ç©é›»",
            "é´»æµ·ç²¾å¯†å·¥æ¥­è‚¡ä»½æœ‰é™å…¬å¸": "é´»æµ·",
            "é´»æµ·ç²¾å¯†": "é´»æµ·",
            "ä¸­åœ‹é‹¼éµè‚¡ä»½æœ‰é™å…¬å¸": "ä¸­é‹¼",
            "ä¸­åœ‹é‹¼éµ": "ä¸­é‹¼",
            "å°ç£æ°´æ³¥è‚¡ä»½æœ‰é™å…¬å¸": "å°æ³¥", 
            "å°ç£æ°´æ³¥": "å°æ³¥",
            "çµ±ä¸€ä¼æ¥­è‚¡ä»½æœ‰é™å…¬å¸": "çµ±ä¸€",
            "çµ±ä¸€ä¼æ¥­": "çµ±ä¸€",
            "ä¸­è¯é›»ä¿¡è‚¡ä»½æœ‰é™å…¬å¸": "ä¸­è¯é›»",
            "ä¸­è¯é›»ä¿¡": "ä¸­è¯é›»"
        }
        
        # ç²¾ç¢ºåŒ¹é…
        if company_name in simplifications:
            return simplifications[company_name]
        
        # ç§»é™¤å¸¸è¦‹å¾Œç¶´ä¸¦ç°¡åŒ–
        simplified = company_name
        suffixes = ["è‚¡ä»½æœ‰é™å…¬å¸", "æœ‰é™å…¬å¸", "è‚¡ä»½å…¬å¸", "å…¬å¸", "é›†åœ˜", "ä¼æ¥­"]
        
        for suffix in suffixes:
            if simplified.endswith(suffix):
                simplified = simplified[:-len(suffix)].strip()
                break
        
        # å¦‚æœç°¡åŒ–å¾Œé•·åº¦åˆé©ï¼Œè¿”å›ç°¡åŒ–çµæœ
        if 2 <= len(simplified) <= 4:
            return simplified
        
        # å¦å‰‡è¿”å›å‰4å€‹å­—ç¬¦
        return simplified[:4] if len(simplified) > 4 else simplified

# =============================================================================
# åŸæœ‰æ•¸æ“šçµæ§‹ï¼ˆä¿æŒä¸è®Šï¼‰
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""
    keyword_distance: int = 0

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# å¢å¼·ç‰ˆESGæå–å™¨ä¸»é¡
# =============================================================================

class EnhancedESGExtractor:
    """å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨ä¸»é¡"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = EnhancedESGMatcher()  # ä½¿ç”¨å¢å¼·ç‰ˆåŒ¹é…å™¨
        self.keyword_config = EnhancedKeywordConfig()  # ä½¿ç”¨å¢å¼·ç‰ˆé—œéµå­—é…ç½®
        self.stock_mapper = StockCodeMapper()
        self.word_exporter = ESGWordExporter()  # æ–°å¢Wordå°å‡ºå™¨
        
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ”§ æ–°åŠŸèƒ½ï¼šæ”¯æ´æ“´å±•é—œéµå­—ã€æé«˜æå–æº–ç¢ºåº¦ã€Wordæ–‡æª”è¼¸å‡º")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 400) -> Tuple[List[NumericExtraction], ProcessingSummary, str, str]:
        """è™•ç†å–®å€‹æ–‡æª” - å¢å¼·ç‰ˆï¼Œè¿”å›Excelå’ŒWordæª”æ¡ˆè·¯å¾‘"""
        start_time = datetime.now()
        print(f"\nğŸ“Š è™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. æ–‡æª”æª¢ç´¢
        documents = self._document_retrieval(db, max_documents)
        
        # 3. æ•¸æ“šæå–ï¼ˆä½¿ç”¨å¢å¼·ç‰ˆåŒ¹é…å™¨ï¼‰
        extractions = self._extract_data(documents, doc_info)
        
        # 4. å¾Œè™•ç†
        extractions = self._post_process_extractions(extractions)
        
        # 5. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            total_documents=len(documents),
            stage1_passed=len(documents),
            stage2_passed=len(extractions),
            total_extractions=len(extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 6. ç²å–è‚¡ç¥¨ä¿¡æ¯
        vector_db_name = Path(doc_info.db_path).name
        stock_code, extracted_company, extracted_year = self.stock_mapper.extract_stock_info_from_vector_name(vector_db_name)
        
        final_company = extracted_company if extracted_company else doc_info.company_name
        final_year = extracted_year if extracted_year else doc_info.report_year
        final_stock_code = stock_code
        
        if not final_stock_code:
            final_stock_code = self.stock_mapper.find_stock_code_by_company(final_company)
        
        short_company_name = self.stock_mapper.get_short_company_name(final_company, final_stock_code)
        
        # 7. åŒ¯å‡ºExcelçµæœ
        excel_path = self._export_to_excel(extractions, summary, doc_info, final_stock_code, short_company_name)
        
        # 8. åŒ¯å‡ºWordçµæœ
        word_path = self.word_exporter.create_word_document(extractions, doc_info, final_stock_code, short_company_name)
        
        print(f"ğŸ“Š Excelæª”æ¡ˆ: {Path(excel_path).name}")
        print(f"ğŸ“ Wordæª”æ¡ˆ: {Path(word_path).name}")
        
        return extractions, summary, excel_path, word_path
    
    def process_multiple_documents(self, docs_info: Dict[str, DocumentInfo], max_documents: int = 400) -> Dict[str, Tuple]:
        """æ‰¹é‡è™•ç†å¤šå€‹æ–‡æª” - å¢å¼·ç‰ˆ"""
        print(f"ğŸ“Š é–‹å§‹æ‰¹é‡è™•ç† {len(docs_info)} å€‹æ–‡æª”")
        print("=" * 60)
        
        results = {}
        
        for pdf_path, doc_info in docs_info.items():
            try:
                print(f"\nğŸ“„ è™•ç†: {doc_info.company_name} - {doc_info.report_year}")
                
                extractions, summary, excel_path, word_path = self.process_single_document(doc_info, max_documents)
                
                results[pdf_path] = (extractions, summary, excel_path, word_path)
                
                print(f"âœ… å®Œæˆ: ç”Ÿæˆ {len(extractions)} å€‹çµæœ")
                print(f"   ğŸ“Š Excel: {Path(excel_path).name}")
                print(f"   ğŸ“ Word: {Path(word_path).name}")
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•— {doc_info.company_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\nğŸ‰ æ‰¹é‡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(docs_info)} å€‹æ–‡æª”")
        return results
    
    # ä»¥ä¸‹æ–¹æ³•å¤§éƒ¨åˆ†ä¿æŒä¸è®Šï¼Œåªä¿®æ”¹é—œéµéƒ¨åˆ†
    
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _document_retrieval(self, db, max_docs: int) -> List[LangchainDocument]:
        """æ–‡æª”æª¢ç´¢ - ä½¿ç”¨æ–°çš„é—œéµå­—é…ç½®"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # é—œéµå­—æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œé—œéµå­—æª¢ç´¢...")
        for keyword in keywords[:30]:  # å¢åŠ æª¢ç´¢ç¯„åœ
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = db.similarity_search(search_term, k=12)
            all_docs.extend(docs)
        
        # ä¸»é¡Œæª¢ç´¢ - å¢å¼·ç‰ˆ
        print("   ğŸ” åŸ·è¡Œä¸»é¡Œæª¢ç´¢...")
        topic_queries = [
            "å¡‘è†  å›æ”¶ ææ–™",
            "å¯¶ç‰¹ç“¶ å¾ªç’° ç¶“æ¿Ÿ",
            "å†ç”Ÿ ç’°ä¿ æ°¸çºŒ",
            "å»¢æ–™ è™•ç† åˆ©ç”¨",
            "ææ–™ å¾ªç’° ä½¿ç”¨",  # æ–°å¢ä¸»é¡Œ
            "ç¢³æ’ æ¸›é‡ æ•ˆç›Š",  # æ–°å¢ä¸»é¡Œ
            "å†ç”Ÿèƒ½æº ç¶ é›»",   # æ–°å¢ä¸»é¡Œ
        ]
        
        for query in topic_queries:
            docs = db.similarity_search(query, k=15)
            all_docs.extend(docs)
        
        # æ•¸å€¼æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œæ•¸å€¼æª¢ç´¢...")
        number_queries = [
            "å„„æ”¯", "è¬å™¸", "åƒå™¸", "ç”¢èƒ½", "å›æ”¶é‡", "ä½¿ç”¨é‡",
            "æ¸›ç¢³", "ç™¾åˆ†æ¯”", "æ•ˆç›Š", "æ•¸é‡", "æ¯”ä¾‹", "æ¯”ç‡"
        ]
        
        for query in number_queries:
            docs = db.similarity_search(query, k=8)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹å€™é¸æ–‡æª”")
        return result_docs
    
    def _extract_data(self, documents: List[LangchainDocument], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """æ•¸æ“šæå– - ä½¿ç”¨å¢å¼·ç‰ˆåŒ¹é…å™¨"""
        print("ğŸ¯ åŸ·è¡Œæ•¸æ“šæå–ï¼ˆå¢å¼·ç‰ˆï¼‰...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="æ•¸æ“šæå–"):
            # æ®µè½åˆ†å‰²
            paragraphs = self._split_paragraphs(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 20:  # æé«˜æœ€å°æ®µè½é•·åº¦
                    continue
                
                # å°æ¯å€‹é—œéµå­—é€²è¡ŒåŒ¹é…
                for keyword in keywords:
                    # ä½¿ç”¨å¢å¼·ç‰ˆç›¸é—œæ€§æª¢æŸ¥
                    is_relevant, relevance_score, details = self.matcher.comprehensive_relevance_check(paragraph, keyword)
                    
                    if is_relevant and relevance_score > 0.65:  # æé«˜ç›¸é—œæ€§é–¾å€¼
                        # æå–æ•¸å€¼é…å°
                        value_pairs = self.matcher.extract_keyword_value_pairs(paragraph, keyword)
                        
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ•¸å€¼ä½†ç›¸é—œæ€§å¾ˆé«˜ï¼Œä¿ç•™ä½œç‚ºæè¿°
                        if not value_pairs and relevance_score > 0.80:  # æé«˜æè¿°ä¿ç•™é–¾å€¼
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value="[ç›¸é—œæè¿°]",
                                value_type='description',
                                unit='',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                keyword_distance=0
                            )
                            extractions.append(extraction)
                        
                        # è™•ç†æ‰¾åˆ°çš„æ•¸å€¼é…å°
                        for value, value_type, association_score, distance in value_pairs:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            final_confidence = (relevance_score * 0.4 + association_score * 0.6)
                            
                            # åªä¿ç•™é«˜ä¿¡å¿ƒåº¦çš„çµæœ
                            if final_confidence > 0.70:
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=value,
                                    value_type=value_type,
                                    unit=self._extract_unit(value) if value_type == 'number' else '%',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=final_confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph),
                                    company_name=doc_info.company_name,
                                    report_year=doc_info.report_year,
                                    keyword_distance=distance
                                )
                                extractions.append(extraction)
        
        print(f"âœ… æ•¸æ“šæå–å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹çµæœ")
        return extractions
    
    def _post_process_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å¾Œè™•ç†å’Œå»é‡ - å¢å¼·ç‰ˆ"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”§ å¾Œè™•ç† {len(extractions)} å€‹æå–çµæœ...")
        
        # ç²¾ç¢ºå»é‡ï¼ˆæ›´åš´æ ¼ï¼‰
        unique_extractions = []
        seen_combinations = set()
        
        for extraction in extractions:
            identifier = (
                extraction.keyword,
                extraction.value,
                extraction.value_type,
                extraction.paragraph[:150]  # å¢åŠ æ®µè½æª¢æŸ¥é•·åº¦
            )
            
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_extractions.append(extraction)
        
        print(f"ğŸ“Š ç²¾ç¢ºå»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™3ç­†é«˜è³ªé‡çµæœï¼‰
        page_filtered_extractions = self._apply_per_page_filtering(unique_extractions, max_per_page=3)
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        page_filtered_extractions.sort(key=lambda x: x.confidence, reverse=True)
        
        print(f"âœ… å¾Œè™•ç†å®Œæˆ: ä¿ç•™ {len(page_filtered_extractions)} å€‹æœ€çµ‚çµæœ")
        return page_filtered_extractions
    
    def _apply_per_page_filtering(self, extractions: List[NumericExtraction], max_per_page: int = 3) -> List[NumericExtraction]:
        """æŒ‰é é¢å»é‡"""
        if not extractions:
            return extractions
        
        print(f"ğŸ“„ åŸ·è¡ŒæŒ‰é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™ {max_per_page} ç­†ï¼‰...")
        
        # æŒ‰é ç¢¼åˆ†çµ„
        page_groups = {}
        for extraction in extractions:
            page_key = str(extraction.page_number).strip()
            if page_key not in page_groups:
                page_groups[page_key] = []
            page_groups[page_key].append(extraction)
        
        # æ¯é é¢å…§æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åºä¸¦ä¿ç•™æœ€ä½³çµæœ
        filtered_extractions = []
        
        for page_key, page_extractions in page_groups.items():
            # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
            page_extractions.sort(key=lambda x: x.confidence, reverse=True)
            
            # é€²ä¸€æ­¥å»é‡ï¼šé¿å…åŒä¸€é é¢çš„é‡è¤‡å…§å®¹
            page_unique = []
            seen_values = set()
            
            for extraction in page_extractions:
                value_key = (extraction.value, extraction.keyword)
                if value_key not in seen_values:
                    seen_values.add(value_key)
                    page_unique.append(extraction)
            
            kept_extractions = page_unique[:max_per_page]
            filtered_extractions.extend(kept_extractions)
        
        print(f"   âœ… é é¢å»é‡å®Œæˆ: {len(filtered_extractions)} ç­†æœ€çµ‚çµæœ")
        return filtered_extractions
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, 
                        doc_info: DocumentInfo, stock_code: str, short_company_name: str) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excel - ä¿æŒåŸæœ‰é‚è¼¯"""
        # ç”ŸæˆExcelæª”æ¡ˆå
        if stock_code:
            if len(extractions) == 0:
                output_filename = f"ç„¡æå–_{stock_code}_{short_company_name}_{doc_info.report_year}.xlsx"
                status_message = "ç„¡æå–çµæœ"
            else:
                output_filename = f"{stock_code}_{short_company_name}_{doc_info.report_year}.xlsx"
                status_message = f"æå–çµæœ: {len(extractions)} é …"
        else:
            company_safe = re.sub(r'[^\w\s-]', '', short_company_name).strip()
            if len(extractions) == 0:
                output_filename = f"ç„¡æå–_{company_safe}_{doc_info.report_year}.xlsx"
                status_message = "ç„¡æå–çµæœ"
            else:
                output_filename = f"{company_safe}_{doc_info.report_year}.xlsx"
                status_message = f"æå–çµæœ: {len(extractions)} é …"
        
        output_path = os.path.join(RESULTS_PATH, output_filename)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºExcelçµæœ: {output_filename}")
        if stock_code:
            print(f"   ğŸ¢ {stock_code} - {short_company_name} ({doc_info.report_year})")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        
        # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯ï¼ˆåŒ…å«è‚¡ç¥¨ä»£è™Ÿï¼‰
        header_row = {
            'é—œéµå­—': f"è‚¡ç¥¨ä»£è™Ÿ: {stock_code or 'N/A'} | å…¬å¸: {doc_info.company_name}",
            'æå–æ•¸å€¼': f"å ±å‘Šå¹´åº¦: {doc_info.report_year}",
            'æ•¸æ“šé¡å‹': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            'å–®ä½': f"å‘é‡åº«: {Path(doc_info.db_path).name}",
            'æ®µè½å…§å®¹': f"{status_message}ï¼ˆESGå ±å‘Šæ›¸æå–å™¨ v2.0 å¢å¼·ç‰ˆï¼‰",
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': '',
            'ä¸Šä¸‹æ–‡': ''
        }
        main_data.append(header_row)
        
        # ç©ºè¡Œåˆ†éš”
        main_data.append({col: '' for col in header_row.keys()})
        
        # å¦‚æœæœ‰æå–çµæœï¼Œæ·»åŠ çµæœæ•¸æ“š
        if len(extractions) > 0:
            for extraction in extractions:
                main_data.append({
                    'é—œéµå­—': extraction.keyword,
                    'æå–æ•¸å€¼': extraction.value,
                    'æ•¸æ“šé¡å‹': extraction.value_type,
                    'å–®ä½': extraction.unit,
                    'æ®µè½å…§å®¹': extraction.paragraph,
                    'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                    'é ç¢¼': extraction.page_number,
                    'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                    'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
                })
        else:
            # å¦‚æœæ²’æœ‰æå–çµæœï¼Œæ·»åŠ èªªæ˜è¡Œ
            no_result_row = {
                'é—œéµå­—': 'ç„¡ç›¸é—œé—œéµå­—åŒ¹é…',
                'æå–æ•¸å€¼': 'N/A',
                'æ•¸æ“šé¡å‹': 'no_data',
                'å–®ä½': '',
                'æ®µè½å…§å®¹': 'åœ¨æ­¤ä»½ESGå ±å‘Šä¸­æœªæ‰¾åˆ°å†ç”Ÿå¡‘è† æˆ–æ°¸çºŒææ–™ç›¸é—œçš„æ•¸å€¼æ•¸æ“š',
                'æ®µè½ç·¨è™Ÿ': '',
                'é ç¢¼': '',
                'ä¿¡å¿ƒåˆ†æ•¸': 0.0,
                'ä¸Šä¸‹æ–‡': 'å¯èƒ½çš„åŸå› ï¼š1) è©²å…¬å¸æœªæ¶‰åŠç›¸é—œæ¥­å‹™ 2) å ±å‘Šä¸­æœªè©³ç´°æŠ«éœ²ç›¸é—œæ•¸æ“š 3) é—œéµå­—åŒ¹é…ç¯„åœéœ€è¦èª¿æ•´'
            }
            main_data.append(no_result_row)
        
        # çµ±è¨ˆæ•¸æ“š
        stats_data = []
        if len(extractions) > 0:
            for keyword, count in summary.keywords_found.items():
                keyword_extractions = [e for e in extractions if e.keyword == keyword]
                
                stats_data.append({
                    'é—œéµå­—': keyword,
                    'æå–æ•¸é‡': count,
                    'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                    'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
                })
        else:
            stats_data.append({
                'é—œéµå­—': 'æœå°‹æ‘˜è¦',
                'æå–æ•¸é‡': 0,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': 0.0,
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': 0.0
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ä¸»è¦çµæœå·¥ä½œè¡¨
            sheet_name = 'æå–çµæœ' if len(extractions) > 0 else 'ç„¡æå–çµæœ'
            pd.DataFrame(main_data).to_excel(writer, sheet_name=sheet_name, index=False)
            
            # çµ±è¨ˆå·¥ä½œè¡¨
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='çµ±è¨ˆæ‘˜è¦', index=False)
            
            # è™•ç†æ‘˜è¦ï¼ˆåŒ…å«è‚¡ç¥¨ä»£è™Ÿä¿¡æ¯ï¼‰
            summary_data = [{
                'è‚¡ç¥¨ä»£è™Ÿ': stock_code or 'N/A',
                'å…¬å¸åç¨±': doc_info.company_name,
                'å…¬å¸ç°¡ç¨±': short_company_name,
                'å ±å‘Šå¹´åº¦': doc_info.report_year,
                'ç¸½æ–‡æª”æ•¸': summary.total_documents,
                'ç¸½æå–çµæœ': summary.total_extractions,
                'è™•ç†ç‹€æ…‹': 'æˆåŠŸæå–' if len(extractions) > 0 else 'ç„¡ç›¸é—œæ•¸æ“š',
                'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
                'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'æå–å™¨ç‰ˆæœ¬': 'ESGå ±å‘Šæ›¸æå–å™¨ v2.0 å¢å¼·ç‰ˆ',
                'å‘é‡è³‡æ–™åº«': Path(doc_info.db_path).name
            }]
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        if len(extractions) > 0:
            print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼ŒåŒ…å« {len(extractions)} é …æå–çµæœ")
        else:
            print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼Œæ¨™è¨˜ç‚ºç„¡æå–çµæœ")
        
        return output_path
    
    # è¼”åŠ©æ–¹æ³•ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰
    def _split_paragraphs(self, text: str) -> List[str]:
        """æ®µè½åˆ†å‰²"""
        paragraphs = []
        
        # æ¨™æº–åˆ†å‰²
        standard_paras = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs.extend([p.strip() for p in standard_paras if len(p.strip()) >= 20])
        
        # å¥è™Ÿåˆ†å‰²
        sentence_paras = re.split(r'ã€‚{2,}|\.{2,}', text)
        paragraphs.extend([p.strip() for p in sentence_paras if len(p.strip()) >= 40])
        
        # ä¿æŒåŸæ–‡
        if len(text.strip()) >= 60:
            paragraphs.append(text.strip())
        
        # å»é‡
        unique_paragraphs = []
        seen = set()
        for para in paragraphs:
            para_hash = hash(para[:100])
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)
        
        return unique_paragraphs
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 200) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:400]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:400]

# =============================================================================
# ç‚ºäº†å‘å¾Œå…¼å®¹ï¼Œä¿ç•™åŸæœ‰é¡åçš„åˆ¥å
# =============================================================================

ESGExtractor = EnhancedESGExtractor  # åˆ¥åï¼Œç¢ºä¿ä¸»ç¨‹å¼å¯ä»¥æ­£å¸¸ä½¿ç”¨

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦ç”¨"""
    print("ğŸ“Š å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨æ¸¬è©¦æ¨¡å¼")
    
    extractor = EnhancedESGExtractor(enable_llm=False)
    print("âœ… å¢å¼·ç‰ˆESGå ±å‘Šæ›¸æå–å™¨åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    main()