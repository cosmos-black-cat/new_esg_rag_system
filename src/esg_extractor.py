#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ v2.1
æ•´åˆå¢å¼·é—œéµå­—éæ¿¾ã€ç²¾ç¢ºç›¸é—œæ€§æª¢æŸ¥ã€LLMå¢å¼·ã€æ™ºèƒ½å»é‡
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from tqdm import tqdm
import numpy as np
from difflib import SequenceMatcher

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *

# å°å…¥å¢å¼·çš„é—œéµå­—é…ç½®å’Œéæ¿¾ç®¡é“
try:
    from keywords_config import (
        enhanced_filtering_pipeline, 
        EnhancedKeywordConfig, 
        KeywordConfig,
        EnhancedMatcher
    )
    ENHANCED_KEYWORDS_AVAILABLE = True
    print("âœ… å¢å¼·é—œéµå­—é…ç½®å·²è¼‰å…¥")
except ImportError as e:
    print(f"âš ï¸ å¢å¼·é—œéµå­—é…ç½®è¼‰å…¥å¤±æ•—: {e}")
    ENHANCED_KEYWORDS_AVAILABLE = False
    # å›é€€åˆ°åŸºæœ¬é…ç½®
    class KeywordConfig:
        @classmethod 
        def get_all_keywords(cls):
            return ["å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”Ÿpp"]

# å°å…¥APIç®¡ç†å™¨
try:
    from api_manager import GeminiAPIManager
    API_MANAGER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ APIç®¡ç†å™¨è¼‰å…¥å¤±æ•—: {e}")
    API_MANAGER_AVAILABLE = False

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class ExtractionMatch:
    """å–®å€‹åŒ¹é…çµæœ"""
    keyword: str
    keyword_type: str  # 'continuous' or 'discontinuous'
    confidence: float
    matched_text: str
    
@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float
    enhanced_filtering_used: bool = False
    filtering_stats: Dict = None

# =============================================================================
# å¢å¼·çš„LLMç®¡ç†å™¨
# =============================================================================

class EnhancedLLMManager:
    """å¢å¼·çš„LLMç®¡ç†å™¨ï¼Œæ”¯æ´å¤šAPIå’Œæ”¹é€²çš„éŸ¿æ‡‰è§£æ"""
    
    def __init__(self, api_keys: List[str], model_name: str):
        self.api_keys = api_keys
        self.model_name = model_name
        self.success_count = 0
        self.total_count = 0
        
        if len(api_keys) > 1 and API_MANAGER_AVAILABLE:
            print(f"ğŸ”„ å•Ÿç”¨å¤šAPIè¼ªæ›æ¨¡å¼ï¼Œå…± {len(api_keys)} å€‹Keys")
            self.api_manager = GeminiAPIManager(api_keys, model_name)
            self.mode = "multi_api"
        else:
            print("ğŸ”‘ ä½¿ç”¨å–®APIæ¨¡å¼")
            self.llm = ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=api_keys[0],
                temperature=0.1,
                max_tokens=1024,
                convert_system_message_to_human=True
            )
            self.mode = "single_api"
    
    def invoke(self, prompt: str) -> str:
        """çµ±ä¸€çš„LLMèª¿ç”¨ä»‹é¢"""
        self.total_count += 1
        
        try:
            if self.mode == "multi_api":
                response = self.api_manager.invoke(prompt)
                self.success_count += 1
                return response
            else:
                response = self.llm.invoke(prompt)
                self.success_count += 1
                return response.content if hasattr(response, 'content') else str(response)
        except Exception as e:
            print(f"âš ï¸ LLMèª¿ç”¨å¤±æ•—: {e}")
            raise e
    
    def get_success_rate(self) -> float:
        """ç²å–æˆåŠŸç‡"""
        if self.total_count == 0:
            return 0.0
        return (self.success_count / self.total_count) * 100
    
    def print_stats(self):
        """æ‰“å°çµ±è¨ˆä¿¡æ¯"""
        print(f"ğŸ“Š LLMèª¿ç”¨çµ±è¨ˆ: {self.success_count}/{self.total_count} ({self.get_success_rate():.1f}%)")
        
        if hasattr(self, 'api_manager'):
            self.api_manager.print_usage_statistics()

# =============================================================================
# æ™ºèƒ½å»é‡å™¨
# =============================================================================

class ESGResultDeduplicator:
    """ESGæå–çµæœå»é‡å™¨"""
    
    def __init__(self, similarity_threshold: float = 0.8):
        self.similarity_threshold = similarity_threshold
        self.value_match_threshold = 0.95
    
    def deduplicate_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å»é‡æå–çµæœåˆ—è¡¨"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”„ é–‹å§‹æ™ºèƒ½å»é‡: {len(extractions)} å€‹çµæœ")
        
        # å°‡æå–çµæœè½‰æ›ç‚ºDataFrameé€²è¡Œè™•ç†
        data = []
        for i, extraction in enumerate(extractions):
            data.append({
                'index': i,
                'keyword': extraction.keyword,
                'value': extraction.value,
                'value_type': extraction.value_type,
                'paragraph': extraction.paragraph,
                'page_number': extraction.page_number,
                'confidence': extraction.confidence,
                'context_window': extraction.context_window
            })
        
        df = pd.DataFrame(data)
        
        # è­˜åˆ¥é‡è¤‡çµ„
        groups = self._group_similar_results(df)
        
        if not groups:
            print("âœ… æœªç™¼ç¾é‡è¤‡æ•¸æ“š")
            return extractions
        
        # åŸ·è¡Œå»é‡
        deduplicated_extractions = self._merge_duplicate_groups(extractions, df, groups)
        
        print(f"âœ… æ™ºèƒ½å»é‡å®Œæˆ: {len(extractions)} â†’ {len(deduplicated_extractions)} å€‹çµæœ")
        print(f"   åˆä½µäº† {len(groups)} å€‹é‡è¤‡çµ„")
        
        return deduplicated_extractions
    
    def _group_similar_results(self, df: pd.DataFrame) -> List[List[int]]:
        """è­˜åˆ¥ç›¸ä¼¼çš„æå–çµæœ"""
        groups = []
        processed = set()
        
        for i, row1 in df.iterrows():
            if i in processed:
                continue
            
            current_group = [i]
            value1 = self._normalize_value(row1['value'])
            paragraph1 = str(row1['paragraph'])
            
            for j, row2 in df.iterrows():
                if j <= i or j in processed:
                    continue
                
                value2 = self._normalize_value(row2['value'])
                paragraph2 = str(row2['paragraph'])
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºç›¸ä¼¼çµæœ
                is_similar = False
                
                # æ¢ä»¶1: ç›¸åŒæ•¸å€¼ + ç›¸ä¼¼æ–‡æœ¬
                if value1 == value2 and value1 != "N/A":
                    text_similarity = self._calculate_text_similarity(paragraph1, paragraph2)
                    if text_similarity > self.similarity_threshold:
                        is_similar = True
                
                # æ¢ä»¶2: å®Œå…¨ç›¸åŒçš„æ®µè½æ–‡æœ¬
                if self._calculate_text_similarity(paragraph1, paragraph2) > 0.95:
                    is_similar = True
                
                if is_similar:
                    current_group.append(j)
                    processed.add(j)
            
            if len(current_group) > 1:
                groups.append(current_group)
                for idx in current_group:
                    processed.add(idx)
        
        return groups
    
    def _merge_duplicate_groups(self, extractions: List[NumericExtraction], 
                               df: pd.DataFrame, groups: List[List[int]]) -> List[NumericExtraction]:
        """åˆä½µé‡è¤‡çš„æå–çµæœçµ„"""
        # æ”¶é›†è¢«åˆä½µçš„ç´¢å¼•
        merged_indices = set()
        for group in groups:
            merged_indices.update(group)
        
        # ä¿ç•™æœªè¢«åˆä½µçš„çµæœ
        deduplicated = []
        for i, extraction in enumerate(extractions):
            if i not in merged_indices:
                deduplicated.append(extraction)
        
        # è™•ç†æ¯å€‹é‡è¤‡çµ„
        for group in groups:
            group_extractions = [extractions[i] for i in group]
            merged_extraction = self._merge_extraction_group(group_extractions)
            deduplicated.append(merged_extraction)
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        deduplicated.sort(key=lambda x: x.confidence, reverse=True)
        
        return deduplicated
    
    def _merge_extraction_group(self, group_extractions: List[NumericExtraction]) -> NumericExtraction:
        """åˆä½µä¸€çµ„é‡è¤‡çš„æå–çµæœ"""
        # é¸æ“‡ä¿¡å¿ƒåˆ†æ•¸æœ€é«˜çš„ä½œç‚ºåŸºç¤
        best_extraction = max(group_extractions, key=lambda x: x.confidence)
        
        # åˆä½µé—œéµå­—
        keywords = [e.keyword for e in group_extractions]
        unique_keywords = list(dict.fromkeys(keywords))  # ä¿æŒé †åºå»é‡
        primary_keyword = self._select_primary_keyword(unique_keywords)
        
        # åˆä½µé ç¢¼
        pages = [e.page_number for e in group_extractions if e.page_number]
        unique_pages = list(dict.fromkeys(pages))
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
        avg_confidence = np.mean([e.confidence for e in group_extractions])
        
        # åˆä½µä¸Šä¸‹æ–‡
        contexts = [e.context_window for e in group_extractions if e.context_window]
        merged_context = "\n---åˆä½µçµæœ---\n".join(set(contexts))
        
        # å‰µå»ºåˆä½µå¾Œçš„çµæœ
        merged_extraction = NumericExtraction(
            keyword=primary_keyword,
            value=best_extraction.value,
            value_type=best_extraction.value_type,
            unit=best_extraction.unit,
            paragraph=best_extraction.paragraph,
            paragraph_number=best_extraction.paragraph_number,
            page_number=" | ".join(unique_pages),
            confidence=avg_confidence,
            context_window=f"{merged_context}\n[æ™ºèƒ½åˆä½µäº†{len(group_extractions)}å€‹çµæœ: {', '.join(unique_keywords)}]"
        )
        
        return merged_extraction
    
    def _select_primary_keyword(self, keywords: List[str]) -> str:
        """é¸æ“‡ä¸»è¦é—œéµå­—ï¼ˆå„ªå…ˆä¸­æ–‡ã€ç°¡æ½”ï¼‰"""
        if not keywords:
            return 'N/A'
        
        # å„ªå…ˆç´šï¼šä¸­æ–‡é—œéµå­— > è‹±æ–‡é—œéµå­—
        chinese_keywords = [k for k in keywords if re.search(r'[\u4e00-\u9fff]', k)]
        english_keywords = [k for k in keywords if not re.search(r'[\u4e00-\u9fff]', k)]
        
        if chinese_keywords:
            # é¸æ“‡æœ€çŸ­çš„ä¸­æ–‡é—œéµå­—
            return min(chinese_keywords, key=len)
        else:
            # é¸æ“‡æœ€çŸ­çš„è‹±æ–‡é—œéµå­—
            return min(english_keywords, key=len)
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if pd.isna(text1) or pd.isna(text2):
            return 0.0
        
        text1 = str(text1).strip()
        text2 = str(text2).strip()
        
        if not text1 or not text2:
            return 0.0
        
        return SequenceMatcher(None, text1, text2).ratio()
    
    def _normalize_value(self, value: str) -> str:
        """æ¨™æº–åŒ–æ•¸å€¼æ ¼å¼"""
        if pd.isna(value):
            return "N/A"
        
        value_str = str(value).strip()
        
        if not value_str or value_str.lower() in ['nan', 'none', 'null']:
            return "N/A"
        
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        value_str = re.sub(r'\s+', ' ', value_str)
        
        # æ¨™æº–åŒ–ç™¾åˆ†æ¯”
        if '%' in value_str or 'ï¼…' in value_str:
            numbers = re.findall(r'\d+\.?\d*', value_str)
            if numbers:
                return f"{numbers[0]}%"
        
        # æ¨™æº–åŒ–æ•¸å€¼å¸¶å–®ä½
        number_match = re.search(r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*([^\d\s]+)', value_str)
        if number_match:
            number, unit = number_match.groups()
            return f"{number}{unit}"
        
        return value_str

    def deduplicate_excel_file(self, file_path: str) -> str:
        """å»é‡Excelæ–‡ä»¶çš„å…¬é–‹ä»‹é¢"""
        print(f"ğŸ“Š è™•ç†Excelæ–‡ä»¶å»é‡: {Path(file_path).name}")
        
        try:
            # é€™è£¡å¯ä»¥æ·»åŠ Excelæ–‡ä»¶å»é‡çš„å…·é«”å¯¦ç¾
            # ç›®å‰è¿”å›åŸæ–‡ä»¶è·¯å¾‘
            return file_path
        except Exception as e:
            print(f"âŒ Excelå»é‡å¤±æ•—: {e}")
            return None

# =============================================================================
# ä¸»è¦æå–å™¨é¡
# =============================================================================

class ESGExtractor:
    """å¢å¼·ç‰ˆESGè³‡æ–™æå–å™¨ä¸»é¡"""
    
    def __init__(self, vector_db_path: str = None, enable_llm: bool = True, auto_dedupe: bool = True):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆæå–å™¨
        
        Args:
            vector_db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘
            enable_llm: æ˜¯å¦å•Ÿç”¨LLMå¢å¼·
            auto_dedupe: æ˜¯å¦è‡ªå‹•å»é‡
        """
        self.vector_db_path = vector_db_path or VECTOR_DB_PATH
        self.enable_llm = enable_llm
        self.auto_dedupe = auto_dedupe
        
        # åˆå§‹åŒ–çµ„ä»¶
        if ENHANCED_KEYWORDS_AVAILABLE:
            self.keyword_config = EnhancedKeywordConfig()
            self.matcher = EnhancedMatcher()
            print("âœ… ä½¿ç”¨å¢å¼·é—œéµå­—é…ç½®")
        else:
            self.keyword_config = KeywordConfig()
            self.matcher = self._create_basic_matcher()
            print("âš ï¸ ä½¿ç”¨åŸºæœ¬é—œéµå­—é…ç½®")
        
        self.deduplicator = ESGResultDeduplicator()
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        self._load_vector_database()
        
        # åˆå§‹åŒ–LLMï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¢å¼·ç‰ˆESGæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        if self.auto_dedupe:
            print("âœ… æ™ºèƒ½å»é‡åŠŸèƒ½å·²å•Ÿç”¨")

    def _create_basic_matcher(self):
        """å‰µå»ºåŸºæœ¬åŒ¹é…å™¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        class BasicMatcher:
            def extract_numbers_and_percentages(self, text: str):
                numbers = re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:è¬|åƒ)?(?:å™¸|kg|KG|å…¬æ–¤))', text)
                percentages = re.findall(r'\d+(?:\.\d+)?(?:\s*%|ï¼…)', text)
                return numbers, percentages
        
        return BasicMatcher()

    def _load_vector_database(self):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(self.vector_db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {self.vector_db_path}")
        
        print(f"ğŸ“š è¼‰å…¥å‘é‡è³‡æ–™åº«: {self.vector_db_path}")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        self.db = FAISS.load_local(
            self.vector_db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        print(f"âœ… å‘é‡è³‡æ–™åº«è¼‰å…¥å®Œæˆ")
    
    def _init_llm(self):
        """åˆå§‹åŒ–å¢å¼·LLMç®¡ç†å™¨"""
        try:
            print(f"ğŸ¤– åˆå§‹åŒ–å¢å¼·LLMç®¡ç†å™¨...")
            
            self.llm_manager = EnhancedLLMManager(
                api_keys=GEMINI_API_KEYS,
                model_name=GEMINI_MODEL
            )
            
            print("âœ… å¢å¼·LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def enhanced_stage1_filtering(self, documents: List[Document]) -> Tuple[List[Document], List[ExtractionMatch]]:
        """å¢å¼·çš„ç¬¬ä¸€éšæ®µç¯©é¸ï¼šä½¿ç”¨ç²¾ç¢ºéæ¿¾ç®¡é“"""
        print("ğŸ” åŸ·è¡Œå¢å¼·çš„ç¬¬ä¸€éšæ®µç¯©é¸...")
        
        if not ENHANCED_KEYWORDS_AVAILABLE:
            return self._basic_stage1_filtering(documents)
        
        keywords = self.keyword_config.get_all_keywords()
        passed_docs = []
        all_matches = []
        filtering_stats = {
            'total_docs': len(documents),
            'passed_docs': 0,
            'rejected_docs': 0,
            'total_matches': 0
        }
        
        for doc in tqdm(documents, desc="å¢å¼·ç¬¬ä¸€éšæ®µç¯©é¸"):
            # ä½¿ç”¨å¢å¼·éæ¿¾ç®¡é“
            passed, matches = enhanced_filtering_pipeline(doc.page_content, keywords)
            
            if passed and matches:
                # åªä¿ç•™é«˜ç›¸é—œæ€§çš„åŒ¹é…
                high_relevance_matches = [m for m in matches if m.get('relevance_score', 0) > 0.75]
                
                if high_relevance_matches:
                    passed_docs.append(doc)
                    filtering_stats['passed_docs'] += 1
                    
                    # è½‰æ›ç‚ºExtractionMatchæ ¼å¼
                    for match in high_relevance_matches:
                        extraction_match = ExtractionMatch(
                            keyword=match['keyword'],
                            keyword_type=match['match_type'],
                            confidence=match.get('relevance_score', 0.8),
                            matched_text=match.get('match_details', '')
                        )
                        all_matches.append(extraction_match)
                        filtering_stats['total_matches'] += 1
                else:
                    filtering_stats['rejected_docs'] += 1
            else:
                filtering_stats['rejected_docs'] += 1
        
        print(f"âœ… å¢å¼·ç¬¬ä¸€éšæ®µå®Œæˆ: {len(passed_docs)}/{len(documents)} æ–‡æª”é€šé")
        print(f"   ç²¾ç¢ºéæ¿¾æ•ˆæœ: æ‹’çµ•äº† {filtering_stats['rejected_docs']} å€‹ä¸ç›¸é—œæ–‡æª”")
        print(f"   æ‰¾åˆ°é«˜è³ªé‡åŒ¹é…: {filtering_stats['total_matches']} å€‹")
        
        return passed_docs, all_matches
    
    def _basic_stage1_filtering(self, documents: List[Document]) -> Tuple[List[Document], List[ExtractionMatch]]:
        """åŸºæœ¬çš„ç¬¬ä¸€éšæ®µç¯©é¸ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        print("ğŸ” åŸ·è¡ŒåŸºæœ¬ç¬¬ä¸€éšæ®µç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        passed_docs = []
        all_matches = []
        
        for doc in tqdm(documents, desc="åŸºæœ¬ç¬¬ä¸€éšæ®µç¯©é¸"):
            doc_matches = []
            
            for keyword in keywords:
                if isinstance(keyword, str) and keyword.lower() in doc.page_content.lower():
                    match = ExtractionMatch(
                        keyword=keyword,
                        keyword_type='continuous',
                        confidence=0.8,
                        matched_text=f"åŸºæœ¬åŒ¹é…: {keyword}"
                    )
                    doc_matches.append(match)
            
            if doc_matches:
                passed_docs.append(doc)
                all_matches.extend(doc_matches)
        
        print(f"âœ… åŸºæœ¬ç¬¬ä¸€éšæ®µå®Œæˆ: {len(passed_docs)}/{len(documents)} æ–‡æª”é€šé")
        return passed_docs, all_matches
    
    def stage2_filtering(self, documents: List[Document]) -> List[NumericExtraction]:
        """ç¬¬äºŒéšæ®µç¯©é¸ï¼šæå–åŒ…å«æ•¸å€¼æˆ–ç™¾åˆ†æ¯”çš„å…§å®¹"""
        print("ğŸ”¢ åŸ·è¡Œç¬¬äºŒéšæ®µç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="ç¬¬äºŒéšæ®µç¯©é¸"):
            # åˆ†å‰²æˆæ®µè½
            paragraphs = self._split_into_paragraphs(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 10:
                    continue
                
                # æª¢æŸ¥æ®µè½ä¸­çš„é—œéµå­—
                if ENHANCED_KEYWORDS_AVAILABLE:
                    # ä½¿ç”¨å¢å¼·éæ¿¾æª¢æŸ¥æ®µè½
                    passed, matches = enhanced_filtering_pipeline(paragraph, keywords)
                    if not passed or not matches:
                        continue
                    
                    para_matches = [(m['original_keyword'], m.get('relevance_score', 0.8), m.get('match_details', '')) 
                                   for m in matches]
                else:
                    # åŸºæœ¬é—œéµå­—æª¢æŸ¥
                    para_matches = []
                    for keyword in keywords:
                        if isinstance(keyword, str) and keyword.lower() in paragraph.lower():
                            para_matches.append((keyword, 0.8, f"åŸºæœ¬åŒ¹é…: {keyword}"))
                
                if para_matches:
                    # æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”
                    numbers, percentages = self.matcher.extract_numbers_and_percentages(paragraph)
                    
                    if numbers or percentages:
                        # ç‚ºæ¯å€‹æ‰¾åˆ°çš„æ•¸å€¼å‰µå»ºæå–çµæœ
                        for number in numbers:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=number,
                                    value_type='number',
                                    unit=self._extract_unit(number),
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph)
                                )
                                extractions.append(extraction)
                        
                        # ç‚ºæ¯å€‹æ‰¾åˆ°çš„ç™¾åˆ†æ¯”å‰µå»ºæå–çµæœ
                        for percentage in percentages:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=percentage,
                                    value_type='percentage',
                                    unit='%',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph)
                                )
                                extractions.append(extraction)
        
        print(f"âœ… ç¬¬äºŒéšæ®µå®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹æ•¸å€¼æå–çµæœ")
        return extractions
    
    def llm_enhancement(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """LLMå¢å¼·ï¼šé©—è­‰å’Œè±å¯Œæå–çµæœ"""
        if not self.enable_llm or not extractions:
            return extractions
        
        print("ğŸ¤– åŸ·è¡ŒLLMå¢å¼·é©—è­‰...")
        print(f"ğŸ“Š è™•ç† {len(extractions)} å€‹æå–çµæœ")
        
        enhanced_extractions = []
        
        for i, extraction in enumerate(tqdm(extractions, desc="LLMå¢å¼·")):
            try:
                # æ§‹å»ºæ”¹é€²çš„é©—è­‰æç¤º
                prompt = self._build_enhanced_verification_prompt(extraction)
                
                # èª¿ç”¨LLM
                response_content = self.llm_manager.invoke(prompt)
                llm_result = self._parse_enhanced_llm_response(response_content)
                
                # æ›´æ–°æå–çµæœ
                if llm_result and llm_result.get("is_relevant", True):
                    # æ›´æ–°ä¿¡å¿ƒåˆ†æ•¸
                    llm_confidence = llm_result.get("confidence", extraction.confidence)
                    extraction.confidence = min(
                        (extraction.confidence + llm_confidence) / 2, 
                        1.0
                    )
                    
                    # æ·»åŠ LLMçš„è§£é‡‹
                    extraction.context_window += f"\n[LLMé©—è­‰]: {llm_result.get('explanation', '')}"
                    enhanced_extractions.append(extraction)
                elif llm_result and llm_result.get("confidence", 0) > 0.7:
                    # å³ä½¿ä¸ç›¸é—œä½†ä¿¡å¿ƒåˆ†æ•¸é«˜ï¼Œé™ä½ä¿¡å¿ƒå¾Œä¿ç•™
                    extraction.confidence *= 0.7
                    extraction.context_window += f"\n[LLMæ³¨æ„]: {llm_result.get('explanation', '')}"
                    enhanced_extractions.append(extraction)
                # å…¶ä»–æƒ…æ³ä¸Ÿæ£„
                
            except Exception as e:
                print(f"âš ï¸ LLMå¢å¼·å¤±æ•— (ç¬¬{i+1}å€‹): {e}")
                enhanced_extractions.append(extraction)  # ä¿ç•™åŸå§‹çµæœ
        
        # é¡¯ç¤ºè™•ç†çµ±è¨ˆ
        success_rate = self.llm_manager.get_success_rate()
        retention_rate = (len(enhanced_extractions) / len(extractions)) * 100
        
        print(f"âœ… LLMå¢å¼·å®Œæˆ:")
        print(f"   APIæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   çµæœä¿ç•™ç‡: {retention_rate:.1f}% ({len(enhanced_extractions)}/{len(extractions)})")
        
        # é¡¯ç¤ºAPIä½¿ç”¨çµ±è¨ˆ
        self.llm_manager.print_stats()
        
        return enhanced_extractions
    
    def export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(RESULTS_PATH, f"esg_extraction_results_{timestamp}.xlsx")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºå¢å¼·ç‰ˆçµæœåˆ°Excel: {output_path}")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        for extraction in extractions:
            main_data.append({
                'é—œéµå­—': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'æ®µè½å…§å®¹': extraction.paragraph,
                'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                'é ç¢¼': extraction.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
            })
        
        # æº–å‚™çµ±è¨ˆæ•¸æ“š
        stats_data = []
        for keyword, count in summary.keywords_found.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            numbers = [e for e in keyword_extractions if e.value_type == 'number']
            percentages = [e for e in keyword_extractions if e.value_type == 'percentage']
            
            stats_data.append({
                'é—œéµå­—': keyword,
                'ç¸½æå–æ•¸': len(keyword_extractions),
                'æ•¸å€¼é¡å‹': len(numbers),
                'ç™¾åˆ†æ¯”é¡å‹': len(percentages),
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
            })
        
        # æº–å‚™è™•ç†æ‘˜è¦
        process_summary = [{
            'é …ç›®': 'å¢å¼·ç‰ˆè™•ç†æ‘˜è¦',
            'ç¸½æ–‡æª”æ•¸': summary.total_documents,
            'ç¬¬ä¸€éšæ®µé€šé': summary.stage1_passed,
            'ç¬¬äºŒéšæ®µé€šé': summary.stage2_passed,
            'ç¸½æå–çµæœ': summary.total_extractions,
            'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
            'å¢å¼·é—œéµå­—éæ¿¾': 'å·²å•Ÿç”¨' if ENHANCED_KEYWORDS_AVAILABLE else 'æœªå•Ÿç”¨',
            'è‡ªå‹•å»é‡': 'å·²å•Ÿç”¨' if self.auto_dedupe else 'æœªå•Ÿç”¨',
            'LLMå¢å¼·': 'å·²å•Ÿç”¨' if self.enable_llm else 'æœªå•Ÿç”¨'
        }]
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ä¸»è¦çµæœ
            pd.DataFrame(main_data).to_excel(writer, sheet_name='æå–çµæœ', index=False)
            
            # çµ±è¨ˆæ‘˜è¦
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='é—œéµå­—çµ±è¨ˆ', index=False)
            
            # è™•ç†æ‘˜è¦
            pd.DataFrame(process_summary).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        print(f"âœ… å¢å¼·ç‰ˆExcelæª”æ¡ˆå·²ä¿å­˜")
        return output_path
    
    def run_complete_extraction(self, max_documents: int = 200) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """åŸ·è¡Œå®Œæ•´çš„å¢å¼·ç‰ˆè³‡æ–™æå–æµç¨‹"""
        start_time = datetime.now()
        print("ğŸš€ é–‹å§‹å¢å¼·ç‰ˆESGè³‡æ–™æå–æµç¨‹")
        print("=" * 60)
        
        # 1. ç²å–ç›¸é—œæ–‡æª”
        print("ğŸ“„ æª¢ç´¢ç›¸é—œæ–‡æª”...")
        documents = self._retrieve_relevant_documents(max_documents)
        
        # 2. å¢å¼·çš„ç¬¬ä¸€éšæ®µç¯©é¸
        stage1_docs, stage1_matches = self.enhanced_stage1_filtering(documents)
        
        # 3. ç¬¬äºŒéšæ®µç¯©é¸
        stage2_extractions = self.stage2_filtering(stage1_docs)
        
        # 4. LLMå¢å¼·ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        enhanced_extractions = self.llm_enhancement(stage2_extractions)
        
        # 5. æ™ºèƒ½å»é‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.auto_dedupe:
            print("\nğŸ”„ åŸ·è¡Œæ™ºèƒ½å»é‡...")
            final_extractions = self.deduplicator.deduplicate_extractions(enhanced_extractions)
        else:
            final_extractions = enhanced_extractions
        
        # 6. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in final_extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            total_documents=len(documents),
            stage1_passed=len(stage1_docs),
            stage2_passed=len(stage2_extractions),
            total_extractions=len(final_extractions),
            keywords_found=keywords_found,
            processing_time=processing_time,
            enhanced_filtering_used=ENHANCED_KEYWORDS_AVAILABLE
        )
        
        # 7. åŒ¯å‡ºçµæœ
        excel_path = self.export_to_excel(final_extractions, summary)
        
        # 8. é¡¯ç¤ºæœ€çµ‚æ‘˜è¦
        self._print_enhanced_final_summary(summary, final_extractions)
        
        return final_extractions, summary, excel_path
    
    # =============================================================================
    # è¼”åŠ©æ–¹æ³•
    # =============================================================================
    
    def _retrieve_relevant_documents(self, max_docs: int) -> List[Document]:
        """æª¢ç´¢ç›¸é—œæ–‡æª”"""
        if ENHANCED_KEYWORDS_AVAILABLE:
            config = EnhancedKeywordConfig()
            keywords = (
                config.CORE_RECYCLED_PLASTIC_KEYWORDS["é«˜ç›¸é—œé€£çºŒé—œéµå­—"] +
                config.CORE_RECYCLED_PLASTIC_KEYWORDS["é«˜ç›¸é—œä¸é€£çºŒé—œéµå­—"]
            )
        else:
            keywords = self.keyword_config.get_all_keywords()
        
        all_docs = []
        
        # å°æ¯å€‹é—œéµå­—é€²è¡Œæª¢ç´¢
        for keyword in keywords:
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = self.db.similarity_search(search_term, k=30)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹ç›¸é—œæ–‡æª”")
        return result_docs
    
    def _split_into_paragraphs(self, text: str) -> List[str]:
        """å°‡æ–‡æœ¬åˆ†å‰²æˆæ®µè½"""
        paragraphs = re.split(r'\n{2,}|\r{2,}|ã€‚{2,}|\.{2,}', text)
        
        cleaned_paragraphs = []
        for para in paragraphs:
            para = para.strip()
            if len(para) >= 10:
                cleaned_paragraphs.append(para)
        
        return cleaned_paragraphs
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 100) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:200]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:200]
    
    def _build_enhanced_verification_prompt(self, extraction: NumericExtraction) -> str:
        """æ§‹å»ºå¢å¼·çš„LLMé©—è­‰æç¤º"""
        return f"""è«‹åˆ†æä»¥ä¸‹æ•¸æ“šæå–çµæœæ˜¯å¦èˆ‡å†ç”Ÿå¡‘è† /å›æ”¶å¡‘æ–™çš„å¯¦éš›ç”Ÿç”¢ä½¿ç”¨ç›¸é—œï¼š

é—œéµå­—: {extraction.keyword}
æå–å€¼: {extraction.value}
æ•¸æ“šé¡å‹: {extraction.value_type}

æ®µè½å…§å®¹: {extraction.paragraph[:300]}

åˆ¤æ–·æ¨™æº–:
1. æ˜¯å¦èˆ‡å†ç”Ÿå¡‘è† ã€å›æ”¶å¡‘æ–™ã€PCRææ–™çš„å¯¦éš›ç”Ÿç”¢æˆ–ä½¿ç”¨ç›¸é—œï¼Ÿ
2. æ˜¯å¦æ’é™¤äº†è³½äº‹æ´»å‹•ã€è·æ¥­ç½å®³ã€æ°´è³‡æºç®¡ç†ç­‰ç„¡é—œä¸»é¡Œï¼Ÿ
3. æ•¸å€¼æ˜¯å¦ç¢ºå¯¦æè¿°å†ç”Ÿææ–™çš„ç”¢èƒ½ã€ç”¢é‡ã€ä½¿ç”¨é‡æˆ–æ¯”ä¾‹ï¼Ÿ

è«‹åš´æ ¼æŒ‰ç…§JSONæ ¼å¼å›ç­”ï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{"is_relevant": true, "confidence": 0.85, "explanation": "ç°¡çŸ­èªªæ˜ç›¸é—œæ€§"}}"""
    
    def _parse_enhanced_llm_response(self, response_text: str) -> Optional[Dict]:
        """è§£æå¢å¼·çš„LLMéŸ¿æ‡‰"""
        if not response_text:
            return None
        
        try:
            # æ–¹æ³•1: ç›´æ¥è§£æJSON
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                if 'is_relevant' in result:
                    return {
                        'is_relevant': bool(result.get('is_relevant', False)),
                        'confidence': float(result.get('confidence', 0.5)),
                        'explanation': str(result.get('explanation', 'ç„¡èªªæ˜'))
                    }
        except:
            pass
        
        # æ–¹æ³•2: é—œéµå­—è§£æ
        try:
            response_lower = response_text.lower()
            
            # åˆ¤æ–·ç›¸é—œæ€§
            is_relevant = False
            if any(word in response_lower for word in ['true', 'ç›¸é—œ', 'æ˜¯', 'relevant']):
                is_relevant = True
            
            # æå–ä¿¡å¿ƒåˆ†æ•¸
            confidence = 0.5
            confidence_match = re.search(r'(?:confidence|ä¿¡å¿ƒ).*?(\d+\.?\d*)', response_lower)
            if confidence_match:
                confidence = min(float(confidence_match.group(1)), 1.0)
                if confidence > 1:
                    confidence = confidence / 100
            
            return {
                'is_relevant': is_relevant,
                'confidence': confidence,
                'explanation': response_text[:100] + "..." if len(response_text) > 100 else response_text
            }
            
        except:
            pass
        
        # æ–¹æ³•3: ä¿å®ˆé»˜èª
        return {
            'is_relevant': False,
            'confidence': 0.3,
            'explanation': f"éŸ¿æ‡‰è§£æå¤±æ•—: {response_text[:50]}..."
        }
    
    def _print_enhanced_final_summary(self, summary: ProcessingSummary, extractions: List[NumericExtraction]):
        """æ‰“å°å¢å¼·ç‰ˆæœ€çµ‚æ‘˜è¦"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ å¢å¼·ç‰ˆæå–å®Œæˆæ‘˜è¦")
        print("=" * 70)
        print(f"ğŸ“š è™•ç†æ–‡æª”æ•¸: {summary.total_documents}")
        print(f"ğŸ” ç¬¬ä¸€éšæ®µé€šé: {summary.stage1_passed}")
        print(f"ğŸ”¢ ç¬¬äºŒéšæ®µé€šé: {summary.stage2_passed}")
        print(f"ğŸ“Š ç¸½æå–çµæœ: {summary.total_extractions}")
        print(f"â±ï¸ è™•ç†æ™‚é–“: {summary.processing_time:.2f} ç§’")
        print(f"ğŸ¯ å¢å¼·é—œéµå­—éæ¿¾: {'å·²å•Ÿç”¨' if summary.enhanced_filtering_used else 'æœªå•Ÿç”¨'}")
        print(f"ğŸ§¹ æ™ºèƒ½å»é‡: {'å·²å•Ÿç”¨' if self.auto_dedupe else 'æœªå•Ÿç”¨'}")
        print(f"ğŸ¤– LLMå¢å¼·: {'å·²å•Ÿç”¨' if self.enable_llm else 'æœªå•Ÿç”¨'}")
        
        print(f"\nğŸ“ˆ é—œéµå­—åˆ†å¸ƒ:")
        for keyword, count in summary.keywords_found.items():
            print(f"   {keyword}: {count} å€‹çµæœ")
        
        if extractions:
            numbers = [e for e in extractions if e.value_type == 'number']
            percentages = [e for e in extractions if e.value_type == 'percentage']
            
            print(f"\nğŸ”¢ æ•¸æ“šé¡å‹åˆ†å¸ƒ:")
            print(f"   æ•¸å€¼: {len(numbers)} å€‹")
            print(f"   ç™¾åˆ†æ¯”: {len(percentages)} å€‹")
            
            avg_confidence = np.mean([e.confidence for e in extractions])
            print(f"ğŸ“Š å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {avg_confidence:.3f}")
            
            # é¡¯ç¤ºLLMçµ±è¨ˆ
            if self.enable_llm and hasattr(self, 'llm_manager'):
                print(f"ğŸ¤– LLMè™•ç†æˆåŠŸç‡: {self.llm_manager.get_success_rate():.1f}%")

def main():
    """ä¸»å‡½æ•¸ - ç¨ç«‹é‹è¡Œæ¸¬è©¦"""
    try:
        print("ğŸš€ å¢å¼·ç‰ˆESGè³‡æ–™æå–å™¨ - ç¨ç«‹æ¸¬è©¦æ¨¡å¼")
        print("=" * 60)
        
        # åˆå§‹åŒ–æå–å™¨ï¼ˆå•Ÿç”¨æ‰€æœ‰å¢å¼·åŠŸèƒ½ï¼‰
        extractor = ESGExtractor(enable_llm=True, auto_dedupe=True)
        
        # åŸ·è¡Œå®Œæ•´æå–
        extractions, summary, excel_path = extractor.run_complete_extraction()
        
        if extractions:
            print(f"\nğŸ‰ å¢å¼·ç‰ˆæå–å®Œæˆï¼")
            print(f"ğŸ“ çµæœå·²ä¿å­˜è‡³: {excel_path}")
            
            # é¡¯ç¤ºå‰å¹¾å€‹çµæœä½œç‚ºæ¨£ä¾‹
            print(f"\nğŸ“‹ æ¨£ä¾‹çµæœ (å‰3å€‹):")
            for i, extraction in enumerate(extractions[:3], 1):
                print(f"\n{i}. é—œéµå­—: {extraction.keyword}")
                print(f"   æ•¸å€¼: {extraction.value}")
                print(f"   é¡å‹: {extraction.value_type}")
                print(f"   é ç¢¼: {extraction.page_number}")
                print(f"   ä¿¡å¿ƒ: {extraction.confidence:.2f}")
                print(f"   æ®µè½: {extraction.paragraph[:100]}...")
        
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æå–çµæœ")
    
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()