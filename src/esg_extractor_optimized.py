#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ v2.1
æ”¯æ´å¤šæ–‡ä»¶è™•ç†ã€æ”¹é€²éæ¿¾é‚è¼¯ã€æ·»åŠ å…¬å¸ä¿¡æ¯
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np
from difflib import SequenceMatcher

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# é—œéµå­—é…ç½®é¡ï¼ˆæ”¹é€²ç‰ˆï¼‰
# =============================================================================

class KeywordConfig:
    """é—œéµå­—é…ç½®ç®¡ç†é¡"""
    
    # æ“´å±•çš„é—œéµå­—é…ç½®ï¼Œå¢åŠ æ›´å¤šç›¸é—œè©å½™
    CORE_KEYWORDS = {
        "å†ç”Ÿå¡‘è† ææ–™": {
            "continuous": [
                "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”Ÿpp", "å†ç”ŸPET",
                "å›æ”¶å¡‘è† ", "å›æ”¶å¡‘æ–™", "å›æ”¶PP", "å›æ”¶PET",
                "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™", 
                "å¯¶ç‰¹ç“¶å›æ”¶", "å»¢å¡‘è† ", "ç’°ä¿å¡‘è† ",
                "å¾ªç’°å¡‘è† ", "å¯å›æ”¶å¡‘è† "
            ],
            "discontinuous": [
                ("å†ç”Ÿ", "å¡‘è† "), ("å†ç”Ÿ", "å¡‘æ–™"), ("å†ç”Ÿ", "PP"), ("å†ç”Ÿ", "PET"),
                ("PP", "å›æ”¶"), ("PP", "å†ç”Ÿ"), ("PP", "æ£§æ¿", "å›æ”¶"),
                ("PET", "å›æ”¶"), ("PET", "å†ç”Ÿ"),
                ("å¡‘è† ", "å›æ”¶"), ("å¡‘æ–™", "å›æ”¶"), ("å¡‘è† ", "å¾ªç’°"),
                ("PCR", "å¡‘è† "), ("PCR", "å¡‘æ–™"), ("PCR", "ææ–™"),
                ("å›æ”¶", "å¡‘è† "), ("å›æ”¶", "å¡‘æ–™"), ("å›æ”¶", "ææ–™"),
                ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å¯¶ç‰¹ç“¶", "å†é€ "), ("å¯¶ç‰¹ç“¶", "å¾ªç’°"),
                ("å»¢æ£„", "å¡‘è† "), ("å»¢æ£„", "å¡‘æ–™"),
                ("rPET", "å«é‡"), ("å†ç”Ÿ", "ææ–™"), ("MLCC", "å›æ”¶"),
                ("å›æ”¶", "ç”¢èƒ½"), ("å¾ªç’°", "ç¶“æ¿Ÿ"), ("ç’°ä¿", "ææ–™"),
                ("å›æ”¶", "é€ ç²’"), ("å»¢æ–™", "å›æ”¶")
            ]
        }
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—ï¼ˆé€£çºŒ+ä¸é€£çºŒï¼‰"""
        all_keywords = []
        for category in cls.CORE_KEYWORDS.values():
            all_keywords.extend(category["continuous"])
            all_keywords.extend(category["discontinuous"])
        return all_keywords

# =============================================================================
# å¢å¼·åŒ¹é…å¼•æ“ï¼ˆæ”¹é€²ç‰ˆï¼‰
# =============================================================================

class EnhancedMatcher:
    """å¢å¼·çš„é—œéµå­—åŒ¹é…å¼•æ“"""
    
    def __init__(self, max_distance: int = 200):  # å¢åŠ æœ€å¤§è·é›¢
        self.max_distance = max_distance
        
        # æ“´å±•çš„æ•¸å€¼åŒ¹é…æ¨¡å¼
        self.number_patterns = [
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:å„„|è¬|åƒ)?(?:æ”¯|å€‹|ä»¶|æ‰¹|å°|å¥—|æ¬¡|å€))',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:kg|KG|å…¬æ–¤|å™¸|å…‹|g|G|å…¬å…‹|è¬å™¸|åƒå™¸))',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*å™¸/æœˆ|å™¸/å¹´|å™¸/æ—¥)',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:è¬|åƒ)?(?:å™¸|å…¬æ–¤|kg|g))',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*ç«‹æ–¹ç±³|mÂ³)',
            r'\d+(?:,\d{3})*(?:\.\d+)?(?:\s*å„„æ”¯)',  # æ–°å¢ï¼šå„„æ”¯ï¼ˆå¦‚å¯¶ç‰¹ç“¶ï¼‰
        ]
        
        # ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?(?:\s*%|ï¼…|ç™¾åˆ†æ¯”)',
            r'\d+(?:\.\d+)?(?:\s*æˆ)',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
        ]
    
    def match_keyword(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """æ”¹é€²çš„é—œéµå­—åŒ¹é…ï¼Œé™ä½é–€æª»ä½†æé«˜ç²¾ç¢ºåº¦"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            # é€£çºŒé—œéµå­—åŒ¹é…
            if keyword.lower() in text_lower:
                pos = text_lower.find(keyword.lower())
                start = max(0, pos - 30)
                end = min(len(text), pos + len(keyword) + 30)
                context = text[start:end]
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {context}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            # ä¸é€£çºŒé—œéµå­—åŒ¹é… - æ”¹é€²é‚è¼¯
            components = [comp.lower() for comp in keyword]
            positions = []
            
            # æ‰¾åˆ°æ¯å€‹çµ„ä»¶çš„ä½ç½®
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            # è¨ˆç®—è·é›¢å’Œä¿¡å¿ƒåˆ†æ•¸
            min_pos = min(positions)
            max_pos = max(positions)
            distance = max_pos - min_pos
            
            # æä¾›åŒ¹é…ä¸Šä¸‹æ–‡
            start = max(0, min_pos - 50)
            end = min(len(text), max_pos + 50)
            context = text[start:end]
            
            # èª¿æ•´è·é›¢åˆ¤æ–·æ¨™æº–
            if distance <= 50:
                return True, 0.95, f"è¿‘è·é›¢åŒ¹é…({distance}å­—): {context}"
            elif distance <= 120:
                return True, 0.85, f"ä¸­è·é›¢åŒ¹é…({distance}å­—): {context}"
            elif distance <= self.max_distance:
                return True, 0.7, f"é è·é›¢åŒ¹é…({distance}å­—): {context}"
            else:
                return True, 0.5, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—): {context}"
        
        return False, 0.0, ""
    
    def extract_numbers_and_percentages(self, text: str) -> Tuple[List[str], List[str]]:
        """æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”"""
        numbers = []
        percentages = []
        
        # æå–æ•¸å€¼
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        # æå–ç™¾åˆ†æ¯”
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            percentages.extend(matches)
        
        # å»é‡ä¸¦æ’åº
        numbers = list(set(numbers))
        percentages = list(set(percentages))
        
        return numbers, percentages

# =============================================================================
# å¤šæ–‡ä»¶ESGæå–å™¨
# =============================================================================

class MultiFileESGExtractor:
    """æ”¯æ´å¤šæ–‡ä»¶è™•ç†çš„ESGæå–å™¨"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = EnhancedMatcher()
        self.keyword_config = KeywordConfig()
        
        # åˆå§‹åŒ–LLMï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¤šæ–‡ä»¶ESGæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 300) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """è™•ç†å–®å€‹æ–‡æª”"""
        start_time = datetime.now()
        print(f"\nğŸš€ è™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. ç²å–ç›¸é—œæ–‡æª”
        documents = self._retrieve_relevant_documents(db, max_documents)
        
        # 3. æ”¹é€²çš„å…©éšæ®µç¯©é¸
        stage2_extractions = self._improved_two_stage_filtering(documents, doc_info)
        
        # 4. LLMå¢å¼·ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        enhanced_extractions = self._llm_enhancement(stage2_extractions)
        
        # 5. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in enhanced_extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            total_documents=len(documents),
            stage1_passed=len(documents),  # ç°¡åŒ–çµ±è¨ˆ
            stage2_passed=len(enhanced_extractions),
            total_extractions=len(enhanced_extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 6. åŒ¯å‡ºçµæœ
        excel_path = self._export_to_excel(enhanced_extractions, summary, doc_info)
        
        return enhanced_extractions, summary, excel_path
    
    def process_multiple_documents(self, docs_info: Dict[str, DocumentInfo], max_documents: int = 300) -> Dict[str, Tuple]:
        """æ‰¹é‡è™•ç†å¤šå€‹æ–‡æª”"""
        print(f"ğŸš€ é–‹å§‹æ‰¹é‡è™•ç† {len(docs_info)} å€‹æ–‡æª”")
        print("=" * 60)
        
        results = {}
        
        for pdf_path, doc_info in docs_info.items():
            try:
                print(f"\nğŸ“„ è™•ç†: {doc_info.company_name} - {doc_info.report_year}")
                
                extractions, summary, excel_path = self.process_single_document(doc_info, max_documents)
                
                results[pdf_path] = (extractions, summary, excel_path)
                
                print(f"âœ… å®Œæˆ: {Path(excel_path).name}")
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•— {doc_info.company_name}: {e}")
                continue
        
        print(f"\nğŸ‰ æ‰¹é‡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(docs_info)} å€‹æ–‡æª”")
        return results
    
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _retrieve_relevant_documents(self, db, max_docs: int) -> List[Document]:
        """æª¢ç´¢ç›¸é—œæ–‡æª”"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # å°æ¯å€‹é—œéµå­—é€²è¡Œæª¢ç´¢
        for keyword in keywords:
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = db.similarity_search(search_term, k=30)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        return result_docs
    
    def _improved_two_stage_filtering(self, documents: List[Document], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """æ”¹é€²çš„å…©éšæ®µç¯©é¸ï¼Œé¿å…éåº¦éæ¿¾"""
        print("ğŸ” åŸ·è¡Œæ”¹é€²çš„å…©éšæ®µç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="æ”¹é€²ç¯©é¸"):
            # æ”¹é€²ï¼šé™ä½æ®µè½é•·åº¦è¦æ±‚ï¼Œåˆ†å‰²æ›´ç´°
            paragraphs = self._split_into_paragraphs(doc.page_content, min_length=5)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 5:  # é™ä½æœ€å°é•·åº¦è¦æ±‚
                    continue
                
                # ç¬¬ä¸€éšæ®µï¼šé—œéµå­—åŒ¹é…
                para_matches = []
                for keyword in keywords:
                    is_match, confidence, details = self.matcher.match_keyword(paragraph, keyword)
                    if is_match:
                        para_matches.append((keyword, confidence, details))
                
                if para_matches:
                    # ç¬¬äºŒéšæ®µï¼šæ•¸å€¼æå–ï¼ˆæ”¹é€²é‚è¼¯ï¼‰
                    numbers, percentages = self.matcher.extract_numbers_and_percentages(paragraph)
                    
                    # æ”¹é€²ï¼šå³ä½¿æ²’æœ‰æ˜ç¢ºæ•¸å€¼ï¼Œä¹Ÿä¿ç•™åŒ…å«é‡è¦é—œéµè©çš„æ®µè½
                    has_important_content = self._check_important_content(paragraph)
                    
                    if numbers or percentages or has_important_content:
                        # ç‚ºæ¯å€‹æ‰¾åˆ°çš„æ•¸å€¼å‰µå»ºæå–çµæœ
                        for number in numbers:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=number,
                                    value_type='number',
                                    unit=self._extract_unit(number),
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph),
                                    company_name=doc_info.company_name,
                                    report_year=doc_info.report_year
                                )
                                extractions.append(extraction)
                        
                        # ç‚ºæ¯å€‹æ‰¾åˆ°çš„ç™¾åˆ†æ¯”å‰µå»ºæå–çµæœ
                        for percentage in percentages:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value=percentage,
                                    value_type='percentage',
                                    unit='%',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context_window=self._get_context_window(doc.page_content, paragraph),
                                    company_name=doc_info.company_name,
                                    report_year=doc_info.report_year
                                )
                                extractions.append(extraction)
                        
                        # å¦‚æœæ²’æœ‰å…·é«”æ•¸å€¼ä½†æœ‰é‡è¦å…§å®¹ï¼Œå‰µå»ºæè¿°æ€§çµæœ
                        if not numbers and not percentages and has_important_content:
                            for keyword, confidence, details in para_matches:
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value="[é‡è¦æè¿°]",
                                    value_type='description',
                                    unit='',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence * 0.8,  # ç¨å¾®é™ä½ä¿¡å¿ƒåˆ†æ•¸
                                    context_window=self._get_context_window(doc.page_content, paragraph),
                                    company_name=doc_info.company_name,
                                    report_year=doc_info.report_year
                                )
                                extractions.append(extraction)
        
        print(f"âœ… æ”¹é€²ç¯©é¸å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹æå–çµæœ")
        return extractions
    
    def _check_important_content(self, paragraph: str) -> bool:
        """æª¢æŸ¥æ®µè½æ˜¯å¦åŒ…å«é‡è¦å…§å®¹ï¼ˆå³ä½¿æ²’æœ‰å…·é«”æ•¸å€¼ï¼‰"""
        important_indicators = [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "æ¸›ç¢³", "æ¸›æ’", "æ•ˆç›Š", "æˆæœ",
            "ç›®æ¨™", "ç­–ç•¥", "æŠ€è¡“", "æ‡‰ç”¨", "é–‹ç™¼", "ç”Ÿç”¢",
            "æ¨å‹•", "å¯¦æ–½", "å»ºç½®", "å»ºç«‹", "ç™¼å±•"
        ]
        
        paragraph_lower = paragraph.lower()
        matched_indicators = sum(1 for indicator in important_indicators if indicator in paragraph_lower)
        
        # å¦‚æœåŒ…å«2å€‹ä»¥ä¸Šé‡è¦æŒ‡æ¨™è©ï¼Œä¸”æ®µè½é•·åº¦åˆç†ï¼Œå°±èªç‚ºæ˜¯é‡è¦å…§å®¹
        return matched_indicators >= 2 and len(paragraph) >= 30
    
    def _llm_enhancement(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """LLMå¢å¼·ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        if not self.enable_llm or not extractions:
            return extractions
        
        print(f"ğŸ¤– åŸ·è¡ŒLLMå¢å¼· ({len(extractions)} å€‹çµæœ)...")
        
        # ç‚ºäº†ä¿æŒç°¡æ½”ï¼Œé€™è£¡åƒ…å°ä¿¡å¿ƒåˆ†æ•¸ä½æ–¼0.7çš„çµæœé€²è¡ŒLLMé©—è­‰
        enhanced_extractions = []
        
        for extraction in tqdm(extractions, desc="LLMå¢å¼·"):
            if extraction.confidence >= 0.7:
                enhanced_extractions.append(extraction)
            else:
                # å°ä½ä¿¡å¿ƒçµæœé€²è¡ŒLLMé©—è­‰
                try:
                    enhanced_extraction = self._llm_verify_extraction(extraction)
                    enhanced_extractions.append(enhanced_extraction)
                except:
                    enhanced_extractions.append(extraction)  # å¤±æ•—æ™‚ä¿ç•™åŸå§‹çµæœ
        
        return enhanced_extractions
    
    def _llm_verify_extraction(self, extraction: NumericExtraction) -> NumericExtraction:
        """LLMé©—è­‰å–®å€‹æå–çµæœ"""
        prompt = f"""
è«‹é©—è­‰ä»¥ä¸‹æ•¸æ“šæå–æ˜¯å¦åˆç†ï¼š

é—œéµå­—: {extraction.keyword}
æå–å€¼: {extraction.value}
æ®µè½: {extraction.paragraph[:200]}...

é€™å€‹æå–æ˜¯å¦èˆ‡å†ç”Ÿå¡‘è† /å¾ªç’°ç¶“æ¿Ÿç›¸é—œï¼Ÿè«‹å›ç­” "ç›¸é—œ" æˆ– "ä¸ç›¸é—œ"ï¼Œä¸¦çµ¦å‡º1-10çš„ä¿¡å¿ƒåˆ†æ•¸ã€‚
æ ¼å¼ï¼š[ç›¸é—œ/ä¸ç›¸é—œ] [åˆ†æ•¸]
"""
        
        try:
            response = self.api_manager.invoke(prompt)
            
            # ç°¡å–®è§£æå›æ‡‰
            if "ç›¸é—œ" in response and extraction.confidence < 0.9:
                extraction.confidence = min(extraction.confidence + 0.1, 0.9)
            elif "ä¸ç›¸é—œ" in response:
                extraction.confidence = max(extraction.confidence - 0.2, 0.3)
                
        except:
            pass  # LLMå¤±æ•—æ™‚ä¸ä¿®æ”¹
        
        return extraction
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excelï¼ŒåŒ…å«å…¬å¸ä¿¡æ¯"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        company_safe = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
        
        output_filename = f"ESGæå–çµæœ_{company_safe}_{doc_info.report_year}_{timestamp}.xlsx"
        output_path = os.path.join(RESULTS_PATH, output_filename)
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºçµæœåˆ°Excel: {output_filename}")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        
        # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯
        header_row = {
            'é—œéµå­—': f"å…¬å¸: {doc_info.company_name}",
            'æå–æ•¸å€¼': f"å ±å‘Šå¹´åº¦: {doc_info.report_year}",
            'æ•¸æ“šé¡å‹': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            'å–®ä½': '',
            'æ®µè½å…§å®¹': f"ç¸½æå–çµæœ: {len(extractions)} é …",
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': '',
            'ä¸Šä¸‹æ–‡': f"æå–å™¨ç‰ˆæœ¬: v2.1"
        }
        main_data.append(header_row)
        
        # ç©ºè¡Œåˆ†éš”
        main_data.append({col: '' for col in header_row.keys()})
        
        # æå–çµæœ
        for extraction in extractions:
            main_data.append({
                'é—œéµå­—': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'æ®µè½å…§å®¹': extraction.paragraph,
                'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                'é ç¢¼': extraction.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
            })
        
        # æº–å‚™çµ±è¨ˆæ•¸æ“š
        stats_data = []
        for keyword, count in summary.keywords_found.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            numbers = [e for e in keyword_extractions if e.value_type == 'number']
            percentages = [e for e in keyword_extractions if e.value_type == 'percentage']
            descriptions = [e for e in keyword_extractions if e.value_type == 'description']
            
            stats_data.append({
                'é—œéµå­—': keyword,
                'ç¸½æå–æ•¸': len(keyword_extractions),
                'æ•¸å€¼é¡å‹': len(numbers),
                'ç™¾åˆ†æ¯”é¡å‹': len(percentages),
                'æè¿°é¡å‹': len(descriptions),
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ä¸»è¦çµæœ
            pd.DataFrame(main_data).to_excel(writer, sheet_name='æå–çµæœ', index=False)
            
            # çµ±è¨ˆæ‘˜è¦
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='é—œéµå­—çµ±è¨ˆ', index=False)
            
            # è™•ç†æ‘˜è¦
            summary_data = [{
                'å…¬å¸åç¨±': summary.company_name,
                'å ±å‘Šå¹´åº¦': summary.report_year,
                'ç¸½æ–‡æª”æ•¸': summary.total_documents,
                'ç¸½æå–çµæœ': summary.total_extractions,
                'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
                'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }]
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜")
        return output_path
    
    # =============================================================================
    # è¼”åŠ©æ–¹æ³•
    # =============================================================================
    
    def _split_into_paragraphs(self, text: str, min_length: int = 5) -> List[str]:
        """å°‡æ–‡æœ¬åˆ†å‰²æˆæ®µè½"""
        paragraphs = re.split(r'\n{2,}|\r{2,}|ã€‚{2,}|\.{2,}', text)
        
        cleaned_paragraphs = []
        for para in paragraphs:
            para = para.strip()
            if len(para) >= min_length:
                cleaned_paragraphs.append(para)
        
        return cleaned_paragraphs
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 100) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:200]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:200]

def main():
    """ä¸»å‡½æ•¸ - ç¨ç«‹é‹è¡Œæ¸¬è©¦"""
    try:
        print("ğŸš€ å¤šæ–‡ä»¶ESGè³‡æ–™æå–å™¨ - æ¸¬è©¦æ¨¡å¼")
        print("=" * 50)
        
        # æ¨¡æ“¬æ–‡æª”ä¿¡æ¯ï¼ˆå¯¦éš›ä½¿ç”¨æ™‚ç”±é è™•ç†å™¨æä¾›ï¼‰
        sample_docs = {
            "å—äº_2023.pdf": DocumentInfo(
                company_name="å—äºå¡‘è† ",
                report_year="2023",
                pdf_name="å—äº_2023",
                db_path="./vector_db/esg_db_å—äº_2023"
            )
        }
        
        # åˆå§‹åŒ–æå–å™¨
        extractor = MultiFileESGExtractor(enable_llm=True)
        
        # è™•ç†æ–‡æª”
        results = extractor.process_multiple_documents(sample_docs)
        
        if results:
            print(f"\nğŸ‰ è™•ç†å®Œæˆï¼")
            for pdf_path, (extractions, summary, excel_path) in results.items():
                print(f"ğŸ“ {summary.company_name} - {summary.report_year}: {len(extractions)} å€‹çµæœ")
                print(f"   æ–‡ä»¶: {Path(excel_path).name}")
        
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()