#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ - å„ªåŒ–ç‰ˆ v3.0
ç²¾ç¢ºæå–å†ç”Ÿå¡‘è† ç›¸é—œæ•¸æ“šï¼Œæ¸›å°‘LLMèª¿ç”¨æ¬¡æ•¸
"""

import os
import re
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass
from datetime import datetime
from tqdm import tqdm
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

from config import *

@dataclass
class ExtractionResult:
    """æå–çµæœ"""
    keyword: str
    value: str
    value_type: str
    unit: str
    paragraph: str
    page_number: str
    confidence: float
    context: str

class PreciseKeywordMatcher:
    """ç²¾ç¢ºé—œéµå­—åŒ¹é…å™¨"""
    
    def __init__(self):
        # æ ¸å¿ƒå†ç”Ÿå¡‘è† é—œéµå­—ï¼ˆæ›´ç²¾ç¢ºï¼‰
        self.core_keywords = {
            "å†ç”Ÿå¡‘è† ": ["å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™"],
            "å†ç”Ÿèšé…¯": ["å†ç”Ÿèšé…¯", "å›æ”¶èšé…¯", "rPET", "å†ç”ŸPET"],
            "PPå›æ”¶": ["PPå›æ”¶", "PPæ£§æ¿å›æ”¶", "å›æ”¶PP", "å†ç”ŸPP"],
            "MLCCå›æ”¶": ["MLCCå›æ”¶", "é›¢å‹è†œå›æ”¶"],
            "å¯¶ç‰¹ç“¶å›æ”¶": ["å¯¶ç‰¹ç“¶å›æ”¶", "å›æ”¶å¯¶ç‰¹ç“¶"],
            "ç¹”ç‰©å›æ”¶": ["ç¹”ç‰©å›æ”¶", "çº–ç¶­å›æ”¶", "æˆè¡£å›æ”¶"],
            "å†ç”Ÿæ–™": ["å†ç”Ÿæ–™", "å›æ”¶æ–™", "PCRææ–™"]
        }
        
        # æ’é™¤é—œéµå­—ï¼ˆç”¨æ–¼éæ¿¾ä¸ç›¸é—œå…§å®¹ï¼‰
        self.exclude_keywords = [
            "è·æ¥­ç½å®³", "ç’°ä¿ç½°å–®", "åœ°ä¸‹æ°´", "ç›£æ¸¬", "åŸ·è¡Œæ¬¡æ•¸", 
            "æ”¹å–„æ¡ˆ", "ç¯€èƒ½", "ç¯€æ°´", "å»¢æ°´", "é›¨æ°´", "ç”¨é›»é‡",
            "ç”¨æ±½é‡", "ç‡ƒæ–™", "æº«å®¤æ°£é«”", "CO2", "ç¢³æ’", "ç½°å–®",
            "é•å", "ç¨½æŸ¥", "æª¢æŸ¥", "åˆè¦", "æ³•è¦", "æ¨™æº–"
        ]
        
        # æ•¸å€¼æ¨¡å¼ï¼ˆæ›´ç²¾ç¢ºï¼‰
        self.number_patterns = [
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬)?(?:å™¸|å…¬æ–¤|kg|g)\b',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æœˆ\b',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬)?(?:ä»¶|å€‹|æ”¯|æ‰¹)\b',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å„„æ”¯\b'
        ]
        
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%\b',
            r'\d+(?:\.\d+)?\s*ï¼…\b',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?\b'
        ]

    def is_relevant_context(self, text: str) -> bool:
        """æª¢æŸ¥æ–‡æœ¬æ˜¯å¦ç‚ºç›¸é—œä¸Šä¸‹æ–‡"""
        text_lower = text.lower()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ’é™¤é—œéµå­—
        for exclude_word in self.exclude_keywords:
            if exclude_word in text_lower:
                return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¡‘è† /ææ–™ç›¸é—œè©å½™
        material_keywords = [
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "ææ–™", 
            "å¯¶ç‰¹ç“¶", "å›æ”¶", "å†ç”Ÿ", "ç’°ä¿", "å¾ªç’°"
        ]
        
        has_material = any(word in text_lower for word in material_keywords)
        return has_material

    def match_keywords(self, text: str) -> List[Tuple[str, float]]:
        """åŒ¹é…é—œéµå­—"""
        matches = []
        text_lower = text.lower()
        
        # å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºç›¸é—œä¸Šä¸‹æ–‡
        if not self.is_relevant_context(text):
            return matches
        
        for category, keywords in self.core_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    # è¨ˆç®—åŒ¹é…ä½ç½®çš„ä¸Šä¸‹æ–‡ç›¸é—œæ€§
                    pos = text_lower.find(keyword.lower())
                    context_window = text_lower[max(0, pos-50):pos+len(keyword)+50]
                    
                    # æª¢æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦çœŸçš„èˆ‡å¡‘è† å›æ”¶ç›¸é—œ
                    if self._is_plastic_recycling_context(context_window):
                        confidence = self._calculate_confidence(keyword, context_window)
                        matches.append((category, confidence))
        
        return matches

    def _is_plastic_recycling_context(self, context: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºå¡‘è† å›æ”¶ç›¸é—œä¸Šä¸‹æ–‡"""
        positive_indicators = [
            "å›æ”¶", "å†ç”Ÿ", "ç’°ä¿", "å¾ªç’°", "æ¸›ç¢³", "ç”¢èƒ½", "ç”¢é‡",
            "è£½é€ ", "ç”Ÿç”¢", "æ‡‰ç”¨", "ææ–™", "è£½å“", "ç”¢å“"
        ]
        
        negative_indicators = [
            "ç½å®³", "ç½°å–®", "ç›£æ¸¬", "æª¢æŸ¥", "ç¨½æŸ¥", "ç”¨é›»", "ç”¨æ°´",
            "ç‡ƒæ–™", "å»¢æ°´", "é›¨æ°´", "æ³•è¦", "åˆè¦", "æ¨™æº–"
        ]
        
        positive_score = sum(1 for word in positive_indicators if word in context)
        negative_score = sum(1 for word in negative_indicators if word in context)
        
        return positive_score > negative_score and positive_score >= 1

    def _calculate_confidence(self, keyword: str, context: str) -> float:
        """è¨ˆç®—åŒ¹é…ä¿¡å¿ƒåˆ†æ•¸"""
        base_confidence = 0.7
        
        # ç›´æ¥é—œéµå­—æœ‰æ›´é«˜ä¿¡å¿ƒ
        if any(direct in keyword for direct in ["å†ç”Ÿå¡‘è† ", "å†ç”Ÿèšé…¯", "PPå›æ”¶"]):
            base_confidence = 0.9
        
        # æª¢æŸ¥æ•¸å€¼çš„å­˜åœ¨
        numbers = re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?', context)
        if numbers:
            base_confidence += 0.1
        
        # æª¢æŸ¥å–®ä½çš„å­˜åœ¨
        units = re.findall(r'(?:å™¸|kg|ä»¶|å€‹|%|å„„æ”¯)', context)
        if units:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)

    def extract_values(self, text: str) -> Tuple[List[str], List[str]]:
        """æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”"""
        numbers = []
        percentages = []
        
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            percentages.extend(matches)
        
        return list(set(numbers)), list(set(percentages))

class ESGExtractorOptimized:
    """å„ªåŒ–ç‰ˆESGè³‡æ–™æå–å™¨"""
    
    def __init__(self):
        self.matcher = PreciseKeywordMatcher()
        self.vector_db_path = VECTOR_DB_PATH
        self._load_vector_database()
        self._init_llm()
        
        print("âœ… å„ªåŒ–ç‰ˆESGæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_vector_database(self):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        print(f"ğŸ“š è¼‰å…¥å‘é‡è³‡æ–™åº«: {self.vector_db_path}")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        self.db = FAISS.load_local(
            self.vector_db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )

    def _init_llm(self):
        """åˆå§‹åŒ–LLMï¼ˆåƒ…åœ¨éœ€è¦æ™‚ä½¿ç”¨ï¼‰"""
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=GEMINI_MODEL,
                google_api_key=GOOGLE_API_KEY,
                temperature=0,
                max_tokens=512
            )
            self.llm_available = True
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆï¼ˆåƒ…ç”¨æ–¼æ‰¹é‡é©—è­‰ï¼‰")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.llm_available = False

    def retrieve_documents(self, max_docs: int = 100) -> List[Document]:
        """æª¢ç´¢ç›¸é—œæ–‡æª”"""
        search_terms = [
            "å†ç”Ÿå¡‘è† ", "å†ç”Ÿèšé…¯", "PPå›æ”¶", "å¯¶ç‰¹ç“¶å›æ”¶", 
            "MLCCå›æ”¶", "ç¹”ç‰©å›æ”¶", "å›æ”¶æ–™", "å¾ªç’°ç¶“æ¿Ÿ"
        ]
        
        all_docs = []
        for term in search_terms:
            docs = self.db.similarity_search(term, k=15)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹ç›¸é—œæ–‡æª”")
        return result_docs

    def extract_from_documents(self, documents: List[Document]) -> List[ExtractionResult]:
        """å¾æ–‡æª”ä¸­æå–æ•¸æ“š"""
        print("ğŸ” é–‹å§‹ç²¾ç¢ºæå–...")
        extractions = []
        
        for doc in tqdm(documents, desc="è™•ç†æ–‡æª”"):
            # åˆ†å‰²æˆæ®µè½
            paragraphs = self._split_into_paragraphs(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 20:
                    continue
                
                # åŒ¹é…é—œéµå­—
                keyword_matches = self.matcher.match_keywords(paragraph)
                
                if keyword_matches:
                    # æå–æ•¸å€¼
                    numbers, percentages = self.matcher.extract_values(paragraph)
                    
                    if numbers or percentages:
                        # ç‚ºæ¯å€‹åŒ¹é…å‰µå»ºæå–çµæœ
                        for keyword, confidence in keyword_matches:
                            # è™•ç†æ•¸å€¼
                            for number in numbers:
                                extraction = ExtractionResult(
                                    keyword=keyword,
                                    value=number.strip(),
                                    value_type='number',
                                    unit=self._extract_unit(number),
                                    paragraph=paragraph.strip(),
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context=self._get_context(doc.page_content, paragraph)
                                )
                                extractions.append(extraction)
                            
                            # è™•ç†ç™¾åˆ†æ¯”
                            for percentage in percentages:
                                extraction = ExtractionResult(
                                    keyword=keyword,
                                    value=percentage.strip(),
                                    value_type='percentage',
                                    unit='%',
                                    paragraph=paragraph.strip(),
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=confidence,
                                    context=self._get_context(doc.page_content, paragraph)
                                )
                                extractions.append(extraction)
        
        print(f"âœ… åˆæ­¥æå–å®Œæˆ: {len(extractions)} å€‹çµæœ")
        return extractions

    def batch_llm_validation(self, extractions: List[ExtractionResult]) -> List[ExtractionResult]:
        """æ‰¹é‡LLMé©—è­‰ï¼ˆæ¸›å°‘APIèª¿ç”¨ï¼‰"""
        if not self.llm_available or not extractions:
            return extractions
        
        print("ğŸ¤– åŸ·è¡Œæ‰¹é‡LLMé©—è­‰...")
        
        # å°‡æå–çµæœåˆ†æ‰¹ï¼Œæ¯æ‰¹5å€‹
        batch_size = 5
        validated_extractions = []
        
        for i in range(0, len(extractions), batch_size):
            batch = extractions[i:i+batch_size]
            
            try:
                # æ§‹å»ºæ‰¹é‡é©—è­‰æç¤º
                batch_text = self._build_batch_prompt(batch)
                response = self.llm.invoke(batch_text)
                
                # è§£ææ‰¹é‡å›æ‡‰
                validations = self._parse_batch_response(response.content, len(batch))
                
                # æ›´æ–°æå–çµæœ
                for j, extraction in enumerate(batch):
                    if j < len(validations) and validations[j].get('is_relevant', True):
                        extraction.confidence = min(
                            (extraction.confidence + validations[j].get('confidence', 0.5)) / 2,
                            1.0
                        )
                        validated_extractions.append(extraction)
                
                print(f"âœ… æ‰¹é‡é©—è­‰å®Œæˆ {i+1}-{min(i+batch_size, len(extractions))}")
                
            except Exception as e:
                print(f"âš ï¸ æ‰¹é‡é©—è­‰å¤±æ•—: {e}")
                # å¦‚æœLLMå¤±æ•—ï¼Œä¿ç•™åŸå§‹çµæœ
                validated_extractions.extend(batch)
        
        print(f"ğŸ¯ LLMé©—è­‰å¾Œ: {len(validated_extractions)} å€‹æœ‰æ•ˆçµæœ")
        return validated_extractions

    def _build_batch_prompt(self, batch: List[ExtractionResult]) -> str:
        """æ§‹å»ºæ‰¹é‡é©—è­‰æç¤º"""
        prompt = "è«‹é©—è­‰ä»¥ä¸‹æ•¸æ“šæå–æ˜¯å¦èˆ‡å†ç”Ÿå¡‘è† ç›¸é—œã€‚å›ç­”æ ¼å¼ï¼šæ¯è¡Œä¸€å€‹æ•¸å­—(1-5)ï¼Œ1=ç›¸é—œï¼Œ0=ä¸ç›¸é—œ\n\n"
        
        for i, extraction in enumerate(batch, 1):
            prompt += f"{i}. é—œéµå­—ï¼š{extraction.keyword}\n"
            prompt += f"   æ•¸å€¼ï¼š{extraction.value}\n"
            prompt += f"   æ®µè½ï¼š{extraction.paragraph[:100]}...\n\n"
        
        prompt += "è«‹åªå›ç­”æ•¸å­—åºåˆ—ï¼Œä¾‹å¦‚ï¼š1,0,1,1,0"
        return prompt

    def _parse_batch_response(self, response: str, batch_size: int) -> List[Dict]:
        """è§£ææ‰¹é‡å›æ‡‰"""
        try:
            # å°‹æ‰¾æ•¸å­—åºåˆ—
            numbers = re.findall(r'[01]', response)
            
            validations = []
            for i in range(batch_size):
                if i < len(numbers):
                    is_relevant = numbers[i] == '1'
                    validations.append({
                        'is_relevant': is_relevant,
                        'confidence': 0.8 if is_relevant else 0.3
                    })
                else:
                    validations.append({'is_relevant': True, 'confidence': 0.5})
            
            return validations
        except:
            # å¦‚æœè§£æå¤±æ•—ï¼Œå…¨éƒ¨æ¨™è¨˜ç‚ºç›¸é—œ
            return [{'is_relevant': True, 'confidence': 0.5}] * batch_size

    def deduplicate_results(self, extractions: List[ExtractionResult]) -> List[ExtractionResult]:
        """å»é‡çµæœ"""
        if not extractions:
            return extractions
        
        print("ğŸ§¹ åŸ·è¡Œæ™ºèƒ½å»é‡...")
        
        # æŒ‰é—œéµå­—å’Œæ•¸å€¼åˆ†çµ„
        groups = {}
        for extraction in extractions:
            key = f"{extraction.keyword}_{extraction.value}_{extraction.value_type}"
            if key not in groups:
                groups[key] = []
            groups[key].append(extraction)
        
        deduplicated = []
        for group in groups.values():
            if len(group) == 1:
                deduplicated.extend(group)
            else:
                # é¸æ“‡ä¿¡å¿ƒåˆ†æ•¸æœ€é«˜çš„
                best = max(group, key=lambda x: x.confidence)
                # åˆä½µé ç¢¼ä¿¡æ¯
                pages = list(set([e.page_number for e in group]))
                best.page_number = " | ".join(pages)
                best.context += f" [åˆä½µäº†{len(group)}å€‹é‡è¤‡çµæœ]"
                deduplicated.append(best)
        
        print(f"âœ… å»é‡å®Œæˆ: {len(extractions)} â†’ {len(deduplicated)}")
        return deduplicated

    def export_to_excel(self, extractions: List[ExtractionResult]) -> str:
        """åŒ¯å‡ºåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(RESULTS_PATH, f"esg_extraction_optimized_{timestamp}.xlsx")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºçµæœåˆ°Excel: {output_path}")
        
        # æº–å‚™æ•¸æ“š
        data = []
        for extraction in extractions:
            data.append({
                'é—œéµå­—é¡åˆ¥': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'é ç¢¼': extraction.page_number,
                'æ®µè½å…§å®¹': extraction.paragraph[:200] + "..." if len(extraction.paragraph) > 200 else extraction.paragraph
            })
        
        # çµ±è¨ˆæ•¸æ“š
        stats_data = []
        keyword_counts = {}
        for extraction in extractions:
            keyword_counts[extraction.keyword] = keyword_counts.get(extraction.keyword, 0) + 1
        
        for keyword, count in keyword_counts.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            avg_confidence = np.mean([e.confidence for e in keyword_extractions])
            
            stats_data.append({
                'é—œéµå­—é¡åˆ¥': keyword,
                'æå–æ•¸é‡': count,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(avg_confidence, 3),
                'æ•¸å€¼é¡å‹æ•¸': len([e for e in keyword_extractions if e.value_type == 'number']),
                'ç™¾åˆ†æ¯”é¡å‹æ•¸': len([e for e in keyword_extractions if e.value_type == 'percentage'])
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            pd.DataFrame(data).to_excel(writer, sheet_name='æå–çµæœ', index=False)
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='çµ±è¨ˆæ‘˜è¦', index=False)
        
        print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜")
        return output_path

    def run_complete_extraction(self) -> Tuple[List[ExtractionResult], Dict, str]:
        """åŸ·è¡Œå®Œæ•´æå–æµç¨‹"""
        start_time = datetime.now()
        print("ğŸš€ é–‹å§‹å„ªåŒ–ç‰ˆè³‡æ–™æå–æµç¨‹")
        print("=" * 50)
        
        # 1. æª¢ç´¢æ–‡æª”
        documents = self.retrieve_documents()
        
        # 2. æå–æ•¸æ“š
        extractions = self.extract_from_documents(documents)
        
        # 3. æ‰¹é‡LLMé©—è­‰ï¼ˆæ¸›å°‘APIèª¿ç”¨ï¼‰
        validated_extractions = self.batch_llm_validation(extractions)
        
        # 4. å»é‡
        final_extractions = self.deduplicate_results(validated_extractions)
        
        # 5. åŒ¯å‡ºçµæœ
        excel_path = self.export_to_excel(final_extractions)
        
        # 6. ç”Ÿæˆæ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        summary = {
            'total_documents': len(documents),
            'total_extractions': len(final_extractions),
            'processing_time': processing_time,
            'keywords_found': len(set([e.keyword for e in final_extractions]))
        }
        
        # 7. é¡¯ç¤ºæ‘˜è¦
        self._print_summary(summary, final_extractions)
        
        return final_extractions, summary, excel_path

    def _split_into_paragraphs(self, text: str) -> List[str]:
        """åˆ†å‰²æ®µè½"""
        paragraphs = re.split(r'\n{2,}|\r{2,}|ã€‚{2,}', text)
        return [p.strip() for p in paragraphs if len(p.strip()) >= 20]

    def _extract_unit(self, value_str: str) -> str:
        """æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff/]+', value_str)
        return units[-1] if units else ""

    def _get_context(self, full_text: str, paragraph: str, window_size: int = 80) -> str:
        """ç²å–ä¸Šä¸‹æ–‡"""
        try:
            pos = full_text.find(paragraph)
            if pos == -1:
                return paragraph[:150]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(paragraph) + window_size)
            return full_text[start:end]
        except:
            return paragraph[:150]

    def _print_summary(self, summary: Dict, extractions: List[ExtractionResult]):
        """é¡¯ç¤ºæ‘˜è¦"""
        print("\n" + "=" * 50)
        print("ğŸ“‹ å„ªåŒ–ç‰ˆæå–å®Œæˆæ‘˜è¦")
        print("=" * 50)
        print(f"ğŸ“š è™•ç†æ–‡æª”æ•¸: {summary['total_documents']}")
        print(f"ğŸ“Š ç¸½æå–çµæœ: {summary['total_extractions']}")
        print(f"ğŸ¯ é—œéµå­—é¡åˆ¥: {summary['keywords_found']}")
        print(f"â±ï¸ è™•ç†æ™‚é–“: {summary['processing_time']:.2f} ç§’")
        
        if extractions:
            keyword_counts = {}
            for extraction in extractions:
                keyword_counts[extraction.keyword] = keyword_counts.get(extraction.keyword, 0) + 1
            
            print(f"\nğŸ“ˆ é—œéµå­—åˆ†å¸ƒ:")
            for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"   {keyword}: {count} å€‹çµæœ")
            
            avg_confidence = np.mean([e.confidence for e in extractions])
            print(f"\nğŸ“Š å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {avg_confidence:.3f}")

def main():
    """ä¸»å‡½æ•¸ - ç¨ç«‹æ¸¬è©¦"""
    try:
        extractor = ESGExtractorOptimized()
        extractions, summary, excel_path = extractor.run_complete_extraction()
        
        if extractions:
            print(f"\nğŸ‰ æå–å®Œæˆï¼å…± {len(extractions)} å€‹çµæœ")
            print(f"ğŸ“ çµæœå·²ä¿å­˜è‡³: {excel_path}")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç›¸é—œæ•¸æ“š")
    
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()