#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ v2.5 - å¹³è¡¡ç‰ˆï¼ˆå¢å¼·æ•¸å€¼æº–ç¢ºæ€§ + é é¢å»é‡ï¼‰
å¤§å¹…åŠ å¼·é—œéµå­—èˆ‡æ•¸å€¼ä¹‹é–“çš„é—œè¯æ€§æª¢æŸ¥ï¼Œä¸¦å¯¦æ–½é é¢å»é‡ï¼ˆæ¯é æœ€å¤š2ç­†ï¼‰
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""
    keyword_distance: int = 0  # æ–°å¢ï¼šé—œéµå­—èˆ‡æ•¸å€¼çš„è·é›¢

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# å¹³è¡¡ç‰ˆé—œéµå­—é…ç½®ï¼ˆä¿æŒä¸è®Šï¼‰
# =============================================================================

class BalancedKeywordConfig:
    """å¹³è¡¡ç‰ˆé—œéµå­—é…ç½®ï¼Œç¢ºä¿åŸºæœ¬è¦†è“‹ç‡åŒæ™‚æé«˜ç²¾ç¢ºåº¦"""
    
    RECYCLED_PLASTIC_KEYWORDS = {
        "high_relevance_continuous": [
            "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”ŸPET", "å†ç”ŸPP",
            "å›æ”¶å¡‘è† ", "å›æ”¶å¡‘æ–™", "å›æ”¶PP", "å›æ”¶PET", 
            "rPET", "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™",
            "å¯¶ç‰¹ç“¶å›æ”¶", "å»¢å¡‘è† å›æ”¶", "å¡‘è† å¾ªç’°",
            "å›æ”¶é€ ç²’", "å†ç”Ÿèšé…¯", "å›æ”¶èšé…¯",
            "å¾ªç’°ç¶“æ¿Ÿ", "ç‰©æ–™å›æ”¶", "ææ–™å›æ”¶"
        ],
        
        "medium_relevance_continuous": [
            "ç’°ä¿å¡‘è† ", "ç¶ è‰²ææ–™", "æ°¸çºŒææ–™",
            "å»¢æ–™å›æ”¶", "è³‡æºå›æ”¶", "å¾ªç’°åˆ©ç”¨"
        ],
        
        "high_relevance_discontinuous": [
            ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å¯¶ç‰¹ç“¶", "å†é€ "), ("å¯¶ç‰¹ç“¶", "å¾ªç’°"),
            ("å„„æ”¯", "å¯¶ç‰¹ç“¶"), ("è¬æ”¯", "å¯¶ç‰¹ç“¶"),
            ("PET", "å›æ”¶"), ("PET", "å†ç”Ÿ"), ("PP", "å›æ”¶"), ("PP", "å†ç”Ÿ"),
            ("å¡‘è† ", "å›æ”¶"), ("å¡‘æ–™", "å›æ”¶"), ("å¡‘è† ", "å¾ªç’°"),
            ("å›æ”¶", "é€ ç²’"), ("å›æ”¶", "ç”¢èƒ½"), ("å›æ”¶", "ææ–™"),
            ("å†ç”Ÿ", "ææ–™"), ("å»¢æ–™", "å›æ”¶"), ("MLCC", "å›æ”¶"),
            ("åŸç”Ÿ", "ææ–™"), ("ç¢³æ’æ”¾", "æ¸›å°‘"), ("æ¸›ç¢³", "æ•ˆç›Š"),
            ("æ­·å¹´", "å›æ”¶"), ("å›æ”¶", "æ•¸é‡"), ("å›æ”¶", "æ•ˆç›Š"),
            ("å¾ªç’°", "ç¶“æ¿Ÿ"), ("æ°¸çºŒ", "ç™¼å±•"), ("ç’°ä¿", "ç”¢å“")
        ],
        
        "medium_relevance_discontinuous": [
            ("ç’°ä¿", "ææ–™"), ("ç¶ è‰²", "ç”¢å“"), ("æ°¸çºŒ", "ææ–™"),
            ("å»¢æ£„", "ç‰©æ–™"), ("è³‡æº", "åŒ–"), ("å¾ªç’°", "åˆ©ç”¨")
        ]
    }
    
    ENHANCED_EXCLUSION_RULES = {
        "exclude_topics": [
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "è·ç½",
            "é¦¬æ‹‰æ¾", "è³½äº‹", "é¸æ‰‹", "æ¯”è³½", "è³½è¡£", "é‹å‹•",
            "é›¨æ°´å›æ”¶", "å»¢æ°´è™•ç†", "æ°´è³ªç›£æ¸¬",
            "æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ", "æ¡ˆä¾‹é¸æ‹”",
            "èƒ½æºè½‰å‹", "ç‡ƒæ²¹æ”¹ç‡ƒ", "é‹çˆæ”¹å–„", "å¤©ç„¶æ°£ç‡ƒç‡’",
            "ç¯€èƒ½ç”¢å“", "éš”ç†±æ¼†", "ç¯€èƒ½çª—", "éš”ç†±ç´™", "é…·æ¨‚æ¼†",
            "æ°£å¯†çª—", "éš”ç†±ç”¢å“", "ä¿æº«ææ–™", "å»ºæç”¢å“",
            "å¤ªé™½èƒ½", "é¢¨é›»", "ç¶ èƒ½", "å…‰é›»", "é›»æ± ææ–™"
        ],
        
        "exclude_contexts": [
            "å‚ç›´é¦¬æ‹‰æ¾", "å²ä¸Šæœ€ç’°ä¿è³½è¡£", "å„ç•Œå¥½æ‰‹",
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å®‰çµ±è¨ˆ", 
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "å„ªè‰¯æ¡ˆä¾‹",
            "é›¨æ°´å›æ”¶é‡æ¸›å°‘", "é™é›¨é‡æ¸›å°‘",
            "ç‡ƒæ²¹æ”¹ç‡ƒæ±½é‹çˆ", "å¤©ç„¶æ°£ç‡ƒç‡’æ©Ÿ", "é‹çˆæ”¹é€ ",
            "é…·æ¨‚æ¼†", "éš”ç†±æ¼†", "ç¯€èƒ½æ°£å¯†çª—", "å†°é…·éš”ç†±ç´™",
            "å¤æ—¥ç©ºèª¿è€—èƒ½", "ç†±å‚³å°ä¿‚æ•¸", "èƒ½æºæ¶ˆè€—",
            "éš”ç†±ç”¢å“", "ç¯€èƒ½ç”¢å“ç ”ç™¼", "æ¥µç«¯æ°£å€™å½±éŸ¿"
        ],
        
        "exclude_patterns": [
            r'è·æ¥­ç½å®³.*?\d+(?:\.\d+)?%',
            r'å·¥å®‰.*?\d+(?:\.\d+)?',
            r'é¦¬æ‹‰æ¾.*?\d+',
            r'è³½äº‹.*?\d+',
            r'æ”¹å–„æ¡ˆ.*?\d+\s*ä»¶',
            r'æ¡ˆä¾‹.*?\d+\s*ä»¶',
            r'é‹çˆ.*?\d+(?:\.\d+)?.*?åƒå…ƒ',
            r'ç‡ƒæ²¹.*?\d+(?:\.\d+)?.*?å™¸',
            r'ç¯€èƒ½.*?\d+(?:\.\d+)?%',
            r'éš”ç†±.*?\d+(?:\.\d+)?%',
            r'ç©ºèª¿.*?\d+(?:\.\d+)?%'
        ]
    }
    
    PLASTIC_SPECIFIC_INDICATORS = {
        "plastic_materials": [
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "èšåˆç‰©",
            "æ¨¹è„‚", "ç²’å­", "é¡†ç²’", "ææ–™", "å¡‘è† ç²’", "èšé…¯ç²’",
            "å¯¶ç‰¹ç“¶", "ç“¶ç‰‡", "å®¹å™¨", "åŒ…è£", "è†œæ", "çº–ç¶­"
        ],
        
        "recycling_specific": [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨",
            "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "å¾ªç’°ç¶“æ¿Ÿ",
            "å»¢æ–™", "å»¢æ£„", "å›æ”¶æ–™", "å†ç”Ÿæ–™", "PCR"
        ]
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—"""
        all_keywords = []
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_discontinuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_discontinuous"])
        return all_keywords

# =============================================================================
# å¢å¼·ç‰ˆåŒ¹é…å¼•æ“ï¼ˆåŠ å¼·é—œéµå­—-æ•¸å€¼é—œè¯æ€§ï¼‰
# =============================================================================

class EnhancedBalancedMatcher:
    """å¢å¼·ç‰ˆå¹³è¡¡åŒ¹é…å¼•æ“ï¼Œå¤§å¹…æå‡é—œéµå­—èˆ‡æ•¸å€¼çš„é—œè¯æ€§æº–ç¢ºåº¦"""
    
    def __init__(self):
        self.config = BalancedKeywordConfig()
        self.max_distance = 300
        
        # æ•¸å€¼åŒ¹é…æ¨¡å¼ï¼ˆä¿æŒä¸è®Šï¼‰
        self.number_patterns = [
            r'\d+(?:\.\d+)?\s*å„„æ”¯',
            r'\d+(?:\.\d+)?\s*è¬æ”¯',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|åƒ)?å™¸',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kg|KG|å…¬æ–¤)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æœˆ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/å¹´',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æ—¥',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ä»¶|å€‹|æ‰¹|å°|å¥—)',
        ]
        
        # ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%',
            r'\d+(?:\.\d+)?\s*ï¼…',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
        ]
        
        # æ–°å¢ï¼šç„¡é—œæ•¸å€¼æ¨¡å¼ï¼ˆéœ€è¦æ’é™¤çš„æ•¸å€¼é¡å‹ï¼‰
        self.irrelevant_number_patterns = [
            r'20\d{2}\s*å¹´',  # å¹´ä»½
            r'\d{4}-\d{2}-\d{2}',  # æ—¥æœŸ
            r'\d+:\d+',  # æ™‚é–“
            r'\d+\.\d+\.\d+',  # ç‰ˆæœ¬è™Ÿ
            r'ç¬¬\d+é ',  # é ç¢¼
            r'ç¬¬\d+ç« ',  # ç« ç¯€
            r'\d+å…ƒ',  # é‡‘é¡ï¼ˆé™¤éèˆ‡å¡‘è† ç›¸é—œï¼‰
            r'\d+è¬å…ƒ',  # é‡‘é¡
            r'\d+åƒå…ƒ',  # é‡‘é¡
            r'\d+å„„å…ƒ',  # é‡‘é¡
            r'\d+è™Ÿ',  # ç·¨è™Ÿ
        ]
    
    def extract_precise_keyword_value_pairs(self, text: str, keyword: Union[str, tuple]) -> List[Tuple[str, str, float, int]]:
        """
        ç²¾ç¢ºæå–é—œéµå­—èˆ‡æ•¸å€¼çš„é…å°
        è¿”å›: [(æ•¸å€¼, æ•¸å€¼é¡å‹, é—œè¯åº¦åˆ†æ•¸, è·é›¢)]
        """
        text_lower = text.lower()
        
        # 1. å…ˆæª¢æŸ¥é—œéµå­—æ˜¯å¦å­˜åœ¨ä¸”ç›¸é—œ
        keyword_match, keyword_confidence, keyword_details = self._match_keyword_flexible(text, keyword)
        if not keyword_match:
            return []
        
        # 2. æ‰¾åˆ°é—œéµå­—åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        keyword_positions = self._get_keyword_positions(text_lower, keyword)
        if not keyword_positions:
            return []
        
        # 3. åœ¨æ¯å€‹é—œéµå­—ä½ç½®é™„è¿‘å°‹æ‰¾ç›¸é—œæ•¸å€¼
        valid_pairs = []
        
        for kw_start, kw_end in keyword_positions:
            # åœ¨é—œéµå­—å‰å¾Œ100å­—ç¬¦ç¯„åœå…§å°‹æ‰¾æ•¸å€¼
            search_start = max(0, kw_start - 100)
            search_end = min(len(text), kw_end + 100)
            search_window = text[search_start:search_end]
            
            # æå–æ•¸å€¼
            numbers = self._extract_numbers_in_window(search_window)
            percentages = self._extract_percentages_in_window(search_window)
            
            # é©—è­‰æ¯å€‹æ•¸å€¼èˆ‡é—œéµå­—çš„é—œè¯æ€§
            for number in numbers:
                number_pos = search_window.find(number)
                if number_pos != -1:
                    # è¨ˆç®—å¯¦éš›è·é›¢
                    actual_number_pos = search_start + number_pos
                    distance = min(abs(actual_number_pos - kw_start), abs(actual_number_pos - kw_end))
                    
                    # æª¢æŸ¥é—œè¯æ€§
                    association_score = self._calculate_keyword_value_association(
                        text, keyword, number, kw_start, kw_end, actual_number_pos
                    )
                    
                    if association_score > 0.5 and distance <= 80:  # æ›´åš´æ ¼çš„è·é›¢è¦æ±‚
                        valid_pairs.append((number, 'number', association_score, distance))
            
            # é©—è­‰ç™¾åˆ†æ¯”
            for percentage in percentages:
                percentage_pos = search_window.find(percentage)
                if percentage_pos != -1:
                    actual_percentage_pos = search_start + percentage_pos
                    distance = min(abs(actual_percentage_pos - kw_start), abs(actual_percentage_pos - kw_end))
                    
                    association_score = self._calculate_keyword_value_association(
                        text, keyword, percentage, kw_start, kw_end, actual_percentage_pos
                    )
                    
                    if association_score > 0.5 and distance <= 80:
                        valid_pairs.append((percentage, 'percentage', association_score, distance))
        
        # å»é‡ä¸¦æŒ‰é—œè¯åº¦æ’åº
        unique_pairs = []
        seen_values = set()
        
        for value, value_type, score, distance in sorted(valid_pairs, key=lambda x: x[2], reverse=True):
            if value not in seen_values:
                seen_values.add(value)
                unique_pairs.append((value, value_type, score, distance))
        
        return unique_pairs[:3]  # æœ€å¤šè¿”å›3å€‹æœ€ç›¸é—œçš„æ•¸å€¼
    
    def _get_keyword_positions(self, text: str, keyword: Union[str, tuple]) -> List[Tuple[int, int]]:
        """ç²å–é—œéµå­—åœ¨æ–‡æœ¬ä¸­çš„æ‰€æœ‰ä½ç½®"""
        positions = []
        
        if isinstance(keyword, str):
            keyword_lower = keyword.lower()
            start = 0
            while True:
                pos = text.find(keyword_lower, start)
                if pos == -1:
                    break
                positions.append((pos, pos + len(keyword_lower)))
                start = pos + 1
        
        elif isinstance(keyword, tuple):
            # å°æ–¼çµ„åˆé—œéµå­—ï¼Œæ‰¾åˆ°æ‰€æœ‰çµ„ä»¶éƒ½å­˜åœ¨çš„å€åŸŸ
            components = [comp.lower() for comp in keyword]
            component_positions = {}
            
            for comp in components:
                comp_positions = []
                start = 0
                while True:
                    pos = text.find(comp, start)
                    if pos == -1:
                        break
                    comp_positions.append((pos, pos + len(comp)))
                    start = pos + 1
                component_positions[comp] = comp_positions
            
            # æ‰¾åˆ°æ‰€æœ‰çµ„ä»¶éƒ½åœ¨åˆç†è·é›¢å…§çš„çµ„åˆ
            for comp1_pos in component_positions.get(components[0], []):
                for comp2_pos in component_positions.get(components[1], []):
                    distance = abs(comp1_pos[0] - comp2_pos[0])
                    if distance <= self.max_distance:
                        start_pos = min(comp1_pos[0], comp2_pos[0])
                        end_pos = max(comp1_pos[1], comp2_pos[1])
                        positions.append((start_pos, end_pos))
        
        return positions
    
    def _extract_numbers_in_window(self, window_text: str) -> List[str]:
        """åœ¨æŒ‡å®šçª—å£å…§æå–æ•¸å€¼"""
        numbers = []
        for pattern in self.number_patterns:
            matches = re.findall(pattern, window_text, re.IGNORECASE)
            for match in matches:
                # æª¢æŸ¥æ˜¯å¦ç‚ºç„¡é—œæ•¸å€¼
                if not self._is_irrelevant_number(match):
                    numbers.append(match)
        return list(set(numbers))
    
    def _extract_percentages_in_window(self, window_text: str) -> List[str]:
        """åœ¨æŒ‡å®šçª—å£å…§æå–ç™¾åˆ†æ¯”"""
        percentages = []
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, window_text, re.IGNORECASE)
            percentages.extend(matches)
        return list(set(percentages))
    
    def _is_irrelevant_number(self, number_str: str) -> bool:
        """æª¢æŸ¥æ•¸å€¼æ˜¯å¦ç‚ºç„¡é—œé¡å‹"""
        for pattern in self.irrelevant_number_patterns:
            if re.match(pattern, number_str, re.IGNORECASE):
                return True
        return False
    
    def _calculate_keyword_value_association(self, text: str, keyword: Union[str, tuple], 
                                           value: str, kw_start: int, kw_end: int, value_pos: int) -> float:
        """
        è¨ˆç®—é—œéµå­—èˆ‡æ•¸å€¼ä¹‹é–“çš„é—œè¯åº¦
        é€™æ˜¯æ–°å¢çš„æ ¸å¿ƒæ–¹æ³•ï¼Œç”¨æ–¼ç²¾ç¢ºåˆ¤æ–·æ•¸å€¼èˆ‡é—œéµå­—çš„ç›¸é—œæ€§
        """
        
        # 1. è·é›¢å› å­ï¼ˆè·é›¢è¶Šè¿‘ï¼Œé—œè¯åº¦è¶Šé«˜ï¼‰
        distance = min(abs(value_pos - kw_start), abs(value_pos - kw_end))
        if distance <= 20:
            distance_score = 1.0
        elif distance <= 50:
            distance_score = 0.8
        elif distance <= 80:
            distance_score = 0.6
        else:
            distance_score = 0.3
        
        # 2. ä¸Šä¸‹æ–‡ç›¸é—œæ€§å› å­
        # ç²å–é—œéµå­—å’Œæ•¸å€¼ä¹‹é–“çš„ä¸Šä¸‹æ–‡
        context_start = min(kw_start, value_pos) - 30
        context_end = max(kw_end, value_pos + len(value)) + 30
        context_start = max(0, context_start)
        context_end = min(len(text), context_end)
        context = text[context_start:context_end].lower()
        
        # æª¢æŸ¥ä¸Šä¸‹æ–‡ä¸­çš„ç›¸é—œè©å½™
        context_score = self._calculate_context_relevance_score(context)
        
        # 3. æ•¸å€¼åˆç†æ€§å› å­
        value_score = self._calculate_value_reasonableness_score(value, context)
        
        # 4. èªæ³•çµæ§‹å› å­ï¼ˆæª¢æŸ¥æ•¸å€¼èˆ‡é—œéµå­—ä¹‹é–“æ˜¯å¦æœ‰åˆç†çš„èªæ³•é€£æ¥ï¼‰
        syntax_score = self._calculate_syntax_connection_score(text, kw_start, kw_end, value_pos)
        
        # ç¶œåˆè©•åˆ†
        final_score = (
            distance_score * 0.35 +    # è·é›¢æ¬Šé‡35%
            context_score * 0.30 +     # ä¸Šä¸‹æ–‡æ¬Šé‡30%
            value_score * 0.20 +       # æ•¸å€¼åˆç†æ€§20%
            syntax_score * 0.15        # èªæ³•çµæ§‹15%
        )
        
        return final_score
    
    def _calculate_context_relevance_score(self, context: str) -> float:
        """è¨ˆç®—ä¸Šä¸‹æ–‡ç›¸é—œæ€§åˆ†æ•¸"""
        
        # å¼·ç›¸é—œè©å½™ï¼ˆé«˜åˆ†ï¼‰
        high_relevance_words = [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "è£½é€ ", "ç”Ÿç”¢", "ç”¢èƒ½", "ä½¿ç”¨",
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "ææ–™", "å¯¶ç‰¹ç“¶", "æ¸›ç¢³", "æ•ˆç›Š"
        ]
        
        # ä¸­ç›¸é—œè©å½™ï¼ˆä¸­åˆ†ï¼‰
        medium_relevance_words = [
            "ç’°ä¿", "æ°¸çºŒ", "ç¶ è‰²", "æ‡‰ç”¨", "åŠ å·¥", "è™•ç†", "è£½å“"
        ]
        
        # è² ç›¸é—œè©å½™ï¼ˆæ‰£åˆ†ï¼‰
        negative_words = [
            "ç½å®³", "äº‹æ•…", "é¦¬æ‹‰æ¾", "è³½äº‹", "æ”¹å–„æ¡ˆ", "æ¡ˆä¾‹",
            "é›¨æ°´", "ç¯€èƒ½", "éš”ç†±", "é‹çˆ", "ç‡ƒæ²¹"
        ]
        
        score = 0.0
        
        # è¨ˆç®—ç›¸é—œè©å½™å¾—åˆ†
        for word in high_relevance_words:
            if word in context:
                score += 0.2
        
        for word in medium_relevance_words:
            if word in context:
                score += 0.1
        
        # æ‰£é™¤è² ç›¸é—œè©å½™å¾—åˆ†
        for word in negative_words:
            if word in context:
                score -= 0.3
        
        return max(0.0, min(1.0, score))
    
    def _calculate_value_reasonableness_score(self, value: str, context: str) -> float:
        """è¨ˆç®—æ•¸å€¼åˆç†æ€§åˆ†æ•¸"""
        
        # æå–ç´”æ•¸å­—
        number_match = re.search(r'\d+(?:,\d{3})*(?:\.\d+)?', value)
        if not number_match:
            return 0.0
        
        try:
            # ç§»é™¤åƒåˆ†ä½é€—è™Ÿä¸¦è½‰æ›ç‚ºæµ®é»æ•¸
            number_str = number_match.group().replace(',', '')
            number = float(number_str)
        except ValueError:
            return 0.0
        
        # åŸºæ–¼å–®ä½å’Œæ•¸å€¼ç¯„åœåˆ¤æ–·åˆç†æ€§
        if "å„„æ”¯" in value:
            # å¯¶ç‰¹ç“¶å„„æ”¯æ•¸é‡ï¼šé€šå¸¸åœ¨1-100å„„ä¹‹é–“
            if 1 <= number <= 100:
                return 1.0
            elif 0.1 <= number <= 500:
                return 0.7
            else:
                return 0.3
        
        elif "è¬å™¸" in value or "åƒå™¸" in value:
            # è¬å™¸/åƒå™¸ï¼šé€šå¸¸åœ¨0.1-50è¬å™¸ä¹‹é–“
            if 0.1 <= number <= 50:
                return 1.0
            elif 0.01 <= number <= 100:
                return 0.7
            else:
                return 0.3
        
        elif "å™¸" in value:
            # å™¸ï¼šé€šå¸¸åœ¨1-10000å™¸ä¹‹é–“
            if 1 <= number <= 10000:
                return 1.0
            elif 0.1 <= number <= 50000:
                return 0.7
            else:
                return 0.3
        
        elif "%" in value or "ï¼…" in value:
            # ç™¾åˆ†æ¯”ï¼šé€šå¸¸åœ¨0-100%ä¹‹é–“
            if 0 <= number <= 100:
                return 1.0
            else:
                return 0.2
        
        elif "ä»¶" in value:
            # ä»¶æ•¸ï¼šé€šå¸¸åœ¨1-10000ä»¶ä¹‹é–“
            if 1 <= number <= 10000:
                return 1.0
            elif 1 <= number <= 100000:
                return 0.7
            else:
                return 0.3
        
        # é è¨­åˆç†æ€§è©•åˆ†
        return 0.5
    
    def _calculate_syntax_connection_score(self, text: str, kw_start: int, kw_end: int, value_pos: int) -> float:
        """è¨ˆç®—èªæ³•é€£æ¥åˆ†æ•¸"""
        
        # ç²å–é—œéµå­—èˆ‡æ•¸å€¼ä¹‹é–“çš„æ–‡å­—
        if value_pos < kw_start:
            between_text = text[value_pos:kw_start]
        else:
            between_text = text[kw_end:value_pos]
        
        between_text = between_text.strip().lower()
        
        # è‰¯å¥½çš„é€£æ¥è©/çŸ­èª
        good_connectors = [
            "é”", "ç‚º", "ç´„", "å…±", "ç¸½è¨ˆ", "åˆè¨ˆ", "å¯", "èƒ½", "ç”¢",
            "ç”Ÿç”¢", "è£½é€ ", "ä½¿ç”¨", "æ‡‰ç”¨", "å«", "åŒ…å«", "æä¾›",
            "ï¼š", ":", "ï¼Œ", ",", "ã€‚", "çš„", "ä¹‹", "ç­‰"
        ]
        
        # ä¸è‰¯çš„é€£æ¥ï¼ˆè¡¨ç¤ºå¯èƒ½ä¸ç›¸é—œï¼‰
        bad_connectors = [
            "ä½†", "ç„¶è€Œ", "ä¸é", "å¦å¤–", "æ­¤å¤–", "åŒæ™‚", "å¦ä¸€æ–¹é¢"
        ]
        
        # å¦‚æœè·é›¢å¾ˆè¿‘ï¼ˆ<=10å­—ç¬¦ï¼‰ï¼Œçµ¦é«˜åˆ†
        if len(between_text) <= 10:
            return 0.9
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è‰¯å¥½é€£æ¥è©
        for connector in good_connectors:
            if connector in between_text:
                return 0.8
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ä¸è‰¯é€£æ¥è©
        for connector in bad_connectors:
            if connector in between_text:
                return 0.2
        
        # å¦‚æœä¸­é–“æ–‡å­—å¤ªé•·ï¼Œé™ä½åˆ†æ•¸
        if len(between_text) > 50:
            return 0.3
        
        # é è¨­åˆ†æ•¸
        return 0.5
    
    def comprehensive_relevance_check(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """ä¿æŒåŸæœ‰çš„ç¶œåˆç›¸é—œæ€§æª¢æŸ¥æ–¹æ³•ä»¥ç¶­æŒå…¼å®¹æ€§"""
        text_lower = text.lower()
        
        # ç¬¬1æ­¥ï¼šå¼·åŒ–æ’é™¤æª¢æŸ¥
        if self._is_clearly_excluded_enhanced(text_lower):
            return False, 0.0, "æ˜ç¢ºç„¡é—œå…§å®¹"
        
        # ç¬¬2æ­¥ï¼šé—œéµå­—åŒ¹é…æª¢æŸ¥
        keyword_match, keyword_confidence, keyword_details = self._match_keyword_flexible(text, keyword)
        if not keyword_match:
            return False, 0.0, "é—œéµå­—ä¸åŒ¹é…"
        
        # ç¬¬3æ­¥ï¼šå¡‘è† ç‰¹å®šæ€§æª¢æŸ¥
        plastic_relevance = self._check_plastic_specific_relevance(text_lower)
        if plastic_relevance < 0.3:
            return False, 0.0, f"éå¡‘è† ç›¸é—œå…§å®¹: {plastic_relevance:.2f}"
        
        # ç¬¬4æ­¥ï¼šç›¸é—œæ€§æŒ‡æ¨™æª¢æŸ¥
        relevance_score = self._calculate_balanced_relevance_score(text_lower)
        
        # ç¬¬5æ­¥ï¼šç‰¹æ®Šæƒ…æ³åŠ åˆ†
        bonus_score = self._calculate_bonus_score(text_lower)
        
        # è¨ˆç®—æœ€çµ‚åˆ†æ•¸
        final_score = (
            keyword_confidence * 0.3 + 
            plastic_relevance * 0.3 + 
            relevance_score * 0.3 + 
            bonus_score * 0.1
        )
        
        is_relevant = final_score > 0.55
        
        details = f"é—œéµå­—:{keyword_confidence:.2f}, å¡‘è† ç›¸é—œ:{plastic_relevance:.2f}, ç›¸é—œæ€§:{relevance_score:.2f}, åŠ åˆ†:{bonus_score:.2f}"
        
        return is_relevant, final_score, details
    
    # ä»¥ä¸‹æ–¹æ³•ä¿æŒä¸è®Šï¼Œç¶­æŒåŸæœ‰åŠŸèƒ½
    def _is_clearly_excluded_enhanced(self, text: str) -> bool:
        """å¼·åŒ–ç‰ˆæ’é™¤æª¢æŸ¥"""
        for topic in self.config.ENHANCED_EXCLUSION_RULES["exclude_topics"]:
            if topic in text:
                return True
        
        for context in self.config.ENHANCED_EXCLUSION_RULES["exclude_contexts"]:
            if context in text:
                return True
        
        for pattern in self.config.ENHANCED_EXCLUSION_RULES["exclude_patterns"]:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        # èƒ½æºè½‰å‹ç›¸é—œæª¢æŸ¥
        energy_indicators = ["ç‡ƒæ²¹", "é‹çˆ", "å¤©ç„¶æ°£", "ç‡ƒç‡’æ©Ÿ", "èƒ½æºè½‰å‹"]
        if any(indicator in text for indicator in energy_indicators):
            plastic_indicators = ["å¡‘è† ", "å¡‘æ–™", "PET", "PP", "å¯¶ç‰¹ç“¶", "èšé…¯"]
            if not any(plastic in text for plastic in plastic_indicators):
                return True
        
        # ç¯€èƒ½ç”¢å“ç›¸é—œæª¢æŸ¥
        energy_saving_indicators = ["éš”ç†±", "ç¯€èƒ½çª—", "ä¿æº«", "ç†±å‚³å°", "ç©ºèª¿è€—èƒ½"]
        if any(indicator in text for indicator in energy_saving_indicators):
            plastic_indicators = ["å¡‘è† ", "å¡‘æ–™", "PET", "PP", "å¯¶ç‰¹ç“¶", "èšé…¯"]
            if not any(plastic in text for plastic in plastic_indicators):
                return True
        
        return False
    
    def _check_plastic_specific_relevance(self, text: str) -> float:
        """æª¢æŸ¥å¡‘è† ç‰¹å®šç›¸é—œæ€§"""
        plastic_score = 0.0
        recycling_score = 0.0
        
        plastic_count = 0
        for indicator in self.config.PLASTIC_SPECIFIC_INDICATORS["plastic_materials"]:
            if indicator in text:
                plastic_count += 1
        
        plastic_score = min(plastic_count / 3.0, 1.0)
        
        recycling_count = 0
        for indicator in self.config.PLASTIC_SPECIFIC_INDICATORS["recycling_specific"]:
            if indicator in text:
                recycling_count += 1
        
        recycling_score = min(recycling_count / 2.0, 1.0)
        
        if plastic_score > 0 and recycling_score > 0:
            return (plastic_score + recycling_score) / 2.0
        else:
            return 0.0
    
    def _calculate_balanced_relevance_score(self, text: str) -> float:
        """è¨ˆç®—å¹³è¡¡ç‰ˆç›¸é—œæ€§åˆ†æ•¸"""
        total_score = 0.0
        category_weights = {
            "plastic_materials": 0.25,
            "recycling_process": 0.30,
            "production_application": 0.15,
            "environmental_benefit": 0.15,
            "quantity_indicators": 0.15
        }
        
        relevance_indicators = {
            "plastic_materials": [
                "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "èšåˆç‰©",
                "æ¨¹è„‚", "ç²’å­", "é¡†ç²’", "ææ–™", "èšåˆç‰©", "å¡‘è† ç²’"
            ],
            "recycling_process": [
                "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨",
                "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "å¾ªç’°ç¶“æ¿Ÿ"
            ],
            "production_application": [
                "ç”Ÿç”¢", "è£½é€ ", "ç”¢èƒ½", "ç”¢é‡", "ä½¿ç”¨", "æ‡‰ç”¨",
                "è£½æˆ", "åŠ å·¥", "ç”Ÿç”¢ç·š", "å·¥å» ", "ç”¢å“"
            ],
            "environmental_benefit": [
                "æ¸›ç¢³", "ç¢³æ’æ”¾", "ç’°ä¿", "æ°¸çºŒ", "ç¯€èƒ½", "æ¸›æ’",
                "ç¢³è¶³è·¡", "ç¶ è‰²", "ä½ç¢³", "æ•ˆç›Š", "ç’°å¢ƒ"
            ],
            "quantity_indicators": [
                "å„„æ”¯", "è¬æ”¯", "å™¸", "å…¬æ–¤", "kg", "è¬å™¸", "åƒå™¸",
                "ä»¶", "å€‹", "æ‰¹", "%", "ç™¾åˆ†æ¯”"
            ]
        }
        
        for category, indicators in relevance_indicators.items():
            category_score = 0.0
            for indicator in indicators:
                if indicator in text:
                    category_score += 1
            
            normalized_score = min(category_score / len(indicators), 1.0)
            weight = category_weights.get(category, 0.1)
            total_score += normalized_score * weight
        
        return total_score
    
    def _calculate_bonus_score(self, text: str) -> float:
        """è¨ˆç®—åŠ åˆ†é …ç›®"""
        bonus_score = 0.0
        
        bonus_indicators = [
            ("å„„æ”¯", 0.3),
            ("å¯¶ç‰¹ç“¶", 0.2),
            ("å›æ”¶æ•¸é‡", 0.2),
            ("æ¸›ç¢³", 0.15),
            ("å¾ªç’°ç¶“æ¿Ÿ", 0.15),
            ("æ­·å¹´", 0.1),
            ("ç”¢èƒ½", 0.1)
        ]
        
        for indicator, bonus in bonus_indicators:
            if indicator in text:
                bonus_score += bonus
        
        return min(bonus_score, 1.0)
    
    def _match_keyword_flexible(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """éˆæ´»çš„é—œéµå­—åŒ¹é…"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            if keyword.lower() in text_lower:
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {keyword}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            positions = []
            
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            distance = max(positions) - min(positions)
            
            if distance <= 80:
                return True, 0.9, f"è¿‘è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 200:
                return True, 0.8, f"ä¸­è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= self.max_distance:
                return True, 0.6, f"é è·é›¢åŒ¹é…({distance}å­—)"
            else:
                return True, 0.4, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—)"
        
        return False, 0.0, ""

# =============================================================================
# å¢å¼·ç‰ˆå¹³è¡¡å¤šæ–‡ä»¶ESGæå–å™¨
# =============================================================================

class BalancedMultiFileESGExtractor:
    """å¢å¼·ç‰ˆå¹³è¡¡å¤šæ–‡ä»¶ESGæå–å™¨ï¼ˆç²¾ç¢ºæ•¸å€¼é—œè¯ + é é¢å»é‡ï¼‰"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = EnhancedBalancedMatcher()  # ä½¿ç”¨å¢å¼·ç‰ˆåŒ¹é…å™¨
        self.keyword_config = BalancedKeywordConfig()
        
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¢å¼·ç‰ˆå¹³è¡¡å¤šæ–‡ä»¶ESGæå–å™¨åˆå§‹åŒ–å®Œæˆï¼ˆç²¾ç¢ºæ•¸å€¼é—œè¯ + é é¢å»é‡ï¼‰")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 400) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """è™•ç†å–®å€‹æ–‡æª” - å¢å¼·ç‰ˆï¼ˆç²¾ç¢ºæ•¸å€¼é—œè¯ + é é¢å»é‡ï¼‰"""
        start_time = datetime.now()
        print(f"\nâš–ï¸ å¢å¼·ç‰ˆè™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. å¢å¼·æ–‡æª”æª¢ç´¢
        documents = self._enhanced_document_retrieval(db, max_documents)
        
        # 3. ç²¾ç¢ºæ•¸å€¼é—œè¯ç¯©é¸
        extractions = self._precise_value_association_filtering(documents, doc_info)
        
        # 4. å¼·åŒ–å¾Œè™•ç†å’Œå»é‡
        extractions = self._enhanced_post_process_extractions(extractions)
        
        # 5. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            total_documents=len(documents),
            stage1_passed=len(documents),
            stage2_passed=len(extractions),
            total_extractions=len(extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 6. åŒ¯å‡ºçµæœ
        excel_path = self._export_to_excel(extractions, summary, doc_info)
        
        return extractions, summary, excel_path
    
    def process_multiple_documents(self, docs_info: Dict[str, DocumentInfo], max_documents: int = 400) -> Dict[str, Tuple]:
        """æ‰¹é‡è™•ç†å¤šå€‹æ–‡æª”"""
        print(f"âš–ï¸ é–‹å§‹å¢å¼·ç‰ˆæ‰¹é‡è™•ç† {len(docs_info)} å€‹æ–‡æª”ï¼ˆç²¾ç¢ºæ•¸å€¼é—œè¯ + é é¢å»é‡ï¼‰")
        print("=" * 60)
        
        results = {}
        
        for pdf_path, doc_info in docs_info.items():
            try:
                print(f"\nğŸ“„ è™•ç†: {doc_info.company_name} - {doc_info.report_year}")
                
                extractions, summary, excel_path = self.process_single_document(doc_info, max_documents)
                
                results[pdf_path] = (extractions, summary, excel_path)
                
                print(f"âœ… å®Œæˆ: ç”Ÿæˆ {len(extractions)} å€‹ç²¾ç¢ºçµæœï¼ˆå·²é é¢å»é‡ï¼‰ -> {Path(excel_path).name}")
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•— {doc_info.company_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\nğŸ‰ å¢å¼·ç‰ˆæ‰¹é‡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(docs_info)} å€‹æ–‡æª”ï¼ˆå·²æ‡‰ç”¨é é¢å»é‡ï¼‰")
        return results
    
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _enhanced_document_retrieval(self, db, max_docs: int) -> List[Document]:
        """å¢å¼·çš„æ–‡æª”æª¢ç´¢"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # ç­–ç•¥1: é—œéµå­—æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œé—œéµå­—æª¢ç´¢...")
        for keyword in keywords[:20]:
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = db.similarity_search(search_term, k=15)
            all_docs.extend(docs)
        
        # ç­–ç•¥2: å»£æ³›ä¸»é¡Œæª¢ç´¢
        print("   ğŸ” åŸ·è¡Œä¸»é¡Œæª¢ç´¢...")
        topic_queries = [
            "å¡‘è†  å›æ”¶ ææ–™",
            "å¯¶ç‰¹ç“¶ å¾ªç’° ç¶“æ¿Ÿ",
            "å†ç”Ÿ ç’°ä¿ æ°¸çºŒ",
            "å»¢æ–™ è™•ç† åˆ©ç”¨",
            "æ¸›ç¢³ æ•ˆç›Š ç’°å¢ƒ"
        ]
        
        for query in topic_queries:
            docs = db.similarity_search(query, k=20)
            all_docs.extend(docs)
        
        # ç­–ç•¥3: æ•¸å€¼æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œæ•¸å€¼æª¢ç´¢...")
        number_queries = [
            "å„„æ”¯", "è¬å™¸", "åƒå™¸", "ç”¢èƒ½", "å›æ”¶é‡",
            "æ¸›ç¢³", "ç™¾åˆ†æ¯”", "æ•ˆç›Š", "æ•¸é‡"
        ]
        
        for query in number_queries:
            docs = db.similarity_search(query, k=10)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹å€™é¸æ–‡æª”")
        return result_docs
    
    def _precise_value_association_filtering(self, documents: List[Document], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """
        ç²¾ç¢ºæ•¸å€¼é—œè¯ç¯©é¸ - æ ¸å¿ƒæ”¹é€²æ–¹æ³•
        ä½¿ç”¨æ–°çš„ç²¾ç¢ºé—œéµå­—-æ•¸å€¼é…å°é‚è¼¯
        """
        print("ğŸ¯ åŸ·è¡Œç²¾ç¢ºæ•¸å€¼é—œè¯ç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="ç²¾ç¢ºç¯©é¸"):
            # ä½¿ç”¨å¤šç¨®æ®µè½åˆ†å‰²ç­–ç•¥
            paragraphs = self._flexible_paragraph_split(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 15:
                    continue
                
                # å°æ¯å€‹é—œéµå­—é€²è¡Œç²¾ç¢ºé…å°
                for keyword in keywords:
                    # å…ˆæª¢æŸ¥åŸºæœ¬ç›¸é—œæ€§
                    is_relevant, relevance_score, details = self.matcher.comprehensive_relevance_check(paragraph, keyword)
                    
                    if is_relevant and relevance_score > 0.55:
                        # ä½¿ç”¨æ–°çš„ç²¾ç¢ºé…å°æ–¹æ³•
                        precise_pairs = self.matcher.extract_precise_keyword_value_pairs(paragraph, keyword)
                        
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç²¾ç¢ºé…å°çš„æ•¸å€¼ï¼Œä½†ç›¸é—œæ€§å¾ˆé«˜ï¼Œä¿ç•™ä½œç‚ºæè¿°
                        if not precise_pairs and relevance_score > 0.75:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value="[ç›¸é—œæè¿°]",
                                value_type='description',
                                unit='',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                keyword_distance=0
                            )
                            extractions.append(extraction)
                        
                        # è™•ç†æ‰¾åˆ°çš„ç²¾ç¢ºé…å°
                        for value, value_type, association_score, distance in precise_pairs:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            # çµåˆåŸå§‹ç›¸é—œæ€§åˆ†æ•¸å’Œé—œè¯åˆ†æ•¸
                            final_confidence = (relevance_score * 0.4 + association_score * 0.6)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=value,
                                value_type=value_type,
                                unit=self._extract_unit(value) if value_type == 'number' else '%',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=final_confidence,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year,
                                keyword_distance=distance
                            )
                            extractions.append(extraction)
        
        print(f"âœ… ç²¾ç¢ºç¯©é¸å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹ç²¾ç¢ºé—œè¯çµæœ")
        return extractions
    
    def _enhanced_post_process_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å¼·åŒ–çš„å¾Œè™•ç†å’Œå»é‡"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”§ å¼·åŒ–å¾Œè™•ç† {len(extractions)} å€‹æå–çµæœ...")
        
        # ç¬¬1æ­¥ï¼šç²¾ç¢ºå»é‡
        unique_extractions = []
        seen_combinations = set()
        
        for extraction in extractions:
            # å‰µå»ºç²¾ç¢ºå”¯ä¸€æ¨™è­˜
            identifier = (
                extraction.keyword,
                extraction.value,
                extraction.value_type,
                extraction.paragraph[:100]
            )
            
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_extractions.append(extraction)
        
        print(f"ğŸ“Š ç²¾ç¢ºå»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # ç¬¬2æ­¥ï¼šåŸºæ–¼è·é›¢å’Œä¿¡å¿ƒåˆ†æ•¸çš„é«˜ç´šå»é‡
        if len(unique_extractions) > 1:
            filtered_extractions = []
            
            for i, extraction in enumerate(unique_extractions):
                is_duplicate = False
                
                for j, existing in enumerate(filtered_extractions):
                    # æª¢æŸ¥æ˜¯å¦ç‚ºé«˜åº¦ç›¸ä¼¼çš„æå–çµæœ
                    if self._is_highly_similar_extraction(extraction, existing):
                        is_duplicate = True
                        # é¸æ“‡æ›´å¥½çš„çµæœï¼ˆè·é›¢æ›´è¿‘ä¸”ä¿¡å¿ƒåˆ†æ•¸æ›´é«˜ï¼‰
                        if (extraction.confidence > existing.confidence or 
                            (extraction.confidence == existing.confidence and 
                             extraction.keyword_distance < existing.keyword_distance)):
                            filtered_extractions[j] = extraction
                        break
                
                if not is_duplicate:
                    filtered_extractions.append(extraction)
            
            unique_extractions = filtered_extractions
            print(f"ğŸ“Š é«˜ç´šå»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # ç¬¬3æ­¥ï¼šæŒ‰é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™2ç­†ä¿¡å¿ƒåˆ†æ•¸æœ€é«˜çš„æ•¸æ“šï¼‰
        page_filtered_extractions = self._apply_per_page_filtering(unique_extractions)
        
        # ç¬¬4æ­¥ï¼šæŒ‰ä¿¡å¿ƒåˆ†æ•¸å’Œè·é›¢æ’åº
        page_filtered_extractions.sort(key=lambda x: (x.confidence, -x.keyword_distance), reverse=True)
        
        print(f"âœ… å¼·åŒ–å¾Œè™•ç†å®Œæˆ: ä¿ç•™ {len(page_filtered_extractions)} å€‹æœ€çµ‚çµæœ")
        return page_filtered_extractions
    
    def _is_highly_similar_extraction(self, extraction1: NumericExtraction, extraction2: NumericExtraction) -> bool:
        """æª¢æŸ¥å…©å€‹æå–çµæœæ˜¯å¦é«˜åº¦ç›¸ä¼¼"""
        # æª¢æŸ¥é—œéµå­—ç›¸ä¼¼åº¦
        if extraction1.keyword != extraction2.keyword:
            return False
        
        # æª¢æŸ¥æ•¸å€¼å®Œå…¨ç›¸åŒ
        if extraction1.value == extraction2.value:
            return True
        
        # æª¢æŸ¥æ®µè½å…§å®¹é«˜åº¦ç›¸ä¼¼
        para1_words = set(extraction1.paragraph[:200].split())
        para2_words = set(extraction2.paragraph[:200].split())
        
        if para1_words and para2_words:
            overlap = len(para1_words & para2_words)
            total = len(para1_words | para2_words)
            similarity = overlap / total if total > 0 else 0
            
            # å¦‚æœæ®µè½ç›¸ä¼¼åº¦è¶…é80%ï¼Œèªç‚ºé«˜åº¦ç›¸ä¼¼
            if similarity > 0.8:
                return True
        
        return False
    
    def _apply_per_page_filtering(self, extractions: List[NumericExtraction], max_per_page: int = 2) -> List[NumericExtraction]:
        """
        æŒ‰é é¢å»é‡ï¼šæ¯é æœ€å¤šä¿ç•™æŒ‡å®šæ•¸é‡çš„æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸çµæœ
        
        Args:
            extractions: å¾…è™•ç†çš„æå–çµæœåˆ—è¡¨
            max_per_page: æ¯é æœ€å¤šä¿ç•™çš„çµæœæ•¸é‡ï¼Œé»˜èªç‚º2
            
        Returns:
            æŒ‰é é¢éæ¿¾å¾Œçš„æå–çµæœåˆ—è¡¨
        """
        if not extractions:
            return extractions
        
        print(f"ğŸ“„ åŸ·è¡ŒæŒ‰é é¢å»é‡ï¼ˆæ¯é æœ€å¤šä¿ç•™ {max_per_page} ç­†ï¼‰...")
        
        # æŒ‰é ç¢¼åˆ†çµ„
        page_groups = {}
        for extraction in extractions:
            # æ¨™æº–åŒ–é é¢ç·¨è™Ÿï¼Œå»é™¤å¯èƒ½çš„æ ¼å¼å·®ç•°
            page_key = str(extraction.page_number).strip()
            if page_key not in page_groups:
                page_groups[page_key] = []
            page_groups[page_key].append(extraction)
        
        print(f"   ğŸ“Š å…±æ¶‰åŠ {len(page_groups)} å€‹é é¢")
        
        # é¡¯ç¤ºæ¯é çš„æ•¸æ“šé‡
        page_counts = [(page, len(extractions)) for page, extractions in page_groups.items()]
        page_counts.sort(key=lambda x: x[1], reverse=True)
        
        print(f"   ğŸ“‹ å„é é¢æ•¸æ“šé‡:")
        for page, count in page_counts[:10]:  # åªé¡¯ç¤ºå‰10å€‹æœ€å¤šæ•¸æ“šçš„é é¢
            print(f"      â€¢ {page}: {count} ç­†")
        if len(page_counts) > 10:
            print(f"      â€¢ ... é‚„æœ‰ {len(page_counts) - 10} å€‹é é¢")
        
        # æ¯é é¢å…§æŒ‰ç¶œåˆè©•åˆ†æ’åºä¸¦ä¿ç•™æœ€ä½³çµæœ
        filtered_extractions = []
        page_stats = []
        
        for page_key, page_extractions in page_groups.items():
            # æŒ‰ç¶œåˆè©•åˆ†æ’åºï¼šä¿¡å¿ƒåˆ†æ•¸ç‚ºä¸»ï¼Œé—œéµå­—è·é›¢ç‚ºè¼”
            # ä¿¡å¿ƒåˆ†æ•¸é«˜çš„åœ¨å‰ï¼Œè·é›¢è¿‘çš„åœ¨å‰
            page_extractions.sort(key=lambda x: (x.confidence, -x.keyword_distance), reverse=True)
            
            # é¡¯ç¤ºç•¶å‰é é¢çš„æ’åºçµæœï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
            if len(page_extractions) > max_per_page:
                print(f"   ğŸ” {page_key} æ’åºçµæœ:")
                for i, ext in enumerate(page_extractions[:max_per_page + 2]):  # é¡¯ç¤ºå‰4å€‹
                    status = "âœ…ä¿ç•™" if i < max_per_page else "âŒç§»é™¤"
                    print(f"      {status} {ext.keyword}: {ext.value} (ä¿¡å¿ƒ:{ext.confidence:.3f}, è·é›¢:{ext.keyword_distance}å­—)")
            
            # åªä¿ç•™å‰ max_per_page å€‹çµæœ
            kept_extractions = page_extractions[:max_per_page]
            filtered_extractions.extend(kept_extractions)
            
            # è¨˜éŒ„çµ±è¨ˆä¿¡æ¯
            original_count = len(page_extractions)
            kept_count = len(kept_extractions)
            page_stats.append({
                'page': page_key,
                'original': original_count,
                'kept': kept_count,
                'removed': original_count - kept_count
            })
        
        # é¡¯ç¤ºè©³ç´°çµ±è¨ˆ
        total_original = sum(stat['original'] for stat in page_stats)
        total_kept = sum(stat['kept'] for stat in page_stats)
        total_removed = total_original - total_kept
        pages_with_removal = sum(1 for stat in page_stats if stat['removed'] > 0)
        
        print(f"   ğŸ“ˆ é é¢å»é‡çµ±è¨ˆ:")
        print(f"      â€¢ åŸå§‹ç¸½æ•¸: {total_original} ç­†")
        print(f"      â€¢ æœ€çµ‚ä¿ç•™: {total_kept} ç­†")
        print(f"      â€¢ ç¸½ç§»é™¤æ•¸é‡: {total_removed} ç­†")
        print(f"      â€¢ æœ‰ç§»é™¤è³‡æ–™çš„é é¢: {pages_with_removal} é ")
        
        # é¡¯ç¤ºç§»é™¤è¼ƒå¤šè³‡æ–™çš„é é¢è©³æƒ…
        high_removal_pages = [stat for stat in page_stats if stat['removed'] > 0]
        if high_removal_pages:
            print(f"   ğŸ” æœ‰ç§»é™¤è³‡æ–™çš„é é¢:")
            for stat in sorted(high_removal_pages, key=lambda x: x['removed'], reverse=True)[:10]:
                print(f"      â€¢ {stat['page']}: ä¿ç•™{stat['kept']}ç­†ï¼Œç§»é™¤{stat['removed']}ç­†")
        
        print(f"   âœ… é é¢å»é‡å®Œæˆ: {len(filtered_extractions)} ç­†æœ€çµ‚çµæœ")
        return filtered_extractions
    
    def _flexible_paragraph_split(self, text: str) -> List[str]:
        """éˆæ´»çš„æ®µè½åˆ†å‰²"""
        paragraphs = []
        
        # æ–¹å¼1: æ¨™æº–åˆ†å‰²
        standard_paras = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs.extend([p.strip() for p in standard_paras if len(p.strip()) >= 15])
        
        # æ–¹å¼2: å¥è™Ÿåˆ†å‰²ï¼ˆå°æ–¼ç·Šå¯†æ–‡æœ¬ï¼‰
        sentence_paras = re.split(r'ã€‚{2,}|\.{2,}', text)
        paragraphs.extend([p.strip() for p in sentence_paras if len(p.strip()) >= 30])
        
        # æ–¹å¼3: ä¿æŒåŸæ–‡çš„å¤§å¡Šæ–‡æœ¬ï¼ˆå°æ–¼è¡¨æ ¼ï¼‰
        if len(text.strip()) >= 50:
            paragraphs.append(text.strip())
        
        # å»é‡
        unique_paragraphs = []
        seen = set()
        for para in paragraphs:
            para_hash = hash(para[:100])
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)
        
        return unique_paragraphs
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
    """åŒ¯å‡ºçµæœåˆ°Excelï¼Œæ ¹æ“šæå–æ•¸é‡æ±ºå®šæª”å"""
    company_safe = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
    
    # æ ¹æ“šæå–çµæœæ•¸é‡æ±ºå®šæª”å
    if len(extractions) == 0:
        output_filename = f"ESGæå–çµæœ_ç„¡æå–_{company_safe}_{doc_info.report_year}.xlsx"
        status_message = "ç„¡æå–çµæœ"
        result_count_message = "æœªæ‰¾åˆ°ç›¸é—œå†ç”Ÿå¡‘è† æ•¸æ“š"
        version_info = f"æå–å™¨ç‰ˆæœ¬: v2.5 å¹³è¡¡ç‰ˆï¼ˆç„¡æå–çµæœï¼‰"
    else:
        output_filename = f"ESGæå–çµæœ_{company_safe}_{doc_info.report_year}.xlsx"
        status_message = f"å¹³è¡¡ç‰ˆæå–çµæœ: {len(extractions)} é …"
        result_count_message = f"æˆåŠŸæå– {len(extractions)} é …ç›¸é—œæ•¸æ“š"
        version_info = f"æå–å™¨ç‰ˆæœ¬: v2.5 å¹³è¡¡ç‰ˆï¼ˆç²¾ç¢ºé—œè¯ï¼‰"
    
    output_path = os.path.join(RESULTS_PATH, output_filename)
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    print(f"ğŸ“Š åŒ¯å‡ºçµæœåˆ°Excel: {output_filename}")
    
    # æº–å‚™ä¸»è¦æ•¸æ“š
    main_data = []
    
    # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯
    header_row = {
        'é—œéµå­—': f"å…¬å¸: {doc_info.company_name}",
        'æå–æ•¸å€¼': f"å ±å‘Šå¹´åº¦: {doc_info.report_year}",
        'æ•¸æ“šé¡å‹': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        'å–®ä½': '',
        'æ®µè½å…§å®¹': result_count_message,
        'æ®µè½ç·¨è™Ÿ': '',
        'é ç¢¼': '',
        'ä¿¡å¿ƒåˆ†æ•¸': '',
        'ä¸Šä¸‹æ–‡': version_info
    }
    main_data.append(header_row)
    
    # ç©ºè¡Œåˆ†éš”
    main_data.append({col: '' for col in header_row.keys()})
    
    # å¦‚æœæœ‰æå–çµæœï¼Œæ·»åŠ çµæœæ•¸æ“š
    if len(extractions) > 0:
        for extraction in extractions:
            main_data.append({
                'é—œéµå­—': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'æ®µè½å…§å®¹': extraction.paragraph,
                'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                'é ç¢¼': extraction.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
            })
    else:
        # å¦‚æœæ²’æœ‰æå–çµæœï¼Œæ·»åŠ èªªæ˜è¡Œ
        no_result_row = {
            'é—œéµå­—': 'ç„¡ç›¸é—œé—œéµå­—åŒ¹é…',
            'æå–æ•¸å€¼': 'N/A',
            'æ•¸æ“šé¡å‹': 'no_data',
            'å–®ä½': '',
            'æ®µè½å…§å®¹': 'åœ¨æ­¤ä»½ESGå ±å‘Šä¸­æœªæ‰¾åˆ°å†ç”Ÿå¡‘è† ç›¸é—œçš„æ•¸å€¼æ•¸æ“š',
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': 0.0,
            'ä¸Šä¸‹æ–‡': 'å¯èƒ½çš„åŸå› ï¼š1) è©²å…¬å¸æœªæ¶‰åŠå†ç”Ÿå¡‘è† æ¥­å‹™ 2) å ±å‘Šä¸­æœªè©³ç´°æŠ«éœ²ç›¸é—œæ•¸æ“š 3) é—œéµå­—åŒ¹é…ç¯„åœéœ€è¦èª¿æ•´'
        }
        main_data.append(no_result_row)
    
    # çµ±è¨ˆæ•¸æ“š
    stats_data = []
    if len(extractions) > 0:
        for keyword, count in summary.keywords_found.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            
            stats_data.append({
                'é—œéµå­—': keyword,
                'æå–æ•¸é‡': count,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
            })
    else:
        # ç„¡çµæœæ™‚çš„çµ±è¨ˆèªªæ˜
        stats_data.append({
            'é—œéµå­—': 'æœå°‹æ‘˜è¦',
            'æå–æ•¸é‡': 0,
            'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': 0.0,
            'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': 0.0,
            'èªªæ˜': 'æœªæ‰¾åˆ°åŒ¹é…çš„å†ç”Ÿå¡‘è† ç›¸é—œé—œéµå­—'
        })
    
    # å¯«å…¥Excel
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # ä¸»è¦çµæœå·¥ä½œè¡¨
        sheet_name = 'æå–çµæœ' if len(extractions) > 0 else 'ç„¡æå–çµæœ'
        pd.DataFrame(main_data).to_excel(writer, sheet_name=sheet_name, index=False)
        
        # çµ±è¨ˆå·¥ä½œè¡¨
        pd.DataFrame(stats_data).to_excel(writer, sheet_name='çµ±è¨ˆæ‘˜è¦', index=False)
        
        # è™•ç†æ‘˜è¦
        summary_data = [{
            'å…¬å¸åç¨±': summary.company_name,
            'å ±å‘Šå¹´åº¦': summary.report_year,
            'ç¸½æ–‡æª”æ•¸': summary.total_documents,
            'ç¸½æå–çµæœ': summary.total_extractions,
            'è™•ç†ç‹€æ…‹': 'æˆåŠŸæå–' if len(extractions) > 0 else 'ç„¡ç›¸é—œæ•¸æ“š',
            'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
            'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'æå–å™¨ç‰ˆæœ¬': 'v2.5 å¹³è¡¡ç‰ˆ',
            'å‚™è¨»': '' if len(extractions) > 0 else 'è©²å…¬å¸å ±å‘Šä¸­æœªç™¼ç¾å†ç”Ÿå¡‘è† ç›¸é—œæ•¸æ“š'
        }]
        pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
    
    # æ ¹æ“šçµæœè¼¸å‡ºä¸åŒçš„æˆåŠŸè¨Šæ¯
    if len(extractions) > 0:
        print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼ŒåŒ…å« {len(extractions)} é …æå–çµæœ")
    else:
        print(f"âœ… Excelæª”æ¡ˆå·²ä¿å­˜ï¼Œæ¨™è¨˜ç‚ºç„¡æå–çµæœ")
        print(f"ğŸ’¡ å»ºè­°ï¼šæª¢æŸ¥è©²å…¬å¸æ˜¯å¦æ¶‰åŠå†ç”Ÿå¡‘è† æ¥­å‹™æˆ–èª¿æ•´æœå°‹é—œéµå­—")
    
    return output_path

    # =============================================================================
    # è¼”åŠ©æ–¹æ³•
    # =============================================================================
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 150) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:300]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:300]

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦ç”¨"""
    print("âš–ï¸ å¢å¼·ç‰ˆå¹³è¡¡ESGæå–å™¨æ¸¬è©¦æ¨¡å¼ï¼ˆç²¾ç¢ºæ•¸å€¼é—œè¯ + é é¢å»é‡ï¼‰")
    
    extractor = BalancedMultiFileESGExtractor(enable_llm=False)
    print("âœ… å¢å¼·ç‰ˆå¹³è¡¡æå–å™¨åˆå§‹åŒ–å®Œæˆï¼ˆç²¾ç¢ºé—œè¯ + æ¯é æœ€å¤š2ç­†ï¼‰")

if __name__ == "__main__":
    main()