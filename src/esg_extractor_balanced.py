#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ v2.4 - å¹³è¡¡ç‰ˆ
åœ¨æå–æº–ç¢ºåº¦å’Œè¦†è“‹ç‡ä¹‹é–“å–å¾—å¹³è¡¡
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# å¹³è¡¡ç‰ˆé—œéµå­—é…ç½®
# =============================================================================

class BalancedKeywordConfig:
    """å¹³è¡¡ç‰ˆé—œéµå­—é…ç½®ï¼Œç¢ºä¿åŸºæœ¬è¦†è“‹ç‡åŒæ™‚æé«˜ç²¾ç¢ºåº¦"""
    
    # æ ¸å¿ƒå†ç”Ÿå¡‘è† é—œéµå­—
    RECYCLED_PLASTIC_KEYWORDS = {
        # é«˜ç›¸é—œé€£çºŒé—œéµå­—
        "high_relevance_continuous": [
            "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”ŸPET", "å†ç”ŸPP",
            "å›æ”¶å¡‘è† ", "å›æ”¶å¡‘æ–™", "å›æ”¶PP", "å›æ”¶PET", 
            "rPET", "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™",
            "å¯¶ç‰¹ç“¶å›æ”¶", "å»¢å¡‘è† å›æ”¶", "å¡‘è† å¾ªç’°",
            "å›æ”¶é€ ç²’", "å†ç”Ÿèšé…¯", "å›æ”¶èšé…¯",
            "å¾ªç’°ç¶“æ¿Ÿ", "ç‰©æ–™å›æ”¶", "ææ–™å›æ”¶"
        ],
        
        # ä¸­ç›¸é—œé€£çºŒé—œéµå­—
        "medium_relevance_continuous": [
            "ç’°ä¿å¡‘è† ", "ç¶ è‰²ææ–™", "æ°¸çºŒææ–™",
            "å»¢æ–™å›æ”¶", "è³‡æºå›æ”¶", "å¾ªç’°åˆ©ç”¨"
        ],
        
        # é«˜ç›¸é—œä¸é€£çºŒé—œéµå­—çµ„åˆ
        "high_relevance_discontinuous": [
            ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å¯¶ç‰¹ç“¶", "å†é€ "), ("å¯¶ç‰¹ç“¶", "å¾ªç’°"),
            ("å„„æ”¯", "å¯¶ç‰¹ç“¶"), ("è¬æ”¯", "å¯¶ç‰¹ç“¶"),
            ("PET", "å›æ”¶"), ("PET", "å†ç”Ÿ"), ("PP", "å›æ”¶"), ("PP", "å†ç”Ÿ"),
            ("å¡‘è† ", "å›æ”¶"), ("å¡‘æ–™", "å›æ”¶"), ("å¡‘è† ", "å¾ªç’°"),
            ("å›æ”¶", "é€ ç²’"), ("å›æ”¶", "ç”¢èƒ½"), ("å›æ”¶", "ææ–™"),
            ("å†ç”Ÿ", "ææ–™"), ("å»¢æ–™", "å›æ”¶"), ("MLCC", "å›æ”¶"),
            ("åŸç”Ÿ", "ææ–™"), ("ç¢³æ’æ”¾", "æ¸›å°‘"), ("æ¸›ç¢³", "æ•ˆç›Š"),
            ("æ­·å¹´", "å›æ”¶"), ("å›æ”¶", "æ•¸é‡"), ("å›æ”¶", "æ•ˆç›Š"),
            ("å¾ªç’°", "ç¶“æ¿Ÿ"), ("æ°¸çºŒ", "ç™¼å±•"), ("ç’°ä¿", "ç”¢å“")
        ],
        
        # ä¸­ç›¸é—œä¸é€£çºŒé—œéµå­—çµ„åˆ
        "medium_relevance_discontinuous": [
            ("ç’°ä¿", "ææ–™"), ("ç¶ è‰²", "ç”¢å“"), ("æ°¸çºŒ", "ææ–™"),
            ("å»¢æ£„", "ç‰©æ–™"), ("è³‡æº", "åŒ–"), ("å¾ªç’°", "åˆ©ç”¨")
        ]
    }
    
    # å¹³è¡¡ç‰ˆæ’é™¤è¦å‰‡ - åªæ’é™¤æ˜ç¢ºç„¡é—œçš„å…§å®¹
    BALANCED_EXCLUSION_RULES = {
        # æ˜ç¢ºæ’é™¤çš„ä¸»é¡Œï¼ˆæ¸›å°‘æ’é™¤é …ç›®ï¼‰
        "exclude_topics": [
            # è·æ¥­å®‰å…¨
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "è·ç½",
            
            # æ´»å‹•è³½äº‹
            "é¦¬æ‹‰æ¾", "è³½äº‹", "é¸æ‰‹", "æ¯”è³½", "è³½è¡£", "é‹å‹•",
            
            # æ°´è³‡æºï¼ˆåªæ’é™¤æ˜ç¢ºçš„æ°´è™•ç†ç›¸é—œï¼‰
            "é›¨æ°´å›æ”¶", "å»¢æ°´è™•ç†", "æ°´è³ªç›£æ¸¬",
            
            # æ”¹å–„æ¡ˆæ•¸é‡çµ±è¨ˆ
            "æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ", "æ¡ˆä¾‹é¸æ‹”"
        ],
        
        # æ’é™¤çš„ç‰¹å®šä¸Šä¸‹æ–‡ç‰‡æ®µ
        "exclude_contexts": [
            "å‚ç›´é¦¬æ‹‰æ¾", "å²ä¸Šæœ€ç’°ä¿è³½è¡£", "å„ç•Œå¥½æ‰‹",
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å®‰çµ±è¨ˆ", 
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "å„ªè‰¯æ¡ˆä¾‹",
            "é›¨æ°´å›æ”¶é‡æ¸›å°‘", "é™é›¨é‡æ¸›å°‘"
        ],
        
        # æ’é™¤çš„æ•¸å€¼æ¨¡å¼ï¼ˆæ›´ç²¾ç¢ºï¼‰
        "exclude_patterns": [
            r'è·æ¥­ç½å®³.*?\d+(?:\.\d+)?%',
            r'å·¥å®‰.*?\d+(?:\.\d+)?',
            r'é¦¬æ‹‰æ¾.*?\d+',
            r'è³½äº‹.*?\d+',
            r'æ”¹å–„æ¡ˆ.*?\d+\s*ä»¶',
            r'æ¡ˆä¾‹.*?\d+\s*ä»¶'
        ]
    }
    
    # ç›¸é—œæ€§æŒ‡æ¨™ï¼ˆèª¿æ•´æ¬Šé‡ï¼Œé™ä½è¦æ±‚ï¼‰
    RELEVANCE_INDICATORS = {
        "plastic_materials": [
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "èšåˆç‰©",
            "æ¨¹è„‚", "ç²’å­", "é¡†ç²’", "ææ–™", "èšåˆç‰©", "å¡‘è† ç²’"
        ],
        
        "recycling_process": [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨",
            "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "å¾ªç’°ç¶“æ¿Ÿ"
        ],
        
        "production_application": [
            "ç”Ÿç”¢", "è£½é€ ", "ç”¢èƒ½", "ç”¢é‡", "ä½¿ç”¨", "æ‡‰ç”¨",
            "è£½æˆ", "åŠ å·¥", "ç”Ÿç”¢ç·š", "å·¥å» ", "ç”¢å“"
        ],
        
        "environmental_benefit": [
            "æ¸›ç¢³", "ç¢³æ’æ”¾", "ç’°ä¿", "æ°¸çºŒ", "ç¯€èƒ½", "æ¸›æ’",
            "ç¢³è¶³è·¡", "ç¶ è‰²", "ä½ç¢³", "æ•ˆç›Š", "ç’°å¢ƒ"
        ],
        
        "quantity_indicators": [
            "å„„æ”¯", "è¬æ”¯", "å™¸", "å…¬æ–¤", "kg", "è¬å™¸", "åƒå™¸",
            "ä»¶", "å€‹", "æ‰¹", "%", "ç™¾åˆ†æ¯”"
        ]
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—"""
        all_keywords = []
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_discontinuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_discontinuous"])
        return all_keywords

# =============================================================================
# å¹³è¡¡ç‰ˆåŒ¹é…å¼•æ“
# =============================================================================

class BalancedMatcher:
    """å¹³è¡¡ç‰ˆåŒ¹é…å¼•æ“ï¼Œç¢ºä¿åˆç†çš„æå–è¦†è“‹ç‡"""
    
    def __init__(self):
        self.config = BalancedKeywordConfig()
        self.max_distance = 300
        
        # æ•¸å€¼åŒ¹é…æ¨¡å¼ï¼ˆä¿æŒå…¨é¢ï¼‰
        self.number_patterns = [
            r'\d+(?:\.\d+)?\s*å„„æ”¯',
            r'\d+(?:\.\d+)?\s*è¬æ”¯',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|åƒ)?å™¸',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kg|KG|å…¬æ–¤)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æœˆ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/å¹´',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æ—¥',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ä»¶|å€‹|æ‰¹|å°|å¥—)',
        ]
        
        # ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%',
            r'\d+(?:\.\d+)?\s*ï¼…',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
        ]
    
    def comprehensive_relevance_check(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """
        å¹³è¡¡ç‰ˆç›¸é—œæ€§æª¢æŸ¥ - é™ä½é–€æª»ä½†ä¿æŒè³ªé‡
        """
        text_lower = text.lower()
        
        # ç¬¬1æ­¥ï¼šå¿«é€Ÿæ’é™¤æª¢æŸ¥ï¼ˆåªæ’é™¤æ˜ç¢ºç„¡é—œçš„ï¼‰
        if self._is_clearly_excluded(text_lower):
            return False, 0.0, "æ˜ç¢ºç„¡é—œå…§å®¹"
        
        # ç¬¬2æ­¥ï¼šé—œéµå­—åŒ¹é…æª¢æŸ¥
        keyword_match, keyword_confidence, keyword_details = self._match_keyword_flexible(text, keyword)
        if not keyword_match:
            return False, 0.0, "é—œéµå­—ä¸åŒ¹é…"
        
        # ç¬¬3æ­¥ï¼šç›¸é—œæ€§æŒ‡æ¨™æª¢æŸ¥ï¼ˆé™ä½è¦æ±‚ï¼‰
        relevance_score = self._calculate_balanced_relevance_score(text_lower)
        
        # ç¬¬4æ­¥ï¼šç‰¹æ®Šæƒ…æ³åŠ åˆ†
        bonus_score = self._calculate_bonus_score(text_lower)
        
        # è¨ˆç®—æœ€çµ‚åˆ†æ•¸ï¼ˆæ›´å¯¬é¬†çš„è©•åˆ†ï¼‰
        final_score = keyword_confidence * 0.4 + relevance_score * 0.4 + bonus_score * 0.2
        
        # é™ä½é–€æª»åˆ°0.5
        is_relevant = final_score > 0.5
        
        details = f"é—œéµå­—:{keyword_confidence:.2f}, ç›¸é—œæ€§:{relevance_score:.2f}, åŠ åˆ†:{bonus_score:.2f}"
        
        return is_relevant, final_score, details
    
    def _is_clearly_excluded(self, text: str) -> bool:
        """åªæ’é™¤æ˜ç¢ºç„¡é—œçš„å…§å®¹"""
        # æª¢æŸ¥æ˜ç¢ºæ’é™¤ä¸»é¡Œ
        for topic in self.config.BALANCED_EXCLUSION_RULES["exclude_topics"]:
            if topic in text:
                return True
        
        # æª¢æŸ¥ç‰¹å®šæ’é™¤ä¸Šä¸‹æ–‡
        for context in self.config.BALANCED_EXCLUSION_RULES["exclude_contexts"]:
            if context in text:
                return True
        
        # æª¢æŸ¥æ’é™¤æ¨¡å¼
        for pattern in self.config.BALANCED_EXCLUSION_RULES["exclude_patterns"]:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        return False
    
    def _calculate_balanced_relevance_score(self, text: str) -> float:
        """è¨ˆç®—å¹³è¡¡ç‰ˆç›¸é—œæ€§åˆ†æ•¸ï¼ˆé™ä½è¦æ±‚ï¼‰"""
        total_score = 0.0
        category_weights = {
            "plastic_materials": 0.25,
            "recycling_process": 0.30,
            "production_application": 0.15,
            "environmental_benefit": 0.15,
            "quantity_indicators": 0.15
        }
        
        for category, indicators in self.config.RELEVANCE_INDICATORS.items():
            category_score = 0.0
            for indicator in indicators:
                if indicator in text:
                    category_score += 1
            
            # æ­£è¦åŒ–åˆ†æ•¸
            normalized_score = min(category_score / len(indicators), 1.0)
            weight = category_weights.get(category, 0.1)
            total_score += normalized_score * weight
        
        return total_score
    
    def _calculate_bonus_score(self, text: str) -> float:
        """è¨ˆç®—åŠ åˆ†é …ç›®"""
        bonus_score = 0.0
        
        # ç‰¹æ®Šæƒ…æ³åŠ åˆ†
        bonus_indicators = [
            ("å„„æ”¯", 0.3),  # å¯¶ç‰¹ç“¶æ•¸é‡
            ("å¯¶ç‰¹ç“¶", 0.2),
            ("å›æ”¶æ•¸é‡", 0.2),
            ("æ¸›ç¢³", 0.15),
            ("å¾ªç’°ç¶“æ¿Ÿ", 0.15),
            ("æ­·å¹´", 0.1),
            ("ç”¢èƒ½", 0.1)
        ]
        
        for indicator, bonus in bonus_indicators:
            if indicator in text:
                bonus_score += bonus
        
        return min(bonus_score, 1.0)
    
    def _match_keyword_flexible(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """éˆæ´»çš„é—œéµå­—åŒ¹é…"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            if keyword.lower() in text_lower:
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {keyword}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            positions = []
            
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            distance = max(positions) - min(positions)
            
            # æ›´å¯¬é¬†çš„è·é›¢åˆ¤æ–·
            if distance <= 80:
                return True, 0.9, f"è¿‘è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 200:
                return True, 0.8, f"ä¸­è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= self.max_distance:
                return True, 0.6, f"é è·é›¢åŒ¹é…({distance}å­—)"
            else:
                return True, 0.4, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—)"  # å³ä½¿å¾ˆé ä¹Ÿçµ¦ä½åˆ†
        
        return False, 0.0, ""
    
    def extract_numbers_and_percentages(self, text: str) -> Tuple[List[str], List[str]]:
        """æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”"""
        numbers = []
        percentages = []
        
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            percentages.extend(matches)
        
        return list(set(numbers)), list(set(percentages))

# =============================================================================
# å¹³è¡¡ç‰ˆå¤šæ–‡ä»¶ESGæå–å™¨
# =============================================================================

class BalancedMultiFileESGExtractor:
    """å¹³è¡¡ç‰ˆå¤šæ–‡ä»¶ESGæå–å™¨"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = BalancedMatcher()
        self.keyword_config = BalancedKeywordConfig()
        
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¹³è¡¡ç‰ˆå¤šæ–‡ä»¶ESGæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 400) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """è™•ç†å–®å€‹æ–‡æª” - å¹³è¡¡ç‰ˆ"""
        start_time = datetime.now()
        print(f"\nâš–ï¸ å¹³è¡¡ç‰ˆè™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. å¢å¼·æ–‡æª”æª¢ç´¢
        documents = self._enhanced_document_retrieval(db, max_documents)
        
        # 3. å¹³è¡¡ç‰ˆç¯©é¸
        extractions = self._balanced_filtering(documents, doc_info)
        
        # 4. å¾Œè™•ç†å’Œå»é‡
        extractions = self._post_process_extractions(extractions)
        
        # 5. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            total_documents=len(documents),
            stage1_passed=len(documents),
            stage2_passed=len(extractions),
            total_extractions=len(extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 6. åŒ¯å‡ºçµæœ
        excel_path = self._export_to_excel(extractions, summary, doc_info)
        
        return extractions, summary, excel_path
    
    def process_multiple_documents(self, docs_info: Dict[str, DocumentInfo], max_documents: int = 400) -> Dict[str, Tuple]:
        """æ‰¹é‡è™•ç†å¤šå€‹æ–‡æª”"""
        print(f"âš–ï¸ é–‹å§‹å¹³è¡¡ç‰ˆæ‰¹é‡è™•ç† {len(docs_info)} å€‹æ–‡æª”")
        print("=" * 60)
        
        results = {}
        
        for pdf_path, doc_info in docs_info.items():
            try:
                print(f"\nğŸ“„ è™•ç†: {doc_info.company_name} - {doc_info.report_year}")
                
                extractions, summary, excel_path = self.process_single_document(doc_info, max_documents)
                
                results[pdf_path] = (extractions, summary, excel_path)
                
                print(f"âœ… å®Œæˆ: ç”Ÿæˆ {len(extractions)} å€‹å¹³è¡¡çµæœ -> {Path(excel_path).name}")
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•— {doc_info.company_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\nğŸ‰ å¹³è¡¡ç‰ˆæ‰¹é‡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(docs_info)} å€‹æ–‡æª”")
        return results
    
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _enhanced_document_retrieval(self, db, max_docs: int) -> List[Document]:
        """å¢å¼·çš„æ–‡æª”æª¢ç´¢"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # ç­–ç•¥1: é—œéµå­—æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œé—œéµå­—æª¢ç´¢...")
        for keyword in keywords[:20]:  # å¢åŠ é—œéµå­—æ•¸é‡
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = db.similarity_search(search_term, k=15)
            all_docs.extend(docs)
        
        # ç­–ç•¥2: å»£æ³›ä¸»é¡Œæª¢ç´¢
        print("   ğŸ” åŸ·è¡Œä¸»é¡Œæª¢ç´¢...")
        topic_queries = [
            "å¡‘è†  å›æ”¶ ææ–™",
            "å¯¶ç‰¹ç“¶ å¾ªç’° ç¶“æ¿Ÿ",
            "å†ç”Ÿ ç’°ä¿ æ°¸çºŒ",
            "å»¢æ–™ è™•ç† åˆ©ç”¨",
            "æ¸›ç¢³ æ•ˆç›Š ç’°å¢ƒ"
        ]
        
        for query in topic_queries:
            docs = db.similarity_search(query, k=20)
            all_docs.extend(docs)
        
        # ç­–ç•¥3: æ•¸å€¼æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œæ•¸å€¼æª¢ç´¢...")
        number_queries = [
            "å„„æ”¯", "è¬å™¸", "åƒå™¸", "ç”¢èƒ½", "å›æ”¶é‡",
            "æ¸›ç¢³", "ç™¾åˆ†æ¯”", "æ•ˆç›Š", "æ•¸é‡"
        ]
        
        for query in number_queries:
            docs = db.similarity_search(query, k=10)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹å€™é¸æ–‡æª”")
        return result_docs
    
    def _balanced_filtering(self, documents: List[Document], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """å¹³è¡¡ç‰ˆç¯©é¸ - ç¢ºä¿åŸºæœ¬è¦†è“‹ç‡"""
        print("âš–ï¸ åŸ·è¡Œå¹³è¡¡ç‰ˆç¯©é¸...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="å¹³è¡¡ç¯©é¸"):
            # ä½¿ç”¨å¤šç¨®æ®µè½åˆ†å‰²ç­–ç•¥
            paragraphs = self._flexible_paragraph_split(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 15:  # é™ä½æœ€å°é•·åº¦è¦æ±‚
                    continue
                
                # å°æ¯å€‹é—œéµå­—é€²è¡ŒåŒ¹é…
                for keyword in keywords:
                    is_relevant, relevance_score, details = self.matcher.comprehensive_relevance_check(paragraph, keyword)
                    
                    if is_relevant and relevance_score > 0.5:  # å¹³è¡¡é–€æª»
                        # æå–æ•¸å€¼
                        numbers, percentages = self.matcher.extract_numbers_and_percentages(paragraph)
                        
                        # å¦‚æœæ²’æœ‰æ˜ç¢ºæ•¸å€¼ï¼Œä½†æœ‰é‡è¦é—œéµå­—ï¼Œä¹Ÿä¿ç•™
                        if not numbers and not percentages:
                            if relevance_score > 0.7:  # é«˜ç›¸é—œæ€§çš„æè¿°æ€§å…§å®¹
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value="[ç›¸é—œæè¿°]",
                                    value_type='description',
                                    unit='',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=relevance_score,
                                    context_window=self._get_context_window(doc.page_content, paragraph),
                                    company_name=doc_info.company_name,
                                    report_year=doc_info.report_year
                                )
                                extractions.append(extraction)
                        
                        # ç‚ºæ•¸å€¼å‰µå»ºæå–çµæœ
                        for number in numbers:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=number,
                                value_type='number',
                                unit=self._extract_unit(number),
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year
                            )
                            extractions.append(extraction)
                        
                        # ç‚ºç™¾åˆ†æ¯”å‰µå»ºæå–çµæœ
                        for percentage in percentages:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=percentage,
                                value_type='percentage',
                                unit='%',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year
                            )
                            extractions.append(extraction)
        
        print(f"âœ… å¹³è¡¡ç¯©é¸å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹å€™é¸çµæœ")
        return extractions
    
    def _flexible_paragraph_split(self, text: str) -> List[str]:
        """éˆæ´»çš„æ®µè½åˆ†å‰²"""
        # å˜—è©¦å¤šç¨®åˆ†å‰²æ–¹å¼
        paragraphs = []
        
        # æ–¹å¼1: æ¨™æº–åˆ†å‰²
        standard_paras = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs.extend([p.strip() for p in standard_paras if len(p.strip()) >= 15])
        
        # æ–¹å¼2: å¥è™Ÿåˆ†å‰²ï¼ˆå°æ–¼ç·Šå¯†æ–‡æœ¬ï¼‰
        sentence_paras = re.split(r'ã€‚{2,}|\.{2,}', text)
        paragraphs.extend([p.strip() for p in sentence_paras if len(p.strip()) >= 30])
        
        # æ–¹å¼3: ä¿æŒåŸæ–‡çš„å¤§å¡Šæ–‡æœ¬ï¼ˆå°æ–¼è¡¨æ ¼ï¼‰
        if len(text.strip()) >= 50:
            paragraphs.append(text.strip())
        
        # å»é‡
        unique_paragraphs = []
        seen = set()
        for para in paragraphs:
            para_hash = hash(para[:100])  # ä½¿ç”¨å‰100å­—ç¬¦ä½œç‚ºæ¨™è­˜
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)
        
        return unique_paragraphs
    
    def _post_process_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å¾Œè™•ç†å’Œå»é‡"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”§ å¾Œè™•ç† {len(extractions)} å€‹æå–çµæœ...")
        
        # å»é‡
        unique_extractions = []
        seen_combinations = set()
        
        for extraction in extractions:
            # å‰µå»ºå”¯ä¸€æ¨™è­˜
            identifier = (
                extraction.keyword,
                extraction.value,
                extraction.paragraph[:50]  # ä½¿ç”¨æ®µè½å‰50å­—ç¬¦
            )
            
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_extractions.append(extraction)
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        unique_extractions.sort(key=lambda x: x.confidence, reverse=True)
        
        print(f"âœ… å¾Œè™•ç†å®Œæˆ: ä¿ç•™ {len(unique_extractions)} å€‹å”¯ä¸€çµæœ")
        return unique_extractions
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        company_safe = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
        
        output_filename = f"ESGæå–çµæœ_å¹³è¡¡ç‰ˆ_{company_safe}_{doc_info.report_year}_{timestamp}.xlsx"
        output_path = os.path.join(RESULTS_PATH, output_filename)
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºå¹³è¡¡ç‰ˆçµæœåˆ°Excel: {output_filename}")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        
        # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯
        header_row = {
            'é—œéµå­—': f"å…¬å¸: {doc_info.company_name}",
            'æå–æ•¸å€¼': f"å ±å‘Šå¹´åº¦: {doc_info.report_year}",
            'æ•¸æ“šé¡å‹': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            'å–®ä½': '',
            'æ®µè½å…§å®¹': f"å¹³è¡¡ç‰ˆæå–çµæœ: {len(extractions)} é …",
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': '',
            'ä¸Šä¸‹æ–‡': f"æå–å™¨ç‰ˆæœ¬: v2.4 å¹³è¡¡ç‰ˆ"
        }
        main_data.append(header_row)
        
        # ç©ºè¡Œåˆ†éš”
        main_data.append({col: '' for col in header_row.keys()})
        
        # æå–çµæœ
        for extraction in extractions:
            main_data.append({
                'é—œéµå­—': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'æ®µè½å…§å®¹': extraction.paragraph,
                'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                'é ç¢¼': extraction.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
            })
        
        # çµ±è¨ˆæ•¸æ“š
        stats_data = []
        for keyword, count in summary.keywords_found.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            
            stats_data.append({
                'é—œéµå­—': keyword,
                'æå–æ•¸é‡': count,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            pd.DataFrame(main_data).to_excel(writer, sheet_name='å¹³è¡¡ç‰ˆæå–çµæœ', index=False)
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='é—œéµå­—çµ±è¨ˆ', index=False)
            
            # è™•ç†æ‘˜è¦
            summary_data = [{
                'å…¬å¸åç¨±': summary.company_name,
                'å ±å‘Šå¹´åº¦': summary.report_year,
                'ç¸½æ–‡æª”æ•¸': summary.total_documents,
                'ç¸½æå–çµæœ': summary.total_extractions,
                'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
                'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'æå–å™¨ç‰ˆæœ¬': 'v2.4 å¹³è¡¡ç‰ˆ'
            }]
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        print(f"âœ… å¹³è¡¡ç‰ˆExcelæª”æ¡ˆå·²ä¿å­˜")
        return output_path
    
    # =============================================================================
    # è¼”åŠ©æ–¹æ³•
    # =============================================================================
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 150) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:300]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:300]

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦ç”¨"""
    print("âš–ï¸ å¹³è¡¡ç‰ˆESGæå–å™¨æ¸¬è©¦æ¨¡å¼")
    
    extractor = BalancedMultiFileESGExtractor(enable_llm=False)
    print("âœ… å¹³è¡¡ç‰ˆæå–å™¨åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    main()