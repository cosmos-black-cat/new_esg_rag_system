#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ESGè³‡æ–™æå–å™¨ v2.4 - å¹³è¡¡ç‰ˆï¼ˆä¿®æ­£é‡è¤‡å’Œä¸ç›¸é—œå…§å®¹ï¼‰
åœ¨æå–æº–ç¢ºåº¦å’Œè¦†è“‹ç‡ä¹‹é–“å–å¾—å¹³è¡¡ï¼ŒåŠ å¼·å»é‡å’Œéæ¿¾
"""

import json
import re
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from tqdm import tqdm
import numpy as np

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from config import *
from api_manager import create_api_manager

# =============================================================================
# æ•¸æ“šçµæ§‹å®šç¾©
# =============================================================================

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    company_name: str
    report_year: str
    pdf_name: str
    db_path: str

@dataclass
class NumericExtraction:
    """æ•¸å€¼æå–çµæœ"""
    keyword: str
    value: str
    value_type: str  # 'number' or 'percentage'  
    unit: str
    paragraph: str
    paragraph_number: int
    page_number: str
    confidence: float
    context_window: str
    company_name: str = ""
    report_year: str = ""

@dataclass
class ProcessingSummary:
    """è™•ç†æ‘˜è¦"""
    company_name: str
    report_year: str
    total_documents: int
    stage1_passed: int
    stage2_passed: int
    total_extractions: int
    keywords_found: Dict[str, int]
    processing_time: float

# =============================================================================
# å¹³è¡¡ç‰ˆé—œéµå­—é…ç½®ï¼ˆåŠ å¼·éæ¿¾ï¼‰
# =============================================================================

class BalancedKeywordConfig:
    """å¹³è¡¡ç‰ˆé—œéµå­—é…ç½®ï¼Œç¢ºä¿åŸºæœ¬è¦†è“‹ç‡åŒæ™‚æé«˜ç²¾ç¢ºåº¦"""
    
    # æ ¸å¿ƒå†ç”Ÿå¡‘è† é—œéµå­—
    RECYCLED_PLASTIC_KEYWORDS = {
        # é«˜ç›¸é—œé€£çºŒé—œéµå­—
        "high_relevance_continuous": [
            "å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”ŸPET", "å†ç”ŸPP",
            "å›æ”¶å¡‘è† ", "å›æ”¶å¡‘æ–™", "å›æ”¶PP", "å›æ”¶PET", 
            "rPET", "PCRå¡‘è† ", "PCRå¡‘æ–™", "PCRææ–™",
            "å¯¶ç‰¹ç“¶å›æ”¶", "å»¢å¡‘è† å›æ”¶", "å¡‘è† å¾ªç’°",
            "å›æ”¶é€ ç²’", "å†ç”Ÿèšé…¯", "å›æ”¶èšé…¯",
            "å¾ªç’°ç¶“æ¿Ÿ", "ç‰©æ–™å›æ”¶", "ææ–™å›æ”¶"
        ],
        
        # ä¸­ç›¸é—œé€£çºŒé—œéµå­—
        "medium_relevance_continuous": [
            "ç’°ä¿å¡‘è† ", "ç¶ è‰²ææ–™", "æ°¸çºŒææ–™",
            "å»¢æ–™å›æ”¶", "è³‡æºå›æ”¶", "å¾ªç’°åˆ©ç”¨"
        ],
        
        # é«˜ç›¸é—œä¸é€£çºŒé—œéµå­—çµ„åˆ
        "high_relevance_discontinuous": [
            ("å¯¶ç‰¹ç“¶", "å›æ”¶"), ("å¯¶ç‰¹ç“¶", "å†é€ "), ("å¯¶ç‰¹ç“¶", "å¾ªç’°"),
            ("å„„æ”¯", "å¯¶ç‰¹ç“¶"), ("è¬æ”¯", "å¯¶ç‰¹ç“¶"),
            ("PET", "å›æ”¶"), ("PET", "å†ç”Ÿ"), ("PP", "å›æ”¶"), ("PP", "å†ç”Ÿ"),
            ("å¡‘è† ", "å›æ”¶"), ("å¡‘æ–™", "å›æ”¶"), ("å¡‘è† ", "å¾ªç’°"),
            ("å›æ”¶", "é€ ç²’"), ("å›æ”¶", "ç”¢èƒ½"), ("å›æ”¶", "ææ–™"),
            ("å†ç”Ÿ", "ææ–™"), ("å»¢æ–™", "å›æ”¶"), ("MLCC", "å›æ”¶"),
            ("åŸç”Ÿ", "ææ–™"), ("ç¢³æ’æ”¾", "æ¸›å°‘"), ("æ¸›ç¢³", "æ•ˆç›Š"),
            ("æ­·å¹´", "å›æ”¶"), ("å›æ”¶", "æ•¸é‡"), ("å›æ”¶", "æ•ˆç›Š"),
            ("å¾ªç’°", "ç¶“æ¿Ÿ"), ("æ°¸çºŒ", "ç™¼å±•"), ("ç’°ä¿", "ç”¢å“")
        ],
        
        # ä¸­ç›¸é—œä¸é€£çºŒé—œéµå­—çµ„åˆ
        "medium_relevance_discontinuous": [
            ("ç’°ä¿", "ææ–™"), ("ç¶ è‰²", "ç”¢å“"), ("æ°¸çºŒ", "ææ–™"),
            ("å»¢æ£„", "ç‰©æ–™"), ("è³‡æº", "åŒ–"), ("å¾ªç’°", "åˆ©ç”¨")
        ]
    }
    
    # å¼·åŒ–æ’é™¤è¦å‰‡ - ç²¾ç¢ºæ’é™¤ä¸ç›¸é—œå…§å®¹
    ENHANCED_EXCLUSION_RULES = {
        # æ˜ç¢ºæ’é™¤çš„ä¸»é¡Œï¼ˆæ“´å……ç‰ˆï¼‰
        "exclude_topics": [
            # è·æ¥­å®‰å…¨
            "è·æ¥­ç½å®³", "å·¥å®‰", "å®‰å…¨äº‹æ•…", "è·ç½",
            
            # æ´»å‹•è³½äº‹
            "é¦¬æ‹‰æ¾", "è³½äº‹", "é¸æ‰‹", "æ¯”è³½", "è³½è¡£", "é‹å‹•",
            
            # æ°´è³‡æºï¼ˆåªæ’é™¤æ˜ç¢ºçš„æ°´è™•ç†ç›¸é—œï¼‰
            "é›¨æ°´å›æ”¶", "å»¢æ°´è™•ç†", "æ°´è³ªç›£æ¸¬",
            
            # æ”¹å–„æ¡ˆæ•¸é‡çµ±è¨ˆ
            "æ”¹å–„æ¡ˆ", "æ”¹å–„å°ˆæ¡ˆ", "æ¡ˆä¾‹é¸æ‹”",
            
            # èƒ½æºè½‰å‹ï¼ˆéå¡‘è† ç›¸é—œï¼‰
            "èƒ½æºè½‰å‹", "ç‡ƒæ²¹æ”¹ç‡ƒ", "é‹çˆæ”¹å–„", "å¤©ç„¶æ°£ç‡ƒç‡’",
            
            # ç¯€èƒ½ç”¢å“ï¼ˆéå¡‘è† ææ–™ï¼‰
            "ç¯€èƒ½ç”¢å“", "éš”ç†±æ¼†", "ç¯€èƒ½çª—", "éš”ç†±ç´™", "é…·æ¨‚æ¼†",
            "æ°£å¯†çª—", "éš”ç†±ç”¢å“", "ä¿æº«ææ–™", "å»ºæç”¢å“",
            
            # å…¶ä»–éå¡‘è† ç’°ä¿ç”¢å“
            "å¤ªé™½èƒ½", "é¢¨é›»", "ç¶ èƒ½", "å…‰é›»", "é›»æ± ææ–™"
        ],
        
        # æ’é™¤çš„ç‰¹å®šä¸Šä¸‹æ–‡ç‰‡æ®µï¼ˆæ“´å……ç‰ˆï¼‰
        "exclude_contexts": [
            "å‚ç›´é¦¬æ‹‰æ¾", "å²ä¸Šæœ€ç’°ä¿è³½è¡£", "å„ç•Œå¥½æ‰‹",
            "è·æ¥­ç½å®³æ¯”ç‡", "å·¥å®‰çµ±è¨ˆ", 
            "ç¯€èƒ½æ”¹å–„æ¡ˆ", "ç¯€æ°´æ”¹å–„æ¡ˆ", "å„ªè‰¯æ¡ˆä¾‹",
            "é›¨æ°´å›æ”¶é‡æ¸›å°‘", "é™é›¨é‡æ¸›å°‘",
            "ç‡ƒæ²¹æ”¹ç‡ƒæ±½é‹çˆ", "å¤©ç„¶æ°£ç‡ƒç‡’æ©Ÿ", "é‹çˆæ”¹é€ ",
            "é…·æ¨‚æ¼†", "éš”ç†±æ¼†", "ç¯€èƒ½æ°£å¯†çª—", "å†°é…·éš”ç†±ç´™",
            "å¤æ—¥ç©ºèª¿è€—èƒ½", "ç†±å‚³å°ä¿‚æ•¸", "èƒ½æºæ¶ˆè€—",
            "éš”ç†±ç”¢å“", "ç¯€èƒ½ç”¢å“ç ”ç™¼", "æ¥µç«¯æ°£å€™å½±éŸ¿"
        ],
        
        # æ’é™¤çš„æ•¸å€¼æ¨¡å¼ï¼ˆæ›´ç²¾ç¢ºï¼‰
        "exclude_patterns": [
            r'è·æ¥­ç½å®³.*?\d+(?:\.\d+)?%',
            r'å·¥å®‰.*?\d+(?:\.\d+)?',
            r'é¦¬æ‹‰æ¾.*?\d+',
            r'è³½äº‹.*?\d+',
            r'æ”¹å–„æ¡ˆ.*?\d+\s*ä»¶',
            r'æ¡ˆä¾‹.*?\d+\s*ä»¶',
            r'é‹çˆ.*?\d+(?:\.\d+)?.*?åƒå…ƒ',
            r'ç‡ƒæ²¹.*?\d+(?:\.\d+)?.*?å™¸',
            r'ç¯€èƒ½.*?\d+(?:\.\d+)?%',
            r'éš”ç†±.*?\d+(?:\.\d+)?%',
            r'ç©ºèª¿.*?\d+(?:\.\d+)?%'
        ]
    }
    
    # å¿…é ˆåŒ…å«çš„å¡‘è† ç›¸é—œæŒ‡æ¨™ï¼ˆåŠ å¼·ç‰ˆï¼‰
    PLASTIC_SPECIFIC_INDICATORS = {
        "plastic_materials": [
            "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "èšåˆç‰©",
            "æ¨¹è„‚", "ç²’å­", "é¡†ç²’", "ææ–™", "å¡‘è† ç²’", "èšé…¯ç²’",
            "å¯¶ç‰¹ç“¶", "ç“¶ç‰‡", "å®¹å™¨", "åŒ…è£", "è†œæ", "çº–ç¶­"
        ],
        
        "recycling_specific": [
            "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨",
            "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "å¾ªç’°ç¶“æ¿Ÿ",
            "å»¢æ–™", "å»¢æ£„", "å›æ”¶æ–™", "å†ç”Ÿæ–™", "PCR"
        ]
    }
    
    @classmethod
    def get_all_keywords(cls) -> List[Union[str, tuple]]:
        """ç²å–æ‰€æœ‰é—œéµå­—"""
        all_keywords = []
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_continuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["high_relevance_discontinuous"])
        all_keywords.extend(cls.RECYCLED_PLASTIC_KEYWORDS["medium_relevance_discontinuous"])
        return all_keywords

# =============================================================================
# å¹³è¡¡ç‰ˆåŒ¹é…å¼•æ“ï¼ˆåŠ å¼·éæ¿¾å’Œå»é‡ï¼‰
# =============================================================================

class BalancedMatcher:
    """å¹³è¡¡ç‰ˆåŒ¹é…å¼•æ“ï¼Œç¢ºä¿åˆç†çš„æå–è¦†è“‹ç‡ä¸¦ç²¾ç¢ºéæ¿¾"""
    
    def __init__(self):
        self.config = BalancedKeywordConfig()
        self.max_distance = 300
        
        # æ•¸å€¼åŒ¹é…æ¨¡å¼ï¼ˆä¿æŒå…¨é¢ï¼‰
        self.number_patterns = [
            r'\d+(?:\.\d+)?\s*å„„æ”¯',
            r'\d+(?:\.\d+)?\s*è¬æ”¯',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:è¬|åƒ)?å™¸',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:kg|KG|å…¬æ–¤)',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æœˆ',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/å¹´',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*å™¸/æ—¥',
            r'\d+(?:,\d{3})*(?:\.\d+)?\s*(?:ä»¶|å€‹|æ‰¹|å°|å¥—)',
        ]
        
        # ç™¾åˆ†æ¯”åŒ¹é…æ¨¡å¼
        self.percentage_patterns = [
            r'\d+(?:\.\d+)?\s*%',
            r'\d+(?:\.\d+)?\s*ï¼…',
            r'ç™¾åˆ†ä¹‹\d+(?:\.\d+)?',
        ]
    
    def comprehensive_relevance_check(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """
        å¢å¼·ç‰ˆç›¸é—œæ€§æª¢æŸ¥ - ç²¾ç¢ºéæ¿¾éå¡‘è† ç›¸é—œå…§å®¹
        """
        text_lower = text.lower()
        
        # ç¬¬1æ­¥ï¼šå¼·åŒ–æ’é™¤æª¢æŸ¥
        if self._is_clearly_excluded_enhanced(text_lower):
            return False, 0.0, "æ˜ç¢ºç„¡é—œå…§å®¹"
        
        # ç¬¬2æ­¥ï¼šé—œéµå­—åŒ¹é…æª¢æŸ¥
        keyword_match, keyword_confidence, keyword_details = self._match_keyword_flexible(text, keyword)
        if not keyword_match:
            return False, 0.0, "é—œéµå­—ä¸åŒ¹é…"
        
        # ç¬¬3æ­¥ï¼šå¡‘è† ç‰¹å®šæ€§æª¢æŸ¥ï¼ˆæ–°å¢ï¼‰
        plastic_relevance = self._check_plastic_specific_relevance(text_lower)
        if plastic_relevance < 0.3:
            return False, 0.0, f"éå¡‘è† ç›¸é—œå…§å®¹: {plastic_relevance:.2f}"
        
        # ç¬¬4æ­¥ï¼šç›¸é—œæ€§æŒ‡æ¨™æª¢æŸ¥ï¼ˆèª¿æ•´ï¼‰
        relevance_score = self._calculate_balanced_relevance_score(text_lower)
        
        # ç¬¬5æ­¥ï¼šç‰¹æ®Šæƒ…æ³åŠ åˆ†
        bonus_score = self._calculate_bonus_score(text_lower)
        
        # è¨ˆç®—æœ€çµ‚åˆ†æ•¸ï¼ˆåŠ å…¥å¡‘è† ç‰¹å®šæ€§æ¬Šé‡ï¼‰
        final_score = (
            keyword_confidence * 0.3 + 
            plastic_relevance * 0.3 + 
            relevance_score * 0.3 + 
            bonus_score * 0.1
        )
        
        # é–€æª»è¨­ç‚º0.55ï¼Œç¨å¾®æé«˜ä»¥æ¸›å°‘ç„¡é—œå…§å®¹
        is_relevant = final_score > 0.55
        
        details = f"é—œéµå­—:{keyword_confidence:.2f}, å¡‘è† ç›¸é—œ:{plastic_relevance:.2f}, ç›¸é—œæ€§:{relevance_score:.2f}, åŠ åˆ†:{bonus_score:.2f}"
        
        return is_relevant, final_score, details
    
    def _is_clearly_excluded_enhanced(self, text: str) -> bool:
        """å¼·åŒ–ç‰ˆæ’é™¤æª¢æŸ¥"""
        # æª¢æŸ¥æ˜ç¢ºæ’é™¤ä¸»é¡Œ
        for topic in self.config.ENHANCED_EXCLUSION_RULES["exclude_topics"]:
            if topic in text:
                return True
        
        # æª¢æŸ¥ç‰¹å®šæ’é™¤ä¸Šä¸‹æ–‡
        for context in self.config.ENHANCED_EXCLUSION_RULES["exclude_contexts"]:
            if context in text:
                return True
        
        # æª¢æŸ¥æ’é™¤æ¨¡å¼
        for pattern in self.config.ENHANCED_EXCLUSION_RULES["exclude_patterns"]:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        # é¡å¤–æª¢æŸ¥ï¼šèƒ½æºè½‰å‹ç›¸é—œ
        energy_indicators = ["ç‡ƒæ²¹", "é‹çˆ", "å¤©ç„¶æ°£", "ç‡ƒç‡’æ©Ÿ", "èƒ½æºè½‰å‹"]
        if any(indicator in text for indicator in energy_indicators):
            # å¦‚æœåŒæ™‚åŒ…å«å¡‘è† ç›¸é—œè©å½™ï¼Œå‰‡ä¸æ’é™¤
            plastic_indicators = ["å¡‘è† ", "å¡‘æ–™", "PET", "PP", "å¯¶ç‰¹ç“¶", "èšé…¯"]
            if not any(plastic in text for plastic in plastic_indicators):
                return True
        
        # é¡å¤–æª¢æŸ¥ï¼šç¯€èƒ½ç”¢å“ç›¸é—œ
        energy_saving_indicators = ["éš”ç†±", "ç¯€èƒ½çª—", "ä¿æº«", "ç†±å‚³å°", "ç©ºèª¿è€—èƒ½"]
        if any(indicator in text for indicator in energy_saving_indicators):
            # å¦‚æœåŒæ™‚åŒ…å«å¡‘è† ç›¸é—œè©å½™ï¼Œå‰‡ä¸æ’é™¤
            plastic_indicators = ["å¡‘è† ", "å¡‘æ–™", "PET", "PP", "å¯¶ç‰¹ç“¶", "èšé…¯"]
            if not any(plastic in text for plastic in plastic_indicators):
                return True
        
        return False
    
    def _check_plastic_specific_relevance(self, text: str) -> float:
        """æª¢æŸ¥å¡‘è† ç‰¹å®šç›¸é—œæ€§ï¼ˆæ–°å¢æ–¹æ³•ï¼‰"""
        plastic_score = 0.0
        recycling_score = 0.0
        
        # æª¢æŸ¥å¡‘è† ææ–™ç›¸é—œè©å½™
        plastic_count = 0
        for indicator in self.config.PLASTIC_SPECIFIC_INDICATORS["plastic_materials"]:
            if indicator in text:
                plastic_count += 1
        
        plastic_score = min(plastic_count / 3.0, 1.0)  # æ­£è¦åŒ–åˆ°0-1
        
        # æª¢æŸ¥å›æ”¶å†ç”Ÿç›¸é—œè©å½™
        recycling_count = 0
        for indicator in self.config.PLASTIC_SPECIFIC_INDICATORS["recycling_specific"]:
            if indicator in text:
                recycling_count += 1
        
        recycling_score = min(recycling_count / 2.0, 1.0)  # æ­£è¦åŒ–åˆ°0-1
        
        # å¿…é ˆåŒæ™‚åŒ…å«å¡‘è† å’Œå›æ”¶ç›¸é—œè©å½™
        if plastic_score > 0 and recycling_score > 0:
            return (plastic_score + recycling_score) / 2.0
        else:
            return 0.0  # å¦‚æœä»»ä¸€é¡åˆ¥ç‚º0ï¼Œå‰‡è¿”å›0
    
    def _calculate_balanced_relevance_score(self, text: str) -> float:
        """è¨ˆç®—å¹³è¡¡ç‰ˆç›¸é—œæ€§åˆ†æ•¸ï¼ˆä¿æŒä¸è®Šï¼‰"""
        total_score = 0.0
        category_weights = {
            "plastic_materials": 0.25,
            "recycling_process": 0.30,
            "production_application": 0.15,
            "environmental_benefit": 0.15,
            "quantity_indicators": 0.15
        }
        
        relevance_indicators = {
            "plastic_materials": [
                "å¡‘è† ", "å¡‘æ–™", "èšé…¯", "PET", "PP", "èšåˆç‰©",
                "æ¨¹è„‚", "ç²’å­", "é¡†ç²’", "ææ–™", "èšåˆç‰©", "å¡‘è† ç²’"
            ],
            "recycling_process": [
                "å›æ”¶", "å†ç”Ÿ", "å¾ªç’°", "å†åˆ©ç”¨", "å›æ”¶åˆ©ç”¨",
                "é€ ç²’", "å†è£½", "è½‰æ›", "è™•ç†", "å¾ªç’°ç¶“æ¿Ÿ"
            ],
            "production_application": [
                "ç”Ÿç”¢", "è£½é€ ", "ç”¢èƒ½", "ç”¢é‡", "ä½¿ç”¨", "æ‡‰ç”¨",
                "è£½æˆ", "åŠ å·¥", "ç”Ÿç”¢ç·š", "å·¥å» ", "ç”¢å“"
            ],
            "environmental_benefit": [
                "æ¸›ç¢³", "ç¢³æ’æ”¾", "ç’°ä¿", "æ°¸çºŒ", "ç¯€èƒ½", "æ¸›æ’",
                "ç¢³è¶³è·¡", "ç¶ è‰²", "ä½ç¢³", "æ•ˆç›Š", "ç’°å¢ƒ"
            ],
            "quantity_indicators": [
                "å„„æ”¯", "è¬æ”¯", "å™¸", "å…¬æ–¤", "kg", "è¬å™¸", "åƒå™¸",
                "ä»¶", "å€‹", "æ‰¹", "%", "ç™¾åˆ†æ¯”"
            ]
        }
        
        for category, indicators in relevance_indicators.items():
            category_score = 0.0
            for indicator in indicators:
                if indicator in text:
                    category_score += 1
            
            # æ­£è¦åŒ–åˆ†æ•¸
            normalized_score = min(category_score / len(indicators), 1.0)
            weight = category_weights.get(category, 0.1)
            total_score += normalized_score * weight
        
        return total_score
    
    def _calculate_bonus_score(self, text: str) -> float:
        """è¨ˆç®—åŠ åˆ†é …ç›®ï¼ˆä¿æŒä¸è®Šï¼‰"""
        bonus_score = 0.0
        
        # ç‰¹æ®Šæƒ…æ³åŠ åˆ†
        bonus_indicators = [
            ("å„„æ”¯", 0.3),  # å¯¶ç‰¹ç“¶æ•¸é‡
            ("å¯¶ç‰¹ç“¶", 0.2),
            ("å›æ”¶æ•¸é‡", 0.2),
            ("æ¸›ç¢³", 0.15),
            ("å¾ªç’°ç¶“æ¿Ÿ", 0.15),
            ("æ­·å¹´", 0.1),
            ("ç”¢èƒ½", 0.1)
        ]
        
        for indicator, bonus in bonus_indicators:
            if indicator in text:
                bonus_score += bonus
        
        return min(bonus_score, 1.0)
    
    def _match_keyword_flexible(self, text: str, keyword: Union[str, tuple]) -> Tuple[bool, float, str]:
        """éˆæ´»çš„é—œéµå­—åŒ¹é…ï¼ˆä¿æŒä¸è®Šï¼‰"""
        text_lower = text.lower()
        
        if isinstance(keyword, str):
            if keyword.lower() in text_lower:
                return True, 1.0, f"ç²¾ç¢ºåŒ¹é…: {keyword}"
            return False, 0.0, ""
        
        elif isinstance(keyword, tuple):
            components = [comp.lower() for comp in keyword]
            positions = []
            
            for comp in components:
                pos = text_lower.find(comp)
                if pos == -1:
                    return False, 0.0, f"ç¼ºå°‘çµ„ä»¶: {comp}"
                positions.append(pos)
            
            distance = max(positions) - min(positions)
            
            # æ›´å¯¬é¬†çš„è·é›¢åˆ¤æ–·
            if distance <= 80:
                return True, 0.9, f"è¿‘è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= 200:
                return True, 0.8, f"ä¸­è·é›¢åŒ¹é…({distance}å­—)"
            elif distance <= self.max_distance:
                return True, 0.6, f"é è·é›¢åŒ¹é…({distance}å­—)"
            else:
                return True, 0.4, f"æ¥µé è·é›¢åŒ¹é…({distance}å­—)"  # å³ä½¿å¾ˆé ä¹Ÿçµ¦ä½åˆ†
        
        return False, 0.0, ""
    
    def extract_numbers_and_percentages(self, text: str) -> Tuple[List[str], List[str]]:
        """æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”ï¼ˆä¿æŒä¸è®Šï¼‰"""
        numbers = []
        percentages = []
        
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        for pattern in self.percentage_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            percentages.extend(matches)
        
        return list(set(numbers)), list(set(percentages))

# =============================================================================
# å¹³è¡¡ç‰ˆå¤šæ–‡ä»¶ESGæå–å™¨ï¼ˆå¼·åŒ–å»é‡ï¼‰
# =============================================================================

class BalancedMultiFileESGExtractor:
    """å¹³è¡¡ç‰ˆå¤šæ–‡ä»¶ESGæå–å™¨ï¼ˆå¼·åŒ–å»é‡ï¼‰"""
    
    def __init__(self, enable_llm: bool = True):
        self.enable_llm = enable_llm
        self.matcher = BalancedMatcher()
        self.keyword_config = BalancedKeywordConfig()
        
        if self.enable_llm:
            self._init_llm()
        
        print("âœ… å¹³è¡¡ç‰ˆå¤šæ–‡ä»¶ESGæå–å™¨åˆå§‹åŒ–å®Œæˆï¼ˆå¼·åŒ–å»é‡ç‰ˆï¼‰")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            print("ğŸ¤– åˆå§‹åŒ–Gemini APIç®¡ç†å™¨...")
            self.api_manager = create_api_manager()
            print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±æ•—: {e}")
            self.enable_llm = False
    
    def process_single_document(self, doc_info: DocumentInfo, max_documents: int = 400) -> Tuple[List[NumericExtraction], ProcessingSummary, str]:
        """è™•ç†å–®å€‹æ–‡æª” - å¹³è¡¡ç‰ˆï¼ˆå¼·åŒ–å»é‡ï¼‰"""
        start_time = datetime.now()
        print(f"\nâš–ï¸ å¹³è¡¡ç‰ˆè™•ç†æ–‡æª”: {doc_info.company_name} - {doc_info.report_year}")
        print("=" * 60)
        
        # 1. è¼‰å…¥å‘é‡è³‡æ–™åº«
        db = self._load_vector_database(doc_info.db_path)
        
        # 2. å¢å¼·æ–‡æª”æª¢ç´¢
        documents = self._enhanced_document_retrieval(db, max_documents)
        
        # 3. å¹³è¡¡ç‰ˆç¯©é¸ï¼ˆåŠ å¼·éæ¿¾ï¼‰
        extractions = self._balanced_filtering(documents, doc_info)
        
        # 4. å¼·åŒ–å¾Œè™•ç†å’Œå»é‡ï¼ˆé‡é»æ”¹é€²ï¼‰
        extractions = self._enhanced_post_process_extractions(extractions)
        
        # 5. å‰µå»ºè™•ç†æ‘˜è¦
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        keywords_found = {}
        for extraction in extractions:
            keyword = extraction.keyword
            keywords_found[keyword] = keywords_found.get(keyword, 0) + 1
        
        summary = ProcessingSummary(
            company_name=doc_info.company_name,
            report_year=doc_info.report_year,
            total_documents=len(documents),
            stage1_passed=len(documents),
            stage2_passed=len(extractions),
            total_extractions=len(extractions),
            keywords_found=keywords_found,
            processing_time=processing_time
        )
        
        # 6. åŒ¯å‡ºçµæœ
        excel_path = self._export_to_excel(extractions, summary, doc_info)
        
        return extractions, summary, excel_path
    
    def process_multiple_documents(self, docs_info: Dict[str, DocumentInfo], max_documents: int = 400) -> Dict[str, Tuple]:
        """æ‰¹é‡è™•ç†å¤šå€‹æ–‡æª”"""
        print(f"âš–ï¸ é–‹å§‹å¹³è¡¡ç‰ˆæ‰¹é‡è™•ç† {len(docs_info)} å€‹æ–‡æª”ï¼ˆå¼·åŒ–å»é‡ç‰ˆï¼‰")
        print("=" * 60)
        
        results = {}
        
        for pdf_path, doc_info in docs_info.items():
            try:
                print(f"\nğŸ“„ è™•ç†: {doc_info.company_name} - {doc_info.report_year}")
                
                extractions, summary, excel_path = self.process_single_document(doc_info, max_documents)
                
                results[pdf_path] = (extractions, summary, excel_path)
                
                print(f"âœ… å®Œæˆ: ç”Ÿæˆ {len(extractions)} å€‹å¹³è¡¡çµæœ -> {Path(excel_path).name}")
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•— {doc_info.company_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\nğŸ‰ å¹³è¡¡ç‰ˆæ‰¹é‡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)}/{len(docs_info)} å€‹æ–‡æª”")
        return results
    
    def _load_vector_database(self, db_path: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        db = FAISS.load_local(
            db_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return db
    
    def _enhanced_document_retrieval(self, db, max_docs: int) -> List[Document]:
        """å¢å¼·çš„æ–‡æª”æª¢ç´¢ï¼ˆä¿æŒä¸è®Šï¼‰"""
        keywords = self.keyword_config.get_all_keywords()
        all_docs = []
        
        # ç­–ç•¥1: é—œéµå­—æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œé—œéµå­—æª¢ç´¢...")
        for keyword in keywords[:20]:  # å¢åŠ é—œéµå­—æ•¸é‡
            search_term = keyword if isinstance(keyword, str) else " ".join(keyword)
            docs = db.similarity_search(search_term, k=15)
            all_docs.extend(docs)
        
        # ç­–ç•¥2: å»£æ³›ä¸»é¡Œæª¢ç´¢
        print("   ğŸ” åŸ·è¡Œä¸»é¡Œæª¢ç´¢...")
        topic_queries = [
            "å¡‘è†  å›æ”¶ ææ–™",
            "å¯¶ç‰¹ç“¶ å¾ªç’° ç¶“æ¿Ÿ",
            "å†ç”Ÿ ç’°ä¿ æ°¸çºŒ",
            "å»¢æ–™ è™•ç† åˆ©ç”¨",
            "æ¸›ç¢³ æ•ˆç›Š ç’°å¢ƒ"
        ]
        
        for query in topic_queries:
            docs = db.similarity_search(query, k=20)
            all_docs.extend(docs)
        
        # ç­–ç•¥3: æ•¸å€¼æª¢ç´¢
        print("   ğŸ” åŸ·è¡Œæ•¸å€¼æª¢ç´¢...")
        number_queries = [
            "å„„æ”¯", "è¬å™¸", "åƒå™¸", "ç”¢èƒ½", "å›æ”¶é‡",
            "æ¸›ç¢³", "ç™¾åˆ†æ¯”", "æ•ˆç›Š", "æ•¸é‡"
        ]
        
        for query in number_queries:
            docs = db.similarity_search(query, k=10)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = {}
        for doc in all_docs:
            doc_hash = hash(doc.page_content)
            if doc_hash not in unique_docs:
                unique_docs[doc_hash] = doc
        
        result_docs = list(unique_docs.values())[:max_docs]
        print(f"ğŸ“š æª¢ç´¢åˆ° {len(result_docs)} å€‹å€™é¸æ–‡æª”")
        return result_docs
    
    def _balanced_filtering(self, documents: List[Document], doc_info: DocumentInfo) -> List[NumericExtraction]:
        """å¹³è¡¡ç‰ˆç¯©é¸ - ç¢ºä¿åŸºæœ¬è¦†è“‹ç‡ï¼ˆåŠ å¼·éæ¿¾ï¼‰"""
        print("âš–ï¸ åŸ·è¡Œå¹³è¡¡ç‰ˆç¯©é¸ï¼ˆåŠ å¼·éæ¿¾ï¼‰...")
        
        keywords = self.keyword_config.get_all_keywords()
        extractions = []
        
        for doc in tqdm(documents, desc="å¹³è¡¡ç¯©é¸"):
            # ä½¿ç”¨å¤šç¨®æ®µè½åˆ†å‰²ç­–ç•¥
            paragraphs = self._flexible_paragraph_split(doc.page_content)
            page_num = doc.metadata.get('page', 'æœªçŸ¥')
            
            for para_idx, paragraph in enumerate(paragraphs):
                if len(paragraph.strip()) < 15:  # é™ä½æœ€å°é•·åº¦è¦æ±‚
                    continue
                
                # å°æ¯å€‹é—œéµå­—é€²è¡ŒåŒ¹é…
                for keyword in keywords:
                    is_relevant, relevance_score, details = self.matcher.comprehensive_relevance_check(paragraph, keyword)
                    
                    if is_relevant and relevance_score > 0.55:  # ç¨å¾®æé«˜é–€æª»
                        # æå–æ•¸å€¼
                        numbers, percentages = self.matcher.extract_numbers_and_percentages(paragraph)
                        
                        # å¦‚æœæ²’æœ‰æ˜ç¢ºæ•¸å€¼ï¼Œä½†æœ‰é‡è¦é—œéµå­—ï¼Œä¹Ÿä¿ç•™
                        if not numbers and not percentages:
                            if relevance_score > 0.75:  # æé«˜æè¿°æ€§å…§å®¹çš„é–€æª»
                                keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                                
                                extraction = NumericExtraction(
                                    keyword=keyword_str,
                                    value="[ç›¸é—œæè¿°]",
                                    value_type='description',
                                    unit='',
                                    paragraph=paragraph.strip(),
                                    paragraph_number=para_idx + 1,
                                    page_number=f"ç¬¬{page_num}é ",
                                    confidence=relevance_score,
                                    context_window=self._get_context_window(doc.page_content, paragraph),
                                    company_name=doc_info.company_name,
                                    report_year=doc_info.report_year
                                )
                                extractions.append(extraction)
                        
                        # ç‚ºæ•¸å€¼å‰µå»ºæå–çµæœ
                        for number in numbers:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=number,
                                value_type='number',
                                unit=self._extract_unit(number),
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year
                            )
                            extractions.append(extraction)
                        
                        # ç‚ºç™¾åˆ†æ¯”å‰µå»ºæå–çµæœ
                        for percentage in percentages:
                            keyword_str = keyword if isinstance(keyword, str) else " + ".join(keyword)
                            
                            extraction = NumericExtraction(
                                keyword=keyword_str,
                                value=percentage,
                                value_type='percentage',
                                unit='%',
                                paragraph=paragraph.strip(),
                                paragraph_number=para_idx + 1,
                                page_number=f"ç¬¬{page_num}é ",
                                confidence=relevance_score,
                                context_window=self._get_context_window(doc.page_content, paragraph),
                                company_name=doc_info.company_name,
                                report_year=doc_info.report_year
                            )
                            extractions.append(extraction)
        
        print(f"âœ… å¹³è¡¡ç¯©é¸å®Œæˆ: æ‰¾åˆ° {len(extractions)} å€‹å€™é¸çµæœ")
        return extractions
    
    def _enhanced_post_process_extractions(self, extractions: List[NumericExtraction]) -> List[NumericExtraction]:
        """å¼·åŒ–çš„å¾Œè™•ç†å’Œå»é‡ - é‡é»æ”¹é€²"""
        if not extractions:
            return extractions
        
        print(f"ğŸ”§ å¼·åŒ–å¾Œè™•ç† {len(extractions)} å€‹æå–çµæœ...")
        
        # ç¬¬1æ­¥ï¼šæŒ‰é é¢åˆ†çµ„
        page_groups = {}
        for extraction in extractions:
            page_key = extraction.page_number
            if page_key not in page_groups:
                page_groups[page_key] = []
            page_groups[page_key].append(extraction)
        
        print(f"ğŸ“Š åˆ†çµ„çµæœ: {len(page_groups)} å€‹é é¢")
        
        # ç¬¬2æ­¥ï¼šå°æ¯å€‹é é¢é€²è¡ŒåŒæ•¸å€¼å»é‡
        deduped_extractions = []
        
        for page_key, page_extractions in page_groups.items():
            print(f"   è™•ç† {page_key}: {len(page_extractions)} å€‹çµæœ")
            
            # æŒ‰æ•¸å€¼åˆ†çµ„
            value_groups = {}
            for extraction in page_extractions:
                value_key = extraction.value.strip()
                if value_key not in value_groups:
                    value_groups[value_key] = []
                value_groups[value_key].append(extraction)
            
            # å°æ¯å€‹æ•¸å€¼çµ„ä¿ç•™æœ€ä½³çµæœ
            for value_key, value_extractions in value_groups.items():
                if len(value_extractions) == 1:
                    # åªæœ‰ä¸€å€‹çµæœï¼Œç›´æ¥ä¿ç•™
                    deduped_extractions.append(value_extractions[0])
                else:
                    # å¤šå€‹ç›¸åŒæ•¸å€¼ï¼Œé¸æ“‡æœ€ä½³çš„
                    best_extraction = self._select_best_extraction(value_extractions)
                    deduped_extractions.append(best_extraction)
                    print(f"     åŒæ•¸å€¼å»é‡: {value_key} å¾ {len(value_extractions)} å€‹ç¸®æ¸›ç‚º 1 å€‹")
        
        print(f"ğŸ“Š åŒæ•¸å€¼å»é‡å¾Œ: {len(deduped_extractions)} å€‹çµæœ")
        
        # ç¬¬3æ­¥ï¼šç²¾ç¢ºå»é‡ï¼ˆå…¨å±€ï¼‰
        unique_extractions = []
        seen_combinations = set()
        
        for extraction in deduped_extractions:
            # å‰µå»ºç²¾ç¢ºå”¯ä¸€æ¨™è­˜
            identifier = (
                extraction.value,
                extraction.value_type,
                extraction.page_number,
                extraction.paragraph[:100]  # ä½¿ç”¨æ®µè½å‰100å­—ç¬¦
            )
            
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_extractions.append(extraction)
        
        print(f"ğŸ“Š ç²¾ç¢ºå»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # ç¬¬4æ­¥ï¼šå…§å®¹ç›¸ä¼¼åº¦å»é‡
        if len(unique_extractions) > 1:
            filtered_extractions = []
            
            for i, extraction in enumerate(unique_extractions):
                is_duplicate = False
                
                for j, existing in enumerate(filtered_extractions):
                    # æª¢æŸ¥æ˜¯å¦ç‚ºç›¸ä¼¼å…§å®¹
                    if self._is_similar_extraction(extraction, existing):
                        is_duplicate = True
                        # ä¿ç•™ä¿¡å¿ƒåˆ†æ•¸æ›´é«˜çš„
                        if extraction.confidence > existing.confidence:
                            filtered_extractions[j] = extraction
                        break
                
                if not is_duplicate:
                    filtered_extractions.append(extraction)
            
            unique_extractions = filtered_extractions
            print(f"ğŸ“Š ç›¸ä¼¼åº¦å»é‡å¾Œ: {len(unique_extractions)} å€‹çµæœ")
        
        # ç¬¬5æ­¥ï¼šæŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        unique_extractions.sort(key=lambda x: x.confidence, reverse=True)
        
        print(f"âœ… å¼·åŒ–å¾Œè™•ç†å®Œæˆ: ä¿ç•™ {len(unique_extractions)} å€‹æœ€çµ‚çµæœ")
        return unique_extractions
    
    def _select_best_extraction(self, extractions: List[NumericExtraction]) -> NumericExtraction:
        """å¾ç›¸åŒæ•¸å€¼çš„å¤šå€‹æå–çµæœä¸­é¸æ“‡æœ€ä½³çš„"""
        # å„ªå…ˆç´šè¦å‰‡ï¼š
        # 1. ä¿¡å¿ƒåˆ†æ•¸æœ€é«˜
        # 2. é—œéµå­—ç›¸é—œæ€§æœ€é«˜ï¼ˆé€£çºŒé—œéµå­— > ä¸é€£çºŒé—œéµå­—ï¼‰
        # 3. æ®µè½é•·åº¦æœ€åˆé©ï¼ˆä¸å¤ªé•·ä¸å¤ªçŸ­ï¼‰
        
        # æŒ‰ä¿¡å¿ƒåˆ†æ•¸æ’åº
        sorted_extractions = sorted(extractions, key=lambda x: x.confidence, reverse=True)
        
        # å¦‚æœæœ€é«˜åˆ†æ•¸æ˜é¡¯é«˜æ–¼å…¶ä»–ï¼Œç›´æ¥é¸æ“‡
        if sorted_extractions[0].confidence - sorted_extractions[1].confidence > 0.1:
            return sorted_extractions[0]
        
        # å¦å‰‡åœ¨é«˜åˆ†çµ„ä¸­é€²ä¸€æ­¥ç¯©é¸
        high_score_extractions = [e for e in sorted_extractions if e.confidence >= sorted_extractions[0].confidence - 0.05]
        
        # å„ªå…ˆé¸æ“‡é€£çºŒé—œéµå­—
        continuous_keyword_extractions = [e for e in high_score_extractions if " + " not in e.keyword]
        if continuous_keyword_extractions:
            return continuous_keyword_extractions[0]
        
        # å¦å‰‡é¸æ“‡æ®µè½é•·åº¦æœ€åˆé©çš„
        optimal_length_extraction = min(high_score_extractions, 
                                      key=lambda x: abs(len(x.paragraph) - 300))  # ç›®æ¨™é•·åº¦300å­—ç¬¦
        
        return optimal_length_extraction
    
    def _is_similar_extraction(self, extraction1: NumericExtraction, extraction2: NumericExtraction) -> bool:
        """æª¢æŸ¥å…©å€‹æå–çµæœæ˜¯å¦ç›¸ä¼¼"""
        # æª¢æŸ¥é—œéµå­—ç›¸ä¼¼åº¦
        if extraction1.keyword != extraction2.keyword:
            return False
        
        # æª¢æŸ¥æ•¸å€¼ç›¸ä¼¼åº¦
        if extraction1.value == extraction2.value:
            return True
        
        # æª¢æŸ¥æ®µè½å…§å®¹ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        para1_words = set(extraction1.paragraph[:200].split())
        para2_words = set(extraction2.paragraph[:200].split())
        
        if para1_words and para2_words:
            overlap = len(para1_words & para2_words)
            total = len(para1_words | para2_words)
            similarity = overlap / total if total > 0 else 0
            
            # å¦‚æœæ®µè½ç›¸ä¼¼åº¦è¶…é70%ï¼Œèªç‚ºæ˜¯é‡è¤‡
            if similarity > 0.7:
                return True
        
        return False
    
    def _flexible_paragraph_split(self, text: str) -> List[str]:
        """éˆæ´»çš„æ®µè½åˆ†å‰²ï¼ˆä¿æŒä¸è®Šï¼‰"""
        # å˜—è©¦å¤šç¨®åˆ†å‰²æ–¹å¼
        paragraphs = []
        
        # æ–¹å¼1: æ¨™æº–åˆ†å‰²
        standard_paras = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs.extend([p.strip() for p in standard_paras if len(p.strip()) >= 15])
        
        # æ–¹å¼2: å¥è™Ÿåˆ†å‰²ï¼ˆå°æ–¼ç·Šå¯†æ–‡æœ¬ï¼‰
        sentence_paras = re.split(r'ã€‚{2,}|\.{2,}', text)
        paragraphs.extend([p.strip() for p in sentence_paras if len(p.strip()) >= 30])
        
        # æ–¹å¼3: ä¿æŒåŸæ–‡çš„å¤§å¡Šæ–‡æœ¬ï¼ˆå°æ–¼è¡¨æ ¼ï¼‰
        if len(text.strip()) >= 50:
            paragraphs.append(text.strip())
        
        # å»é‡
        unique_paragraphs = []
        seen = set()
        for para in paragraphs:
            para_hash = hash(para[:100])  # ä½¿ç”¨å‰100å­—ç¬¦ä½œç‚ºæ¨™è­˜
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)
        
        return unique_paragraphs
    
    def _export_to_excel(self, extractions: List[NumericExtraction], summary: ProcessingSummary, doc_info: DocumentInfo) -> str:
        """åŒ¯å‡ºçµæœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        company_safe = re.sub(r'[^\w\s-]', '', doc_info.company_name).strip()
        
        output_filename = f"ESGæå–çµæœ_å¹³è¡¡ç‰ˆ_{company_safe}_{doc_info.report_year}_{timestamp}.xlsx"
        output_path = os.path.join(RESULTS_PATH, output_filename)
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"ğŸ“Š åŒ¯å‡ºå¹³è¡¡ç‰ˆçµæœåˆ°Excel: {output_filename}")
        
        # æº–å‚™ä¸»è¦æ•¸æ“š
        main_data = []
        
        # ç¬¬ä¸€è¡Œï¼šå…¬å¸ä¿¡æ¯
        header_row = {
            'é—œéµå­—': f"å…¬å¸: {doc_info.company_name}",
            'æå–æ•¸å€¼': f"å ±å‘Šå¹´åº¦: {doc_info.report_year}",
            'æ•¸æ“šé¡å‹': f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            'å–®ä½': '',
            'æ®µè½å…§å®¹': f"å¹³è¡¡ç‰ˆæå–çµæœ: {len(extractions)} é …ï¼ˆå¼·åŒ–å»é‡ç‰ˆï¼‰",
            'æ®µè½ç·¨è™Ÿ': '',
            'é ç¢¼': '',
            'ä¿¡å¿ƒåˆ†æ•¸': '',
            'ä¸Šä¸‹æ–‡': f"æå–å™¨ç‰ˆæœ¬: v2.4 å¹³è¡¡ç‰ˆï¼ˆå¼·åŒ–å»é‡ï¼‰"
        }
        main_data.append(header_row)
        
        # ç©ºè¡Œåˆ†éš”
        main_data.append({col: '' for col in header_row.keys()})
        
        # æå–çµæœ
        for extraction in extractions:
            main_data.append({
                'é—œéµå­—': extraction.keyword,
                'æå–æ•¸å€¼': extraction.value,
                'æ•¸æ“šé¡å‹': extraction.value_type,
                'å–®ä½': extraction.unit,
                'æ®µè½å…§å®¹': extraction.paragraph,
                'æ®µè½ç·¨è™Ÿ': extraction.paragraph_number,
                'é ç¢¼': extraction.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': round(extraction.confidence, 3),
                'ä¸Šä¸‹æ–‡': extraction.context_window[:200] + "..." if len(extraction.context_window) > 200 else extraction.context_window
            })
        
        # çµ±è¨ˆæ•¸æ“š
        stats_data = []
        for keyword, count in summary.keywords_found.items():
            keyword_extractions = [e for e in extractions if e.keyword == keyword]
            
            stats_data.append({
                'é—œéµå­—': keyword,
                'æå–æ•¸é‡': count,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(np.mean([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3),
                'æœ€é«˜ä¿¡å¿ƒåˆ†æ•¸': round(max([e.confidence for e in keyword_extractions]) if keyword_extractions else 0, 3)
            })
        
        # å¯«å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            pd.DataFrame(main_data).to_excel(writer, sheet_name='å¹³è¡¡ç‰ˆæå–çµæœ', index=False)
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='é—œéµå­—çµ±è¨ˆ', index=False)
            
            # è™•ç†æ‘˜è¦
            summary_data = [{
                'å…¬å¸åç¨±': summary.company_name,
                'å ±å‘Šå¹´åº¦': summary.report_year,
                'ç¸½æ–‡æª”æ•¸': summary.total_documents,
                'ç¸½æå–çµæœ': summary.total_extractions,
                'è™•ç†æ™‚é–“(ç§’)': round(summary.processing_time, 2),
                'è™•ç†æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'æå–å™¨ç‰ˆæœ¬': 'v2.4 å¹³è¡¡ç‰ˆï¼ˆå¼·åŒ–å»é‡ï¼‰'
            }]
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='è™•ç†æ‘˜è¦', index=False)
        
        print(f"âœ… å¹³è¡¡ç‰ˆExcelæª”æ¡ˆå·²ä¿å­˜ï¼ˆå¼·åŒ–å»é‡ç‰ˆï¼‰")
        return output_path
    
    # =============================================================================
    # è¼”åŠ©æ–¹æ³•
    # =============================================================================
    
    def _extract_unit(self, value_str: str) -> str:
        """å¾æ•¸å€¼å­—ç¬¦ä¸²ä¸­æå–å–®ä½"""
        units = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', value_str)
        return units[-1] if units else ""
    
    def _get_context_window(self, full_text: str, target_paragraph: str, window_size: int = 150) -> str:
        """ç²å–æ®µè½çš„ä¸Šä¸‹æ–‡çª—å£"""
        try:
            pos = full_text.find(target_paragraph)
            if pos == -1:
                return target_paragraph[:300]
            
            start = max(0, pos - window_size)
            end = min(len(full_text), pos + len(target_paragraph) + window_size)
            
            return full_text[start:end]
        except:
            return target_paragraph[:300]

def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦ç”¨"""
    print("âš–ï¸ å¹³è¡¡ç‰ˆESGæå–å™¨æ¸¬è©¦æ¨¡å¼ï¼ˆå¼·åŒ–å»é‡ç‰ˆï¼‰")
    
    extractor = BalancedMultiFileESGExtractor(enable_llm=False)
    print("âœ… å¹³è¡¡ç‰ˆæå–å™¨åˆå§‹åŒ–å®Œæˆï¼ˆå¼·åŒ–å»é‡ç‰ˆï¼‰")

if __name__ == "__main__":
    main()